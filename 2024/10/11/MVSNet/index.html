<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>MVSNet | CJH's blog</title><meta name="keywords" content="MVSNet"><meta name="author" content="CJH"><meta name="copyright" content="CJH"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MVSNet 多视图几何论文阅读（一）MVSNet - 知乎 (zhihu.com) https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_43013761&#x2F;article&#x2F;details&#x2F;102869562?fromshare&#x3D;blogdetail&amp;sharetype&#x3D;blogdetail&amp;sharerId&#x3D;102869562&amp;sharerefer&#x3D;PC&amp;sh">
<meta property="og:type" content="article">
<meta property="og:title" content="MVSNet">
<meta property="og:url" content="http://cjh0220.github.io/2024/10/11/MVSNet/index.html">
<meta property="og:site_name" content="CJH&#39;s blog">
<meta property="og:description" content="MVSNet 多视图几何论文阅读（一）MVSNet - 知乎 (zhihu.com) https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_43013761&#x2F;article&#x2F;details&#x2F;102869562?fromshare&#x3D;blogdetail&amp;sharetype&#x3D;blogdetail&amp;sharerId&#x3D;102869562&amp;sharerefer&#x3D;PC&amp;sh">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2024/10/11/448b7fc6cc6a415b.png">
<meta property="article:published_time" content="2024-10-11T08:09:42.000Z">
<meta property="article:modified_time" content="2024-10-11T08:11:25.426Z">
<meta property="article:author" content="CJH">
<meta property="article:tag" content="MVSNet">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s3.bmp.ovh/imgs/2024/10/11/448b7fc6cc6a415b.png"><link rel="shortcut icon" href="/img/CJH.png"><link rel="canonical" href="http://cjh0220.github.io/2024/10/11/MVSNet/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MVSNet',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-11 16:11:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">142</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">108</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s3.bmp.ovh/imgs/2024/10/11/448b7fc6cc6a415b.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">CJH's blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">MVSNet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2024-10-11T08:09:42.000Z" title="发表于 2024-10-11 16:09:42">2024-10-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">三维重建</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="MVSNet"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>MVSNet</h1>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360365546">多视图几何论文阅读（一）MVSNet - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43013761/article/details/102869562?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=102869562&amp;sharerefer=PC&amp;sharesource=weixin_52648187&amp;sharefrom=from_link">https://blog.csdn.net/weixin_43013761/article/details/102869562?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=102869562&amp;sharerefer=PC&amp;sharesource=weixin_52648187&amp;sharefrom=from_link</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/liubing8609/article/details/85340015">https://blog.csdn.net/liubing8609/article/details/85340015</a></p>
<p>MVSNet——《MVSNet：Depth Inference for Unstructured Multi-view Stereo》</p>
<p>论文链接：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.pdf">MVSNetopenaccess.thecvf.com/content_ECCV_2018/papers/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.pdf</a></p>
<p>GitHub链接：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/YoYo000/MVSNet">MVSNetgithub.com/YoYo000/MVSNet</a></p>
<ul>
<li>作者简介：香港科技大学的权龙教授团队</li>
<li>论文意义：<strong>MVSNet</strong>（2018年ECCV）开启了用深度做多视图三维重建的先河。</li>
</ul>
<h2 id="一：多目重建背景介绍："><strong>一：多目重建背景介绍：</strong></h2>
<p>多目重建（MVS）是指利用多张RGB图像，来恢复场景或者物体的轮廓。从具有一定重叠度的多视图视角中恢复场景的稠密结构的技术，传统方法利用几何、光学一致性构造匹配代价，进行匹配代价累积，再估计<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E6%B7%B1%E5%BA%A6%E5%80%BC&amp;zhida_source=entity">深度值</a>。虽然传统方法有较高的深度估计精度，但由于存在在缺少纹理或者光照条件剧烈变化的场景中的错误匹配，传统方法的深度估计完整度还有很大的提升空间。</p>
<p>经过很长时间的发展，已经发展成了一个相对成熟的技术，一般可以分为以下四种方法：</p>
<p>Depth and normal fusion、Point cloud based：PWVS等、Volumetric、Variational mesh refinement。</p>
<p><img src="https://pic3.zhimg.com/80/v2-0018f5ef9fcd93f46b4fafc41b870e68_720w.webp" alt=""></p>
<p>而本文是在双目立体匹配的基础上，从双目三维重建扩展到了多目重建。本文区别于其他MVS方法的一个重要概念就是cost volume在MVS中的使用，这个正是双目中的一个核心概念。cost volume的概念简介见<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/297481800/answer/1274666655">链接</a>。</p>
<h2 id="二：本文创新点总结："><strong>二：本文创新点总结：</strong></h2>
<ol>
<li>端到端MVS网络，每次计算一个深度图，而不是立即计算整个三维场景，这样的思路保证了大规模三维重建的可行性。</li>
<li>使用可微的单应性warping（Differentiable Homography warping），来得到一个cost volume，保证了深度学习中的端到端的训练。</li>
<li>为了适应多个输入，提出了一种差异化度量，可以将多个特征放入一个cost feature里。这个<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=4&amp;q=cost+volume&amp;zhida_source=entity">cost volume</a>经过多尺度的3D卷积然后回归得到初始的视差图，最后经过优化得到最终的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=2&amp;q=%E8%A7%86%E5%B7%AE%E5%9B%BE&amp;zhida_source=entity">视差图</a>。</li>
</ol>
<h2 id="三：网络结构分析："><strong>三：网络结构分析：</strong></h2>
<p><img src="https://pica.zhimg.com/80/v2-7076de534cca32cdff998c53acad9d74_720w.webp" alt="img"></p>
<h3 id="1-特征提取">1.<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;zhida_source=entity">特征提取</a>:</h3>
<p>本文使用输入一张reference image**（参考图）** 和几张**source images（待映射图），**然后使用8个2D卷积提取feature maps，最后，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E4%B8%8B%E9%87%87%E6%A0%B7&amp;zhida_source=entity">下采样</a>得到原图1/4分辨率的32通道的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=2&amp;q=feature+maps&amp;zhida_source=entity">feature maps</a>。</p>
<p>值得提一点：很多同学认为特征匹配是原图中像素点之间的像素匹配，但是实验证明：相比较使用原图来进行匹配，使用深度学习得到的多维特征图的匹配结果会好很多。</p>
<h3 id="2-可微单应性映射：">2.可微单应性映射：</h3>
<p>这里是本文的重点，也是难点。</p>
<p>首先介绍一下单应性映射的原理，可参考如下两篇文章：）</p>
<p><a href="https://link.zhihu.com/?target=https%3A//pavancm.github.io/pdf/AIP_Final_Report.pdf">Image Warping using Local Homographypavancm.github.io/pdf/AIP_Final_Report.pdf</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//www.pianshen.com/article/9052297862/">图像到图像的映射、单应性变换（homography）、仿射变换 - 程序员大本营www.pianshen.com/article/9052297862/<img src="https://pic3.zhimg.com/v2-dcf1973094e20316a47ccffdff9c5eca_180x120.jpg" alt="img"></a></p>
<p><strong>(1). Differentiable Homography warping</strong></p>
<p>作者将得到的feature maps，通过{K,R,T}变换到参考平面得到不同的feature volume。其中，每个视差值对应一个feature，对应着不同的单应性<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5&amp;zhida_source=entity">变换矩阵</a>。</p>
<p><img src="https://pic3.zhimg.com/80/v2-d03b15485da0d37cf97076e47d7b478e_720w.webp" alt="img"></p>
<p>本图出自：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/127016721">MVSNet系列</a></p>
<p>这里使用的原理是：</p>
<p><img src="https://pic4.zhimg.com/80/v2-eb2e779cb51c49657538305b23c25cd7_720w.webp" alt="img"></p>
<p>该论文最重要的单应性变换( homography warping)的<strong>公式写错</strong>了，误导了好几篇后续改进的顶会论文，<strong>不过神奇地是提供的代码没有错</strong>：</p>
<p><strong>(2). Cost Metric</strong></p>
<p>这里使用的构建cost volume的方法是使用**方差，**这个过程是将上文得到的feature volume变换成cost volume，聚合多张feature volume{Vi}至一个cost volume C。提出了一个N-view的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F&amp;zhida_source=entity">相似性度量</a>的cost metric的M。这里的使用这里的M组成了cost volume C，M就是这里的方差。<strong>方差越小，说明在该深度上置信度越高。</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-736ca170efadf87b545084de388994b9_720w.webp" alt="img"></p>
<p>作者发现现有的方法使用均值去得到多个patch之间的相似性，但是作者发现均值并不能提供特征的相似性表示，这里选择<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E6%96%B9%E5%B7%AE%E5%BA%A6%E9%87%8F&amp;zhida_source=entity">方差度量</a>利用多视图之间的特征差异。并且作者实验发现方差的精度要比均值高。</p>
<h3 id="3-Cost-Volume正则化：">3.Cost Volume正则化：</h3>
<p>本文中的正则化是为了将Cost Volume <strong>C</strong>变成Probability Volume <strong>P</strong>，这里选择使用多尺度的UNet结构的3D CNN来进行正则化。过程中，将32 channel的cost volume正则化为1 channel，使用softmax 操作得到初始的result，然后和参考图进行concat，经过卷积的到最后的结果。</p>
<p><img src="https://pic4.zhimg.com/80/v2-fbb554ddd5886742a0c07856b65123ab_720w.webp" alt="img"></p>
<p><strong>(1). Depth map</strong></p>
<p>由于<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=argmax&amp;zhida_source=entity">argmax</a>操作不能得到亚像素精度，因此使用</p>
<p><img src="https://pic2.zhimg.com/80/v2-106424ce1ba6759891d6d1bf44965c6b_720w.webp" alt="img"></p>
<p>来估计图。</p>
<p><strong>(2). Probability Map</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-375a750f3540bed01c34d92418970868_720w.webp" alt="img"></p>
<p>这里定义深度估计d帽为ground truth与周围相邻深度的相似性，使用4个相邻假设深度的概率和去度量估计的质量。</p>
<p>作者原文提到：</p>
<p>Although the multi-scale 3D CNN has very strong ability to regularize the probability to the single modal distribution, we notice that for those falsely matched pixels, their probability distributions are scattered and cannot be concentrated to one peak</p>
<h3 id="4-Depth-map-Refinment">4.Depth map Refinment:</h3>
<p>由于<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=5&amp;q=%E6%AD%A3%E5%88%99%E5%8C%96&amp;zhida_source=entity">正则化</a>过程中的大的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E6%84%9F%E5%8F%97%E9%87%8E&amp;zhida_source=entity">感受野</a>，导致了重建的边缘过于平滑，这一点和一些分割很类似。这里使用了深度图的残差学习网络，把3通道的RGB图和initial的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=3&amp;q=%E6%B7%B1%E5%BA%A6%E5%9B%BE&amp;zhida_source=entity">深度图</a>concat到一起成4 channel，然后经过<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=4&amp;q=%E5%8D%B7%E7%A7%AF&amp;zhida_source=entity">卷积</a>变成了1 channel。然后再add-wise。</p>
<p>为了防止在预先深度的上的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=bias&amp;zhida_source=entity">bias</a>,我们把初始深度调整到[0-1]，最后优化结束后又给变回来。</p>
<h3 id="5-Loss">5. Loss:</h3>
<p>这里使用了混合监督的方式 ，具体损失使用的是L1 smooth做深度图的回归估计。</p>
<p><img src="https://pica.zhimg.com/80/v2-4da23766d9b30c33fdefe6d9aa872166_720w.webp" alt="img"></p>
<h3 id="6-三维重建：">6. <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=4&amp;q=%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA&amp;zhida_source=entity">三维重建</a>：</h3>
<p>利用多张图片之间的重建约束（<em>photometric</em> and <em>geometric</em> consistencies）来选择预测正确的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=168210615&amp;content_type=Article&amp;match_order=1&amp;q=%E6%B7%B1%E5%BA%A6%E4%BF%A1%E6%81%AF&amp;zhida_source=entity">深度信息</a>。</p>
<p><img src="https://pic1.zhimg.com/80/v2-a03f44abc7e3f44e9c591c0020378a1a_720w.webp" alt="img"></p>
<p><strong>MVSNet:香港科技大学的权龙教授团队的MVSNet（2018年ECCV）开启了用深度做多视图三维重建的先河。</strong></p>
<p><strong>2019年，2020年又有多篇改进：RMVSNet(CVPR2019),PointMVSNet(ICCV2019),P-MVSNet(ICCV2019),MVSCRF(ICCV2019),Cascade(CVPR2020),CVP-MVSNet(CVPR2020),Fast-MVSNet(CVPR2020),UCSNet(CVPR2020),CIDER(AAAI2020),PVAMVSNet(ECCV2020),D2HC-RMVSNet(ECCV2020)，Vis-MVSNet（BMVC 2020），AA-RMVSNet（<strong>ICCV2021</strong>），EPP-MVSNet（<strong>ICCV2021</strong>）。</strong></p>
<p><strong>以及最新综述：</strong></p>
<p><strong>Deep Learning for Multi-view Stereo via Plane Sweep: A Survey</strong></p>
<p><strong>链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.15328.pdf">https://arxiv.org/pdf/2106.15328.pdf</a></strong></p>
<p><strong>作者：北京大学</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/433228764">https://zhuanlan.zhihu.com/p/433228764</a></p>
<h2 id="自监督MVS论文总结：">自监督MVS论文总结：</h2>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/439210991">澎湃：自监督多视角立体视觉MVSNet系列论文整理</a></p>
<h2 id="第一个开源的基于深度学习的三维重建系统（利用MVSNet进行深度估计）："><strong>第一个开源的基于深度学习的三维重建系统（利用MVSNet进行深度估计）：</strong></h2>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442441726">澎湃：第一个开源的基于深度学习的完整三维重建系统DeepMVS</a></p>
<h2 id="一-MVSNet：目标是预测图片上每个像素的深度信息"><strong>一 MVSNet：目标是预测图片上每个像素的深度信息</strong></h2>
<p>MVSNet: Depth Inference for Unstructured Multi-view Stereo</p>
<p><strong>MVSNet本质是借鉴基于两张图片cost volume的双目立体匹配的深度估计方法，扩展到多张图片的深度估计，而基于cost volume的双目立体匹配已经较为成熟，所以MVSNet本质上也是借鉴一个较为成熟的领域，然后提出基于可微分的单应性变换的cost volume用于多视图深度估计。</strong></p>
<p><strong>论文实现了权龙教授多年的深度三维重建想法。</strong></p>
<p>过程：</p>
<p>（1）输入一张r<strong>eference image（为主）</strong> 和几张<strong>source images（辅助）</strong>；</p>
<p>（2）分别用网络提取出<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=%E4%B8%8B%E9%87%87%E6%A0%B7&amp;zhida_source=entity">下采样</a>四分之一的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=32%E9%80%9A%E9%81%93&amp;zhida_source=entity">32通道</a>的特征图；</p>
<p>（3）采用立体匹配（即双目深度估计）里提出的cost volume的概念，将几张source images的特征利用<strong>单应性变换( homography warping)<strong>转换到reference image，在转换的过程中，<strong>类似极线搜索</strong>，引入了深度信息。构建</strong>cost volume</strong>可以说是<strong>MVSNet的关键</strong>。</p>
<p><strong>具体costvolume上一个点是所有图片在这个点和深度值上特征的方差，方差越小，说明在该深度上置信度越高。</strong></p>
<p>（4）利用3D<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=%E5%8D%B7%E7%A7%AF&amp;zhida_source=entity">卷积</a>操作cost volume，先输出每个深度的概率，然后求深度的加权平均得到预测的深度信息，用L1或smoothL1回归深度信息，是一个<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B&amp;zhida_source=entity">回归模型</a>。</p>
<p>（5）利用多张图片之间的重建约束（<em>photometric</em> and <em>geometric</em> consistencies）来选择预测正确的深度信息，重建成<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91&amp;zhida_source=entity">三维点云</a>。</p>
<p>该论文最重要的单应性变换( homography warping)的<strong>公式写错</strong>了，误导了好几篇后续改进的顶会论文，<strong>不过神奇地是提供的代码没有错</strong>：</p>
<p><img src="https://pic3.zhimg.com/80/v2-2fc50366e5fca94fb63c4c8b50c15d2e_720w.webp" alt="img"></p>
<p>该公式错了！！！</p>
<p><img src="https://picx.zhimg.com/80/v2-bd6adb618c98e7b8c2cb09f4c1f2b805_720w.webp" alt="img"></p>
<p>MVSNet框图</p>
<h3 id="正确公式及MVS的平面扫描原理：">正确公式及MVS的平面扫描原理：</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363830541">ewrfcas：Multi-View Stereo中的平面扫描(plane sweep)</a></p>
<h2 id="二-MVSNet的后续改进论文"><strong>二 MVSNet的后续改进论文</strong></h2>
<h2 id="1-RMVSNet-CVPR2019">1.RMVSNet(CVPR2019)</h2>
<p>Recurrent MVSNet for High-<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=resolution&amp;zhida_source=entity">resolution</a> Multi-view Stereo Depth Inference</p>
<p>RMVSNet是香港科技大学权龙教授团队对自己的MVSNet（ECCV2018)论文的改进，主要是把3D卷积换成GRU的时序网络来降低模型大小，然后loss也改成了多分类的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1&amp;zhida_source=entity">交叉熵损失</a>，其他都一样，还是在四分之一的图上预测深度。模型变小了，但是其实精度也小有降低。</p>
<p>代码是用<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=tensorflow&amp;zhida_source=entity">tensorflow</a>写的，和MVSNet代码合到一起了，github链接：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/YoYo000/MVSNet">https://github.com/YoYo000/MVSNetgithub.com/YoYo000/MVSNet</a></p>
<p><img src="https://picx.zhimg.com/80/v2-f2d638812a5639e8699168c4548347ff_720w.webp" alt="img"></p>
<p>RMVSNet(CVPR2019)</p>
<h2 id="2-MVSNet（pytorch版本）">2. MVSNet（pytorch版本）</h2>
<p>这里需要特别强调一下，Xiaoyang Guo 同学把原来MVSNet的tensorflow代码改成了pytorch，这为几乎后续所有改进MVSNet的论文提供了极大的帮助，后续的论文几乎都是在Xiaoyang Guo同学的MVSNet_pytorch上改的。而且Xiaoyang Guo同学的MVSNet_pytorch已经比原来的MVSNet的效果好了不少，而后续的改进都是对比MVSNet论文里的结果，所以真正的提升其实并不大，后续改进应该对比Xiaoyang Guo同学的MVSNet_pytorch。</p>
<p>MVSNet论文里的结果和Xiaoyang Guo同学的MVSNet_pytorch在DTU数据集上的对比结果，可以看出Xiaoyang Guo已经提升了不少MVSNet的效果。</p>
<p><img src="https://picx.zhimg.com/80/v2-32b85b93e2bd021b18c3723f18c57ffb_720w.webp" alt="img"></p>
<p>Xiaoyang Guo同学的MVSNet_pytorch 链接：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/xy-guo/MVSNet_pytorch">https://github.com/xy-guo/MVSNet_pytorchgithub.com/xy-guo/MVSNet_pytorch</a></p>
<h2 id="3-PointMVSNet-ICCV2019">3 PointMVSNet(ICCV2019)</h2>
<p>Point-Based Multi-View Stereo Network ，清华大学</p>
<p>PointMVSNet(ICCV2019)也是2019年的改进MVSNet论文，想法是预测出深度depth信息然后和图片构成三维点云，再用<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=3D%E7%82%B9%E4%BA%91&amp;zhida_source=entity">3D点云</a>的算法去优化depth的回归。后续复现其代码发现无法得到PointMVSNet论文里的精度，不知道是不是因为3D点云的算法问题。</p>
<p>改的MVSNet_pytorch的代码，PointMVSNet github链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/callmeray/PointMVSNet">https://github.com/callmeray/PointMVSNet</a></p>
<p><img src="https://picx.zhimg.com/80/v2-9665027a1232e3e1a4002ee5a2d3bd29_720w.webp" alt="img"></p>
<p>PointMVSNet(ICCV2019)</p>
<h2 id="4-P-MVSNet-ICCV2019">4 P-MVSNet(ICCV2019)</h2>
<p>P-MVSNet: Learning Patch-wise Matching Confifidence Aggregation for Multi-View Stereo 华中科技大学</p>
<p>P-MVSNet对MVSNet的改进主要在于采用传统<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%AE%97%E6%B3%95&amp;zhida_source=entity">三维重建算法</a>中Patch-wise。还没有找到其代码。</p>
<p><img src="https://pic1.zhimg.com/80/v2-fde5862613d74a37f6d9fa6c73f86f10_720w.webp" alt="img"></p>
<p>P-MVSNet(ICCV2019)</p>
<h2 id="5-MVSCRF-ICCV2019">5 MVSCRF(ICCV2019)</h2>
<p>MVSCRF: Learning Multi-view Stereo with Conditional Random Fields</p>
<p>改进点：接入了一个CRF模块</p>
<p>清华大学。没有找到其代码。</p>
<p><img src="https://pic3.zhimg.com/80/v2-25823966248a0c520faf02a9b8d12f98_720w.webp" alt="img"></p>
<p>MVSCRF(ICCV2019)</p>
<h2 id="6-cascade-MVSNet（CVPR2020">6 cascade MVSNet（CVPR2020)</h2>
<p>Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</p>
<p>阿里，GitHub链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/alibaba/cascade-stereo">https://github.com/alibaba/cascade-stereo</a></p>
<p>改的MVSNet_pytorch的代码，主要是把MVSNet的模型改成了层级的，先预测下采样四分之一的深度，然后用来缩小下采样二分之一的深度，再用其缩小原始图片大小的深度，这样层级的方式，可以采用大的深度间隔和少的深度区间，从而可以一次训练更多数据。</p>
<p>另外由于双目立体匹配和MVSNet的MVS都是用了cost volume，双目立体匹配是用两张图片估计’深度‘，MVS的MVSNet是用三张及以上图片预测深度，所以其实模型差不多，都是相同的，cascade MVSNet也把改进思想用到了双目立体匹配上，一篇论文做了两份工作。</p>
<p><img src="https://pic2.zhimg.com/80/v2-3dc955981b10de9523ac33595cd7cfed_720w.webp" alt="img"></p>
<p>cascade MVSNet（CVPR2020)</p>
<h2 id="7-CVP-MVSNet（CVPR2020">7 CVP-MVSNet（CVPR2020)</h2>
<p>Cost Volume Pyramid Based Depth Inference for Multi-View Stereo</p>
<p>澳大利亚国立和<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=121148124&amp;content_type=Article&amp;match_order=1&amp;q=%E8%8B%B1%E4%BC%9F%E8%BE%BE&amp;zhida_source=entity">英伟达</a>，github链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/JiayuYANG/CVP-MVSNet">https://github.com/JiayuYANG/CVP-MVSNet</a></p>
<p>也是改的MVSNet_pytorch的代码，和上一个cascade MVSNet比较类似，也是先预测出深度信息然后用来缩小更大的图片的深度，CVP-MVSNet相比cascade MVSNet也缩小了cost volume的范围。</p>
<p><img src="https://picx.zhimg.com/80/v2-d9bdd51bfd71ae7131d972fabf1b9729_720w.webp" alt="img"></p>
<p>CVP-MVSNet（CVPR2020)</p>
<h2 id="8-Fast-MVSNet（CVPR2020">8 Fast-MVSNet（CVPR2020)</h2>
<p>Fast-MVSNet: Sparse-to-Dense Multi-View Stereo With Learned Propagation</p>
<p>and Gauss-Newton Refifinement，上海科技大学</p>
<p>也是改的MVSNet_pytorch的代码，github链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/svip-lab/FastMVSNet">https://github.com/svip-lab/FastMVSNet</a></p>
<p>Fast-MVSNet采用稀疏的cost volume以及Gauss-Newton layer，目的是提高MVSNet的速度。</p>
<p><img src="https://pic1.zhimg.com/80/v2-a0ad37f9cbd85fd2eab80af59c56cebe_720w.webp" alt="img"></p>
<p>Fast-MVSNet（CVPR2020)</p>
<h2 id="9-CIDER（AAAI-2020">9 CIDER（AAAI 2020)</h2>
<p>Learning Inverse Depth Regression for Multi-View Stereo with Correlation Cost Volume , 华科的</p>
<p>GitHub链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/GhiXu/CIDER">https://github.com/GhiXu/CIDER</a></p>
<p>CIDER主要采用采用group的方式提出了一个小的cost volume</p>
<p><img src="https://pic4.zhimg.com/80/v2-9e7fa383a44c16cadfcf80ae60079d31_720w.webp" alt="img"></p>
<p>CIDER（AAAI 2020)</p>
<h2 id="10-PVA-MVSNet（ECCV2020">10 PVA-MVSNet（ECCV2020)</h2>
<p>Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation</p>
<p>北大，GitHub链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/yhw-yhw/PVAMVSNet">https://github.com/yhw-yhw/PVAMVSNet</a></p>
<p>主要采用attention机制来自适应学习一些权重，比如不同view的权重。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9023186cd1d3f7d6210dec2d921f3b8a_720w.webp" alt="img"></p>
<p>PVA-MVSNet（ECCV2020)</p>
<h2 id="11-UCSNet（CVPR2020">11 UCSNet（CVPR2020)</h2>
<p>Deep Stereo using Adaptive Thin Volume Representation with Uncertainty Awareness</p>
<p>github链接：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/touristCheng/UCSNet">https://github.com/touristCheng/UCSNetgithub.com/touristCheng/UCSNet</a></p>
<p>UCSNet和上面两个差不多，不过更好一点，depth interval 可以自动调，最大层度的进行网络层级，通过下采样四分之一的深度结果来缩小cost volume和深度的范围，从而让模型尽可能小。</p>
<p><img src="https://pic3.zhimg.com/80/v2-b4a5775cc271e157d0ff6f2663b96da6_720w.webp" alt="img"></p>
<p>UCSNet（CVPR2020)</p>
<h2 id="12-D2HC-RMVSNet-ECCV2020-Spotlight">12 <em>D</em>2HC-RMVSNet(ECCV2020 Spotlight)</h2>
<p>Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency Checking</p>
<p>github链接(还未提供）：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/yhw-yhw/D2HC-RMVSNet">yhw-yhw/D2HC-RMVSNet</a></p>
<p>还没细看，大概和RMVSNet差不多，用LSTM来处理cost volume，同时提出一种Dynamic Consistency Checking来后融合。</p>
<p>可能因为在Tanks榜单上排名较高（目前滑落到第二，论文提交时第一），所以拿了ECCV2020的Spotlight。</p>
<p><img src="https://pic3.zhimg.com/80/v2-a087875c73992ea9288ce6ba50d0d8b0_720w.webp" alt="img"></p>
<p>D2HC-RMVSNet</p>
<h2 id="13-Visibility-aware-Multi-view-Stereo-Network-BMVC2020-oral">13 <strong>Visibility-aware Multi-view Stereo Network</strong>(BMVC2020 oral)</h2>
<p><img src="https://pica.zhimg.com/80/v2-d67066ca1c6809a2075567c756e74b0c_720w.webp" alt="img"></p>
<p>Vis-MVSNet BMVC2020</p>
<p>github:<a href="https://link.zhihu.com/?target=https%3A//github.com/jzhangbs/Vis-MVSNet">https://github.com/jzhangbs/Vis-MVSNet</a></p>
<p>香港科技大学的权龙教授团队的最新的一篇论文，发表在BMVC2020上，主要是考虑了别的基于深度学习的论文都没考虑的一个问题：多视图构建cost volume的可见性问题。代码融合多阶段和group Cost Volume等技巧。</p>
<h2 id="13-AA-RMVSNet-ICCV2021">13 <em>AA</em>-RMVSNet(ICCV2021)</h2>
<p><strong>AA-RMVSNet: Adaptive Aggregation Recurrent Multi-view Stereo Network</strong></p>
<p><strong>单位：北京大学</strong></p>
<p><strong>主要思想：采用Intra-view AA module aims to aggregate context-aware features for multiple scales and regions with varying richness of texture. Inter-view AA module adaptively aggregates cost volumes of different views by yielding pixel-wise attention maps for each view.</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-10893cc74e148fa5845bf72746357d76_720w.webp" alt="img"></p>
<h2 id="总结：">总结：</h2>
<p>香港科技大学的权龙教授团队的Yao Yao把双目立体匹配的cost volume，引入了基于深度学习的三维重建领域，提出了MVSNet，并整理了DTU数据集，开创了通过深度模型预测深度进行三维重建的一个新领域。</p>
<p>后续Xiaoyang Guo 同学把原来MVSNet的tensorflow代码改成了pytorch框架，极大地增加了代码的可读性，方便了后续一系列对MVSNet的改进。也提高了改进的基点。</p>
<p>得特别感谢香港科技大学的权龙教授团队和Xiaoyang Guo 同学。</p>
<p>由于tanks and temples榜单评价的是点云，阻碍tanks and temples榜单上排名的可能并不是深度值预测的不好，而是其他的问题。三维重建涉及的东西很多。榜单上排名高的模型可能是因为在模型以外的地方做了东西。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io">CJH</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io/2024/10/11/MVSNet/">http://cjh0220.github.io/2024/10/11/MVSNet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://cjh0220.github.io" target="_blank">CJH's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MVSNet/">MVSNet</a></div><div class="post_share"><div class="social-share" data-image="https://s3.bmp.ovh/imgs/2024/10/11/448b7fc6cc6a415b.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/10/23/UVmapping/"><img class="prev-cover" src="https://s3.bmp.ovh/imgs/2024/10/20/da943600cfa91298.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">UVmapping</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/08/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E4%BC%B0%E6%A0%87%E5%87%86/"><img class="next-cover" src="https://s3.bmp.ovh/imgs/2024/10/08/249610cbabf14611.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">三维重建的损失函数和评估标准</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJH</div><div class="author-info__description">Hello,my friends</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">142</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">108</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cjh0220"><i class="fab fa-github"></i><span>Gihhub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cjh0220" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1005741898@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">MVSNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%EF%BC%9A%E5%A4%9A%E7%9B%AE%E9%87%8D%E5%BB%BA%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D%EF%BC%9A"><span class="toc-number">1.1.</span> <span class="toc-text">一：多目重建背景介绍：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%EF%BC%9A%E6%9C%AC%E6%96%87%E5%88%9B%E6%96%B0%E7%82%B9%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">1.2.</span> <span class="toc-text">二：本文创新点总结：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%EF%BC%9A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90%EF%BC%9A"><span class="toc-number">1.3.</span> <span class="toc-text">三：网络结构分析：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">1.3.1.</span> <span class="toc-text">1.特征提取:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8F%AF%E5%BE%AE%E5%8D%95%E5%BA%94%E6%80%A7%E6%98%A0%E5%B0%84%EF%BC%9A"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.可微单应性映射：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Cost-Volume%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%9A"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.Cost Volume正则化：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Depth-map-Refinment"><span class="toc-number">1.3.4.</span> <span class="toc-text">4.Depth map Refinment:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Loss"><span class="toc-number">1.3.5.</span> <span class="toc-text">5. Loss:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%EF%BC%9A"><span class="toc-number">1.3.6.</span> <span class="toc-text">6. 三维重建：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E7%9B%91%E7%9D%A3MVS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">1.4.</span> <span class="toc-text">自监督MVS论文总结：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E7%9A%84%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%88%A9%E7%94%A8MVSNet%E8%BF%9B%E8%A1%8C%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%89%EF%BC%9A"><span class="toc-number">1.5.</span> <span class="toc-text">第一个开源的基于深度学习的三维重建系统（利用MVSNet进行深度估计）：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-MVSNet%EF%BC%9A%E7%9B%AE%E6%A0%87%E6%98%AF%E9%A2%84%E6%B5%8B%E5%9B%BE%E7%89%87%E4%B8%8A%E6%AF%8F%E4%B8%AA%E5%83%8F%E7%B4%A0%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BF%A1%E6%81%AF"><span class="toc-number">1.6.</span> <span class="toc-text">一 MVSNet：目标是预测图片上每个像素的深度信息</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E7%A1%AE%E5%85%AC%E5%BC%8F%E5%8F%8AMVS%E7%9A%84%E5%B9%B3%E9%9D%A2%E6%89%AB%E6%8F%8F%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="toc-number">1.6.1.</span> <span class="toc-text">正确公式及MVS的平面扫描原理：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-MVSNet%E7%9A%84%E5%90%8E%E7%BB%AD%E6%94%B9%E8%BF%9B%E8%AE%BA%E6%96%87"><span class="toc-number">1.7.</span> <span class="toc-text">二 MVSNet的后续改进论文</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RMVSNet-CVPR2019"><span class="toc-number">1.8.</span> <span class="toc-text">1.RMVSNet(CVPR2019)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-MVSNet%EF%BC%88pytorch%E7%89%88%E6%9C%AC%EF%BC%89"><span class="toc-number">1.9.</span> <span class="toc-text">2. MVSNet（pytorch版本）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-PointMVSNet-ICCV2019"><span class="toc-number">1.10.</span> <span class="toc-text">3 PointMVSNet(ICCV2019)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-P-MVSNet-ICCV2019"><span class="toc-number">1.11.</span> <span class="toc-text">4 P-MVSNet(ICCV2019)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-MVSCRF-ICCV2019"><span class="toc-number">1.12.</span> <span class="toc-text">5 MVSCRF(ICCV2019)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-cascade-MVSNet%EF%BC%88CVPR2020"><span class="toc-number">1.13.</span> <span class="toc-text">6 cascade MVSNet（CVPR2020)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-CVP-MVSNet%EF%BC%88CVPR2020"><span class="toc-number">1.14.</span> <span class="toc-text">7 CVP-MVSNet（CVPR2020)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Fast-MVSNet%EF%BC%88CVPR2020"><span class="toc-number">1.15.</span> <span class="toc-text">8 Fast-MVSNet（CVPR2020)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-CIDER%EF%BC%88AAAI-2020"><span class="toc-number">1.16.</span> <span class="toc-text">9 CIDER（AAAI 2020)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-PVA-MVSNet%EF%BC%88ECCV2020"><span class="toc-number">1.17.</span> <span class="toc-text">10 PVA-MVSNet（ECCV2020)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-UCSNet%EF%BC%88CVPR2020"><span class="toc-number">1.18.</span> <span class="toc-text">11 UCSNet（CVPR2020)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-D2HC-RMVSNet-ECCV2020-Spotlight"><span class="toc-number">1.19.</span> <span class="toc-text">12 D2HC-RMVSNet(ECCV2020 Spotlight)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-Visibility-aware-Multi-view-Stereo-Network-BMVC2020-oral"><span class="toc-number">1.20.</span> <span class="toc-text">13 Visibility-aware Multi-view Stereo Network(BMVC2020 oral)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-AA-RMVSNet-ICCV2021"><span class="toc-number">1.21.</span> <span class="toc-text">13 AA-RMVSNet(ICCV2021)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">1.22.</span> <span class="toc-text">总结：</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E5%A4%9A%E8%A7%86%E8%A7%92MRI%E5%BF%83%E8%84%8F%E9%87%8D%E5%BB%BA/" title="论文精读（十四）多视角MRI心脏重建"><img src="https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读（十四）多视角MRI心脏重建"/></a><div class="content"><a class="title" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E5%A4%9A%E8%A7%86%E8%A7%92MRI%E5%BF%83%E8%84%8F%E9%87%8D%E5%BB%BA/" title="论文精读（十四）多视角MRI心脏重建">论文精读（十四）多视角MRI心脏重建</a><time datetime="2025-05-13T07:27:38.000Z" title="发表于 2025-05-13 15:27:38">2025-05-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89Carver%E5%8F%B3%E5%BF%83%E5%AE%A4%E9%87%8D%E5%BB%BA/" title="论文精读（十三）Carver右心室重建"><img src="https://s3.bmp.ovh/imgs/2025/05/13/3284b4054df1f355.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读（十三）Carver右心室重建"/></a><div class="content"><a class="title" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89Carver%E5%8F%B3%E5%BF%83%E5%AE%A4%E9%87%8D%E5%BB%BA/" title="论文精读（十三）Carver右心室重建">论文精读（十三）Carver右心室重建</a><time datetime="2025-05-13T07:26:27.000Z" title="发表于 2025-05-13 15:26:27">2025-05-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89PCN/" title="论文精读（十二）PCN"><img src="https://s3.bmp.ovh/imgs/2025/05/13/4a7bc492cc5acaff.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读（十二）PCN"/></a><div class="content"><a class="title" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89PCN/" title="论文精读（十二）PCN">论文精读（十二）PCN</a><time datetime="2025-05-13T07:25:03.000Z" title="发表于 2025-05-13 15:25:03">2025-05-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9ADGCNN/" title="论文精读（十一）：DGCNN"><img src="https://s3.bmp.ovh/imgs/2025/05/13/9904985f6f071d29.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读（十一）：DGCNN"/></a><div class="content"><a class="title" href="/2025/05/13/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9ADGCNN/" title="论文精读（十一）：DGCNN">论文精读（十一）：DGCNN</a><time datetime="2025-05-13T07:23:49.000Z" title="发表于 2025-05-13 15:23:49">2025-05-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/08/%E6%89%8B%E6%90%93Vit%EF%BC%88%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%BD%9C%E4%B8%9A%EF%BC%89/" title="手搓Vit（模式识别作业）"><img src="https://s3.bmp.ovh/imgs/2025/05/08/e2a3ff348d798237.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="手搓Vit（模式识别作业）"/></a><div class="content"><a class="title" href="/2025/05/08/%E6%89%8B%E6%90%93Vit%EF%BC%88%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%BD%9C%E4%B8%9A%EF%BC%89/" title="手搓Vit（模式识别作业）">手搓Vit（模式识别作业）</a><time datetime="2025-05-08T08:49:32.000Z" title="发表于 2025-05-08 16:49:32">2025-05-08</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s3.bmp.ovh/imgs/2024/10/11/448b7fc6cc6a415b.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By CJH</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">欢迎欢迎，热烈欢迎</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>