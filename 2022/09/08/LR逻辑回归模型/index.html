<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>LR逻辑回归模型 | CJH's blog</title><meta name="keywords" content="逻辑回归"><meta name="author" content="CJH"><meta name="copyright" content="CJH"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LR逻辑回归(logistics regression) 逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。 逻辑回归就是根据之前的数据，预测某事件为真的概率值 一.分类和回归任务的区别 ​		我们可以按照任务的种类,将任务分为回归任务和分类任务.那这两者的区别是什么呢?按照较官方些的说法,输入变量与输出变量均为连续变量的预测问题是回归问题,输出变量为有限个离散变量的预测问题成为分类问题.">
<meta property="og:type" content="article">
<meta property="og:title" content="LR逻辑回归模型">
<meta property="og:url" content="http://cjh0220.github.io/2022/09/08/LR%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="CJH&#39;s blog">
<meta property="og:description" content="LR逻辑回归(logistics regression) 逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。 逻辑回归就是根据之前的数据，预测某事件为真的概率值 一.分类和回归任务的区别 ​		我们可以按照任务的种类,将任务分为回归任务和分类任务.那这两者的区别是什么呢?按照较官方些的说法,输入变量与输出变量均为连续变量的预测问题是回归问题,输出变量为有限个离散变量的预测问题成为分类问题.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1.ax1x.com/2022/09/08/vqpCng.jpg">
<meta property="article:published_time" content="2022-09-08T12:02:23.000Z">
<meta property="article:modified_time" content="2022-09-08T12:03:16.012Z">
<meta property="article:author" content="CJH">
<meta property="article:tag" content="逻辑回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2022/09/08/vqpCng.jpg"><link rel="shortcut icon" href="/img/CJH.png"><link rel="canonical" href="http://cjh0220.github.io/2022/09/08/LR%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LR逻辑回归模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-08 20:03:16'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">107</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">85</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s1.ax1x.com/2022/09/08/vqpCng.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">CJH's blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LR逻辑回归模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2022-09-08T12:02:23.000Z" title="发表于 2022-09-08 20:02:23">2022-09-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/">数学建模</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LR逻辑回归模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>LR逻辑回归(logistics regression)</h1>
<p>逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。<br>
逻辑回归就是根据之前的数据，预测某事件为真的概率值</p>
<h2 id="一-分类和回归任务的区别"><strong>一.分类和回归任务的区别</strong></h2>
<p>​		我们可以按照任务的种类,将任务分为回归任务和分类任务.那这两者的区别是什么呢?按照较官方些的说法,输入变量与输出变量均为连续变量的预测问题是回归问题,输出变量为有限个离散变量的预测问题成为分类问题.</p>
<p>​		通俗一点讲,我们要预测的结果是一个数,比如要通过一个人的饮食预测一个人的体重,体重的值可以有无限多个,有的人50kg,有的人51kg,在50和51之间也有无限多个数.这种预测结果是某一个确定数,而具体是哪个数有无限多种可能的问题,我们会训练出一个模型,传入参数后得到这个确定的数,这类问题我们称为回归问题.预测的这个变量(体重)因为有无限多种可能,在数轴上是连续的,所以我们称这种变量为连续变量.<br>
​		我们要预测一个人身体健康或者不健康,预测会得癌症或者不会得癌症,预测他是水瓶座,天蝎座还是射手座,这种结果只有几个值或者多个值的问题,我们可以把每个值都当做一类,预测对象到底属于哪一类.这样的问题称为分类问题.如果一个分类问题的结果只有两个,比如&quot;是&quot;和&quot;不是&quot;两个结果,我们把结果为&quot;是&quot;的样例数据称为&quot;正例&quot;,讲结果为&quot;不是&quot;的样例数据称为&quot;负例&quot;,对应的,这种结果的变量称为离散型变量.</p>
<h2 id="二-逻辑回归不是回归"><strong>二.逻辑回归不是回归</strong></h2>
<p>​		从名字来理解逻辑回归.在逻辑回归中,逻辑一词是logistics [lə’dʒɪstɪks]的音译字,并不是因为这个算法是突出逻辑的特性.</p>
<p>​		至于回归,我们前一段讲到回归任务是结果为连续型变量的任务,logistics regression是用来做分类任务的,为什么叫回归呢?那我们是不是可以假设,逻辑回归就是用回归的办法来做分类的呢.跟上思路.</p>
<h3 id="三-怎么做">三.怎么做</h3>
<p>​		假设刚刚的思路是正确的,逻辑回归就是在用回归的办法做分类任务,那有什么办法可以做到呢,此时我们就先考虑最简单的二分类,结果是正例或者负例的任务.</p>
<p>​		按照多元线性回归的思路,我们可以先对这个任务进行线性回归,学习出这个事情结果的规律,比如根据人的饮食,作息,工作和生存环境等条件预测一个人&quot;有&quot;或者&quot;没有&quot;得恶性肿瘤,可以先通过回归任务来预测人体内肿瘤的大小,取一个平均值作为阈值,假如平均值为y,肿瘤大小超过y为恶心肿瘤,无肿瘤或大小小于y的,为非恶性.这样通过线性回归加设定阈值的办法,就可以完成一个简单的二分类任务.如下图:</p>
 <img src="https://img-blog.csdnimg.cn/20181110164028163.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTQ0NTU1Ng==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />
<p>​		上图中,红色的x轴为肿瘤大小,粉色的线为回归出的函数的图像,绿色的线为阈值.<br>
​		预测肿瘤大小还是一个回归问题,得到的结果(肿瘤的大小)也是一个连续型变量.通过设定阈值,就成功将回归问题转化为了分类问题.但是,这样做还存在一个问题.</p>
<p>​		我们上面的假设,依赖于所有的肿瘤大小都不会特别离谱,如果有一个超大的肿瘤在我们的例子中,阈值就很难设定.加入还是取平均大小为阈值,则会出现下图的情况:</p>
<p>​         <img src="https://img-blog.csdnimg.cn/20181110165551883.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTQ0NTU1Ng==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" /></p>
<p>​		从上边的例子可以看出,使用线性的函数来拟合规律后取阈值的办法是行不通的,行不通的原因在于拟合的函数太直,离群值(也叫异常值)对结果的影响过大,但是我们的整体思路是没有错的,错的是用了太&quot;直&quot;的拟合函数,如果我们用来拟合的函数是非线性的,不这么直,是不是就好一些呢?</p>
<p>​		所以我们下面来做两件事:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1-找到一个办法解决掉回归的函数严重受离群值影响的办法.</span><br><span class="line">2-选定一个阈值.</span><br></pre></td></tr></table></figure>
<h2 id="四-把回归函数掰弯"><strong>四:把回归函数掰弯</strong></h2>
<p>​		没错,本小节用来解决上边说的第一个问题.开玩笑了,无论如何我也不可能掰弯这个函数.我们能做的呢,就是换一个.原来的判别函数我们用线性的y = <img src="https://private.codecogs.com/gif.latex?w%5E%7BT%7Dx" alt="w^{T}x">,逻辑回归的函数呢,我们目前就用sigmod函数,函数如下:</p>
<img src="https://img-blog.csdnimg.cn/20200407172245460.png" style="zoom: 50%;" />
<p>​		公式中,e为欧拉常数(是常数,如果不知道,自行百度),Z就是我们熟悉的多元线性回归中的<img src="https://private.codecogs.com/gif.latex?W%5E%7BT%7DX" alt="W^{T}X">,建议现阶段大家先记住逻辑回归的判别函数用它就好了.</p>
<p>​		就像我们说多元线性回归的判别函数为<img src="https://private.codecogs.com/gif.latex?y%20%3D%20w_%7B0%7Dx_%7B0%7D%20+%20w_%7B1%7Dx_%7B1%7D%20+%20...%20+w_%7Bn%7Dx_%7Bn%7D" alt="y = w_{0}x_{0} + w_{1}x_{1} + ... +w_{n}x_{n}">一样.追究为什么是他花费的经历会比算法本身更多.</p>
<p>sigmod函数的图像如下:</p>
<img src="https://img-blog.csdnimg.cn/20181110172820614.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTQ0NTU1Ng==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 33%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = numpy.dot(X, theta)     <span class="comment">#python代码</span></span><br><span class="line">h = <span class="number">1</span>/(<span class="number">1</span>+numpy.exp(-z))     <span class="comment"># exp: e 的多少次方</span></span><br></pre></td></tr></table></figure>
<p>​		该函数具有很强的鲁棒性(鲁棒是Robust的音译，也就是健壮和强壮的意思),并且将函数的输入范围(∞,-∞)映射到了输出的(0,1)之间且具有概率意义.具有概率意义是怎么理解呢:将一个样本输入到我们学习到的函数中,输出0.7,意思就是这个样本有70%的概率是正例,1-70%就是30%的概率为负例.</p>
<p>​		再次强调一下,如果你的数学功底很好,可以看一下我上边分享的为什么是sigmod函数的连接,如果数学一般,我们这个时候没有必要纠结为什么是sigmod,函数那么多为什么选他.学习到后边你自然就理解了</p>
<p>​		总结一下上边所讲:我们利用线性回归的办法来拟合然后设置阈值的办法容易受到离群值的影响,sigmod函数可以有效的帮助我们解决这一个问题,所以我们只要在拟合的时候把即y =<img src="https://private.codecogs.com/gif.latex?y%20%3D%20w_%7B0%7Dx_%7B0%7D%20+%20w_%7B1%7Dx_%7B1%7D%20+%20...%20+w_%7Bn%7Dx_%7Bn%7D" alt="img"> 换成<img src="https://private.codecogs.com/gif.latex?W%5E%7BT%7DX" alt="img">即可,其中</p>
<p>z=<img src="https://private.codecogs.com/gif.latex?W%5E%7BT%7DX" alt="img">,也就是说g(z) =<img src="https://private.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B1%20+%20e%5E%7Bw%5E%7BT%7Dx%7D%7D" alt="img"> . 同时,因为g(z)函数的特性,它输出的结果也不再是预测结果,而是一个值预测为正例的概率,预测为负例的概率就是1-g(z).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">函数形式表达:</span><br><span class="line"></span><br><span class="line">         P(y=0|w,x) = 1 – g(z)</span><br><span class="line"></span><br><span class="line">         P(y=1|w,x) =  g(z)</span><br><span class="line"></span><br><span class="line">         P(正确) = *         为某一条样本的预测值,取值范围为0或者1.</span><br><span class="line"></span><br><span class="line">到这里,我们得到一个回归函数,它不再像y=wT * x一样受离群值影响,他的输出结果是样本预测为正例的概率(0到1之间的小数).我们接下来解决第二个问题:选定一个阈值.</span><br></pre></td></tr></table></figure>
<h2 id="五-选定阈值"><strong>五:选定阈值</strong></h2>
<p>​		选定阈值的意思就是,当我选阈值为0.5,那么小于0.5的一定是负例,哪怕他是0.49.此时我们判断一个样本为负例一定是准确的吗?其实不一定,因为它还是有49%的概率为正利的.但是即便他是正例的概率为0.1,我们随机选择1w个样本来做预测,还是会有接近100个预测它是负例结果它实际是正例的误差.无论怎么选,误差都是存在的.所以我们选定阈值的时候就是在选择可以接受误差的程度.<br>
​		我们现在知道了sigmod函数预测结果为一个0到1之间的小数,选定阈值的第一反应,大多都是选0.5,其实实际工作中并不一定是0.5,阈值的设定往往是根据实际情况来判断的.本小节我们只举例让大家理解为什么不完全是0.5,并不会有一个万能的答案,都是根据实际工作情况来定的.</p>
<p>​		0到1之间的数阈值选作0.5当然是看着最舒服的,可是假设此时我们的业务是像前边的例子一样,做一个肿瘤的良性恶性判断.选定阈值为0.5就意味着,如果一个患者得恶性肿瘤的概率为0.49,模型依旧认为他没有患恶性肿瘤,结果就是造成了严重的医疗事故.此类情况我们应该将阈值设置的小一些.阈值设置的小,加入0.3,一个人患恶性肿瘤的概率超过0.3我们的算法就会报警,造成的结果就是这个人做一个全面检查,比起医疗事故来讲,显然这个更容易接受.<br>
​		第二种情况,加入我们用来识别验证码,输出的概率为这个验证码识别正确的概率.此时我们大可以将概率设置的高一些.因为即便识别错了又能如何,造成的结果就是在一个session时间段内重试一次.机器识别验证码就是一个不断尝试的过程,错误率本身就很高.</p>
<p>​		 以上两个例子可能不大准确,只做意会,你懂了就好.</p>
<pre><code>	到这里,逻辑回归的由来我们就基本理清楚了,现在我们知道了逻辑回归的判别函数就是![img](https://private.codecogs.com/gif.latex?g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B1+e%5E%7B-z%7D%7D)
</code></pre>
<p>，z=<img src="https://private.codecogs.com/gif.latex?W%5E%7BT%7DX" alt="img"></p>
<h2 id="六-最大似然估计"><strong>六.最大似然估计</strong></h2>
<p>​		此时我们想要找到一组w,使函数<img src="https://private.codecogs.com/gif.latex?g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B1+e%5E%7B-z%7D%7D" alt="g(z) = \frac{1}{1+e^{-z}}">正确的概率最大.而我们在上面的推理过程中已经得到每个单条样本预测正确概率的公式:</p>
<pre><code>	P(正确) =![(g(w,xi))^&#123;y^&#123;i&#125;&#125;](https://private.codecogs.com/gif.latex?%28g%28w%2Cxi%29%29%5E%7By%5E%7Bi%7D%7D) *![img](https://private.codecogs.com/gif.latex?%281-g%28w%2Cxi%29%29%5E%7B1-y%5E%7Bi%7D%7D)

	若想让预测出的结果全部正确的概率最大,根据最大似然估计([多元线性回归推理](https://blog.csdn.net/weixin_39445556/article/details/81416133)中有讲过,此处不再赘述),就是所有样本预测正确的概率相乘得到的P(总体正确)最大,此时我们让&lt;img src=&quot;https://img-blog.csdnimg.cn/20181110220832633.jpg&quot; alt=&quot;img&quot; style=&quot;zoom:50%;&quot; /&gt; ,数学表达形式如下:
</code></pre>
<img src="https://img-blog.csdnimg.cn/2018111022095526.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTQ0NTU1Ng==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 25%;" />
<p>​		上述公式最大时公式中W的值就是我们要的最好的W.下面对公式进行求解.</p>
<p>​     	我们知道,一个连乘的函数是不好计算的,我们可以通过两边同事取log的形式让其变成连加.</p>
<img src="https://img-blog.csdnimg.cn/2018111022143681.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTQ0NTU1Ng==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:33%;" />
<p>得到的这个函数越大,证明我们得到的W就越好.因为在函数最优化的时候习惯让一个函数越小越好,所以我们在前边加一个负号.得到公式如下:</p>
<p><img src="https://img-blog.csdnimg.cn/20181110222213701.jpg" alt="img"></p>
<p>这个函数就是我们逻辑回归(logistics regression)的损失函数,我们叫它<strong>交叉熵损失函数</strong>.</p>
<h2 id="七-求解交叉熵损失函数"><strong>七.求解交叉熵损失函数</strong></h2>
<p>​		求解损失函数的办法我们还是使用梯度下降,同样在<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39445556/article/details/83661219">批量梯度下降与随机梯度下降</a>一节有详细写到,此处我们只做简要概括.</p>
<p>求解步骤如下:</p>
<p>​        1-随机一组W.</p>
<p>​        2-将W带入交叉熵损失函数,让得到的点沿着负梯度的方向移动.</p>
<p>​        3-循环第二步.</p>
<p>​    求解梯度部分同样是对损失函数求偏导,过程如下:</p>
<p><img src="https://img-blog.csdnimg.cn/20181110223349772.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTQ0NTU1Ng==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>交叉熵损失函数的梯度和最小二乘的梯度形式上完全相同,区别在于,此时的<img src="https://private.codecogs.com/gif.latex?h_%7B%5CTheta%7D%28x%29%20%3D%20g%28z%29" alt="h_{\Theta}(x) = g(z)">。而最小二乘的<img src="https://private.codecogs.com/gif.latex?h_%7B%5CTheta%7D%20%3D%20W%5E%7BT%7DX" alt="h_{\Theta} = W^{T}X"></p>
<p>PS:加一个总结:逻辑回归为什么对切斜的数据特别敏感(正负例数据比例相差悬殊时预测效果不好)</p>
<p>​    首先从文章开头部分举例的两个图可以看到,使用线性模型进行分类第一个要面对的问题就是如何降低离群值的影响,而第二大问题就是,在正负例数据比例相差悬殊时预测效果不好.为什么会出现这种情况呢?原因来自于逻辑回归交叉熵损失函数是通过最大似然估计来推导出的.</p>
<p>​		使用最大似然估计来推导损失函数,那无疑,我们得到的结果就是所有样本被预测正确的最大概率.注意重点是我们得到的结果是预测正确率最大的结果,100个样本预测正确90个和预测正确91个的两组w,我们会选正确91个的这一组.那么,当我们的业务场景是来预测垃圾邮件,预测黄色图片时,我们数据中99%的都是负例(不是垃圾邮件不是黄色图片),如果有两组w,第一组为所有的负例都预测正确,而正利预测错误,正确率为99%,第二组是正利预测正确了,但是负例只预测出了97个,正确率为98%.此时我们算法会认为第一组w是比较好的.但实际我们业务需要的是第二组,因为正例检测结果才是业务的根本.</p>
<p>​		此时我们需要对数据进行欠采样/重采样来让正负例保持一个差不多的平衡,或者使用树型算法来做分类.一般树型分类的算法对数据倾斜并不是很敏感,但我们在使用的时候还是要对数据进行欠采样/重采样来观察结果是不是有变好.</p>
<p>————————————————<br>
版权声明：本文为CSDN博主「winrar_setup.rar」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>
原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39445556/article/details/83930186">https://blog.csdn.net/weixin_39445556/article/details/83930186</a></p>
<h2 id="python代码参考">python代码参考</h2>
<h3 id="普通逻辑回归">普通逻辑回归</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	data = numpy.loadtxt(<span class="string">&#x27;logicData2.txt&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    x, y = data[:, :-<span class="number">1</span>], data[:, -<span class="number">1</span>]</span><br><span class="line">    X = (x - numpy.mean(x, axis=<span class="number">0</span>)) / numpy.std(x, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    onex, y = numpy.c_[numpy.ones(<span class="built_in">len</span>(X)), X], numpy.c_[y]</span><br><span class="line">    m, n = onex.shape</span><br><span class="line">    alpha, iter0 = <span class="number">0.1</span>, <span class="number">20000</span></span><br><span class="line">    theta, J = numpy.zeros((n, <span class="number">1</span>)), numpy.zeros(iter0)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter0):</span><br><span class="line">        z = numpy.dot(onex, theta)</span><br><span class="line">        h = <span class="number">1</span> / (<span class="number">1</span> + numpy.exp(-z))</span><br><span class="line">        J[i] = (-<span class="number">1</span> / m) * numpy.<span class="built_in">sum</span>(y * numpy.log(h) + (<span class="number">1</span> - y) * numpy.log(<span class="number">1</span> - h))</span><br><span class="line">        grad = (<span class="number">1</span> / m) * numpy.dot(onex.T, h - y)	</span><br><span class="line">        theta -= alpha * grad</span><br><span class="line">    <span class="built_in">print</span>(theta)</span><br></pre></td></tr></table></figure>
<h3 id="多元逻辑回归">多元逻辑回归</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = numpy.loadtxt(<span class="string">&#x27;logicData2.txt&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    x, y = data[:, :-<span class="number">1</span>], data[:, -<span class="number">1</span>]</span><br><span class="line">    X = (x - numpy.mean(x, axis=<span class="number">0</span>)) / numpy.std(x, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    onex, y = numpy.c_[numpy.ones(<span class="built_in">len</span>(X)), X], numpy.c_[y]</span><br><span class="line">    m, n = onex.shape</span><br><span class="line">    alpha, iter0 = <span class="number">0.1</span>, <span class="number">20000</span></span><br><span class="line">    theta, J = numpy.zeros((n, <span class="number">1</span>)), numpy.zeros(iter0)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter0):</span><br><span class="line">	    z = numpy.dot(onex, theta)</span><br><span class="line">    	h = <span class="number">1</span> / (<span class="number">1</span> + numpy.exp(-z))</span><br><span class="line">		J[i] = (-<span class="number">1</span> / m) * numpy.<span class="built_in">sum</span>(y * numpy.log(h) + (<span class="number">1</span> - y) * numpy.log(<span class="number">1</span> - h)</span><br><span class="line"> 	    grad = (<span class="number">1</span> / m) * numpy.dot(onex.T, h - y)</span><br><span class="line">        theta -= alpha * grad</span><br><span class="line">	<span class="built_in">print</span>(theta)</span><br><span class="line">	<span class="built_in">print</span>(J)</span><br></pre></td></tr></table></figure>
<h3 id="非线性逻辑回归">非线性逻辑回归</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">	data = numpy.loadtxt(<span class="string">&#x27;logicData.txt&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">	<span class="comment"># 多项式的次数</span></span><br><span class="line">	d = <span class="number">3</span></span><br><span class="line">	<span class="comment"># 数据处理</span></span><br><span class="line">	x, y = data[:, :-<span class="number">1</span>], data[:, -<span class="number">1</span>:]</span><br><span class="line">	<span class="comment"># 特征缩放</span></span><br><span class="line">	X = (x - numpy.mean(x)) / numpy.std(x, axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">	<span class="comment"># onex的拼接</span></span><br><span class="line">	onex = numpy.c_[numpy.ones(<span class="built_in">len</span>(X))]</span><br><span class="line">	y = numpy.c_[y]</span><br><span class="line">	<span class="comment"># onex = numpy.c_[numpy.ones(len(X)), X[:, 0] ** 1, X[:, 0] ** 2, X[:, 0] ** 3]</span></span><br><span class="line">	<span class="comment"># 拼接x1^i</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, d + <span class="number">1</span>):</span><br><span class="line">    	onex = numpy.c_[onex, X[:, <span class="number">0</span>] ** i]</span><br><span class="line">	<span class="comment"># 拼接x2</span></span><br><span class="line">	onex = numpy.c_[onex, X[:, <span class="number">1</span>]]</span><br><span class="line">	m, n = onex.shape</span><br><span class="line">	alpha, iter0 = <span class="number">0.1</span>, <span class="number">20000</span></span><br><span class="line">	theta, J = numpy.zeros((n, <span class="number">1</span>)), numpy.zeros(iter0)</span><br><span class="line">	<span class="comment"># 梯度下降</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter0):</span><br><span class="line">   		z = numpy.dot(onex, theta)  <span class="comment"># 公式一</span></span><br><span class="line">    	h = <span class="number">1</span> / (<span class="number">1</span> + numpy.exp(-z))  <span class="comment"># 公式一</span></span><br><span class="line">    	J[i] = (-<span class="number">1</span> / m) * numpy.<span class="built_in">sum</span>(y * numpy.log(h) + (<span class="number">1</span> - y) * numpy.log(<span class="number">1</span> - h))  <span class="comment"># 公式二</span></span><br><span class="line">    	grad = (<span class="number">1</span> / m) * numpy.dot(onex.T, h - y)  <span class="comment"># 公式三</span></span><br><span class="line">    	theta -= alpha * grad  <span class="comment"># 公式三</span></span><br><span class="line">	<span class="built_in">print</span>(theta)</span><br><span class="line">	<span class="built_in">print</span>(J)</span><br><span class="line"></span><br><span class="line">	count = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    	<span class="comment"># numpy.where:条件函数，判断h是不是大于0.5.</span></span><br><span class="line">    	<span class="comment"># 如果是，则返回1，如果不是则返回0</span></span><br><span class="line">    	<span class="keyword">if</span> (numpy.where(h[i] &gt;= <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) == y[i]):</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">	rate = count / m</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;分类准确率为&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(rate))</span><br><span class="line"></span><br><span class="line">	plt.subplot(<span class="number">121</span>)</span><br><span class="line">	plt.plot(J)</span><br><span class="line">	plt.subplot(<span class="number">122</span>)</span><br><span class="line">	<span class="comment"># 利用布尔型索引绘制散点图</span></span><br><span class="line">	plt.scatter(X[y[:, <span class="number">0</span>] == <span class="number">1</span>, <span class="number">0</span>], X[y[:, <span class="number">0</span>] == <span class="number">1</span>, <span class="number">1</span>], c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">	plt.scatter(X[y[:, <span class="number">0</span>] == <span class="number">0</span>, <span class="number">0</span>], X[y[:, <span class="number">0</span>] == <span class="number">0</span>, <span class="number">1</span>], c=<span class="string">&#x27;g&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">	<span class="comment"># 绘制分割线</span></span><br><span class="line">	x1 = numpy.sort(X[:, <span class="number">0</span>])</span><br><span class="line">	x2 = numpy.zeros(<span class="built_in">len</span>(y))</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(d + <span class="number">1</span>):</span><br><span class="line">    	x2 += theta[i] * (x1 ** i)</span><br><span class="line">	x2 = -x2 / theta[-<span class="number">1</span>]</span><br><span class="line">	plt.plot(x1, x2, c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">	plt.grid(<span class="literal">True</span>)</span><br><span class="line">	plt.show()</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io">CJH</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io/2022/09/08/LR%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/">http://cjh0220.github.io/2022/09/08/LR%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://cjh0220.github.io" target="_blank">CJH's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归</a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/08/%E6%95%B0%E6%A8%A1%E5%9B%BD%E8%B5%9B%E5%BD%93%E5%A4%A9%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B/"><img class="prev-cover" src="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">数模国赛当天推荐流程</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/08/%E4%BA%8C%E5%88%86%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"><img class="next-cover" src="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">二分类评价指标</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJH</div><div class="author-info__description">Hello,my friends</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">107</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">85</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cjh0220"><i class="fab fa-github"></i><span>Gihhub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cjh0220" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1005741898@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">LR逻辑回归(logistics regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.1.</span> <span class="toc-text">一.分类和回归任务的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8D%E6%98%AF%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">二.逻辑回归不是回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89-%E6%80%8E%E4%B9%88%E5%81%9A"><span class="toc-number">1.2.1.</span> <span class="toc-text">三.怎么做</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-%E6%8A%8A%E5%9B%9E%E5%BD%92%E5%87%BD%E6%95%B0%E6%8E%B0%E5%BC%AF"><span class="toc-number">1.3.</span> <span class="toc-text">四:把回归函数掰弯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-%E9%80%89%E5%AE%9A%E9%98%88%E5%80%BC"><span class="toc-number">1.4.</span> <span class="toc-text">五:选定阈值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.5.</span> <span class="toc-text">六.最大似然估计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-%E6%B1%82%E8%A7%A3%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.6.</span> <span class="toc-text">七.求解交叉熵损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#python%E4%BB%A3%E7%A0%81%E5%8F%82%E8%80%83"><span class="toc-number">1.7.</span> <span class="toc-text">python代码参考</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%AE%E9%80%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.7.1.</span> <span class="toc-text">普通逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.7.2.</span> <span class="toc-text">多元逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.7.3.</span> <span class="toc-text">非线性逻辑回归</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/08/08/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%9B%9B%EF%BC%893DGS%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85/" title="论文精读（四）3DGS高斯泼溅"><img src="https://s21.ax1x.com/2024/08/08/pkzVbMd.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读（四）3DGS高斯泼溅"/></a><div class="content"><a class="title" href="/2024/08/08/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E5%9B%9B%EF%BC%893DGS%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85/" title="论文精读（四）3DGS高斯泼溅">论文精读（四）3DGS高斯泼溅</a><time datetime="2024-08-08T06:32:59.000Z" title="发表于 2024-08-08 14:32:59">2024-08-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/07/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E4%B8%89%EF%BC%89NeRF%E7%A5%9E%E7%BB%8F%E8%BE%90%E5%B0%84%E5%9C%BA/" title="论文精读（三）NeRF神经辐射场"><img src="https://s3.bmp.ovh/imgs/2024/08/07/1abb397a4866f0c4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读（三）NeRF神经辐射场"/></a><div class="content"><a class="title" href="/2024/08/07/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%88%E4%B8%89%EF%BC%89NeRF%E7%A5%9E%E7%BB%8F%E8%BE%90%E5%B0%84%E5%9C%BA/" title="论文精读（三）NeRF神经辐射场">论文精读（三）NeRF神经辐射场</a><time datetime="2024-08-07T03:07:54.000Z" title="发表于 2024-08-07 11:07:54">2024-08-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/07/Ablation-study%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C/" title="Ablation_study消融实验"><img src="https://s21.ax1x.com/2024/08/07/pkx6rLQ.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ablation_study消融实验"/></a><div class="content"><a class="title" href="/2024/08/07/Ablation-study%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C/" title="Ablation_study消融实验">Ablation_study消融实验</a><time datetime="2024-08-07T02:44:06.000Z" title="发表于 2024-08-07 10:44:06">2024-08-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/06/nnU-net/" title="nnU-net"><img src="https://s21.ax1x.com/2024/08/06/pkxlEYq.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="nnU-net"/></a><div class="content"><a class="title" href="/2024/08/06/nnU-net/" title="nnU-net">nnU-net</a><time datetime="2024-08-06T09:03:41.000Z" title="发表于 2024-08-06 17:03:41">2024-08-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/06/U-net/" title="U-net"><img src="https://s21.ax1x.com/2024/08/07/pkxg5VJ.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="U-net"/></a><div class="content"><a class="title" href="/2024/08/06/U-net/" title="U-net">U-net</a><time datetime="2024-08-06T09:01:32.000Z" title="发表于 2024-08-06 17:01:32">2024-08-06</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s1.ax1x.com/2022/09/08/vqpCng.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By CJH</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">欢迎欢迎，热烈欢迎</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>