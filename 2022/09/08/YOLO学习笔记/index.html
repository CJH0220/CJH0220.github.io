<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>YOLO学习笔记 | CJH's blog</title><meta name="keywords" content="YOLO"><meta name="author" content="CJH"><meta name="copyright" content="CJH"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="推荐博客 从yolov1至yolov4的进阶之路 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (上) 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (中) 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (下) 写给小白的Yolo介绍 比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fa">
<meta property="og:type" content="article">
<meta property="og:title" content="YOLO学习笔记">
<meta property="og:url" content="http://cjh0220.github.io/2022/09/08/YOLO%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="CJH&#39;s blog">
<meta property="og:description" content="推荐博客 从yolov1至yolov4的进阶之路 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (上) 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (中) 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (下) 写给小白的Yolo介绍 比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fa">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1.ax1x.com/2022/09/08/vqpCng.jpg">
<meta property="article:published_time" content="2022-09-08T14:25:58.000Z">
<meta property="article:modified_time" content="2022-09-08T14:40:45.307Z">
<meta property="article:author" content="CJH">
<meta property="article:tag" content="YOLO">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2022/09/08/vqpCng.jpg"><link rel="shortcut icon" href="/img/CJH.png"><link rel="canonical" href="http://cjh0220.github.io/2022/09/08/YOLO%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'YOLO学习笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-08 22:40:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">59</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s1.ax1x.com/2022/09/08/vqpCng.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">CJH's blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">YOLO学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-08T14:25:58.000Z" title="发表于 2022-09-08 22:25:58">2022-09-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-08T14:40:45.307Z" title="更新于 2022-09-08 22:40:45">2022-09-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="YOLO学习笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>推荐博客</h1>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wjinjie/article/details/107509243?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163249930316780271530764%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163249930316780271530764&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-107509243.pc_search_result_cache&amp;utm_term=yolo&amp;spm=1018.2226.3001.4187">从yolov1至yolov4的进阶之路</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/183261974">你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (上)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/183781646">你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (中)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/186014243">你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (下)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94986199">写给小白的Yolo介绍</a></p>
<p>比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。</p>
<h1>YOLO（You Only Look Once）</h1>
<img src="https://pjreddie.com/media/image/yologo_2.png" alt="img" style="zoom: 25%;" />
<p><strong>YOLO v1：直接回归出位置。</strong></p>
<p><strong>YOLO v2：全流程多尺度方法。</strong></p>
<p><strong>YOLO v3：多尺度检测头，resblock darknet53</strong></p>
<p><strong>YOLO v4：cspdarknet53，spp，panet，tricks</strong></p>
<h2 id="0-前言">0 前言</h2>
<p>本文目的是用尽量浅显易懂的语言让零基础小白能够理解什么是YOLO系列模型，以及他们的设计思想和改进思路分别是什么。我不会把YOLO的论文给你用软件翻译一遍，这样做毫无意义；也不会使用太专业晦涩的名词和表达，对于每一个新的概念都会解释得尽量通俗一些，目的是使得你能像看故事一样学习YOLO模型，我觉得这样的学习方式才是知乎博客的意义所在。</p>
<p>为了使本文<strong>尽量生动有趣</strong>，我用葫芦娃作为例子展示YOLO的过程(真的是尽力了。。。)。</p>
<p><img src="https://pic2.zhimg.com/80/v2-6f33f7b365e952d0cb1436c540135a0d_720w.jpg" alt="img"></p>
<p>葫芦娃</p>
<p>同时，会对<strong>YOLO v1和YOLOv5</strong>的代码进行解读，其他的版本就只介绍改进了。</p>
<hr>
<h2 id="1-先从一款强大的app说起">1 先从一款强大的app说起</h2>
<p><img src="https://pic1.zhimg.com/80/v2-6821e6ed09cedb367cde596fbfcb4328_720w.jpg" alt="img">i detection APP</p>
<p><strong>YOLO v5</strong>其实一开始是以一款<strong>app</strong>进入人们的视野的，就是上图的这个，叫：<strong>i detection</strong>(图上标的是YOLO v4，但其实算法是YOLO v5)，使用<strong>iOS</strong>系列的小伙伴呢，就可以立刻<strong>点赞后</strong>关掉我这篇文章，去下载这个app玩一玩。在任何场景下(工业场景，生活场景等等)都可以试试这个app和这个算法，这个app中间还有一个<strong>button</strong>，来调节app使用的模型的大小，更大的模型实时性差但精度高，更小的模型实时性好但精度差。</p>
<p>值得一提的是，这款app就是<strong>YOLO v5的作者</strong>亲自完成的。而且，我写这篇文章的时候YOLO v5的论文还没有出来，还在实验中，等论文出来应该是2020年底或者2021年初了。</p>
<p><strong>读到这里，你觉得YOLO v5的最大特点是什么？</strong></p>
<p><strong>答案就是：一个字：快</strong>，应用于移动端，模型小，速度快。</p>
<p>首先我个人觉得任何一个模型都有下面3部分组成：</p>
<ul>
<li><strong>前向传播部分：90%</strong></li>
<li><strong>损失函数部分</strong></li>
<li><strong>反向传播部分</strong></li>
</ul>
<p>其中前向传播部分占用的时间应该在90%左右，即搞清楚前向传播部分也就搞清楚了这模型的实现流程和细节。本着这一原则，我们开始YOLO系列模型的解读：</p>
<hr>
<h2 id="2-不得不谈的分类模型">2 不得不谈的分类模型</h2>
<p>在进入目标检测任务之前首先得学会图像分类任务，这个任务的特点是输入一张图片，输出是它的类别。</p>
<p>对于<strong>输入图片</strong>，我们一般用一个矩阵表示。</p>
<p>对于<strong>输出结果</strong>，我们一般用一个one-hot vector表示： <img src="https://www.zhihu.com/equation?tex=%5B0%2C0%2C1%2C0%2C0%2C0%5D" alt="[公式]"> ，哪一维是1，就代表图片属于哪一类。</p>
<p>所以，在设计神经网络时，结构大致应该长这样：</p>
<p>img <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp16<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp32<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp64<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp128<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> …<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> fc256-fc[10]</p>
<p>这里的cbrp指的是conv，bn，relu，pooling的串联。</p>
<p>由于输入要是one-hot形式，所以最后我们设计了2个fc层(fully connencted layer)，我们称之为**“分类头”<strong>或者</strong>“决策层”**。</p>
<hr>
<h2 id="3-YOLO系列思想的雏形：YOLO-v0">3 YOLO系列思想的雏形：YOLO v0</h2>
<p>有了上面的分类器，我们能不能用它来做检测呢？</p>
<p>要回答这个问题，首先得看看检测器和分类器的输入输出有什么不一样。首先他们的输入都是image，但是分类器的输出是一个one-hot vector，而检测器的输出是一个框(Bounding Box)。</p>
<p><strong>框</strong>，该怎么表示？</p>
<p>在一个图片里面表示一个框，有很多种方法，比如：</p>
<p><img src="https://pic3.zhimg.com/80/v2-5d8895c050a05b9ce813b5289ec2a4da_720w.jpg" alt="img">x,y,w,h表示一个框</p>
<ul>
<li><strong>x,y,w,h</strong>(如上图)</li>
<li><strong>p1,p2,p3,p4</strong>(4个点坐标)</li>
<li><strong>cx,cy,w,h</strong>(cx,cy为中心点坐标)</li>
<li><strong>x,y,w,h</strong>,<strong>angle</strong>(还有的目标是有角度的，这时叫做Rotated Bounding Box)</li>
<li>…</li>
</ul>
<p>所以表示的方法不是一成不变的，但你会发现：不管你用什么形式去表达这个Bounding Box，你模型输出的结果一定是一个vector，那这个vector和分类模型输出的vector本质上有什么区别吗？</p>
<p><strong>答案是：没有</strong>，都是向量而已，只是分类模型输出是one-hot向量，检测模型输出是我们标注的结果。</p>
<p>所以你应该会发现，检测的方法呼之欲出了。<strong>那分类模型可以用来做检测吗？</strong></p>
<p><strong>当然可以，<strong>这时，你可以把检测的任务当做是</strong>遍历性的分类任务。</strong></p>
<p><strong>如何遍历？</strong></p>
<p><strong>我们的目标是一个个框，那就用这个框去遍历所有的位置，所有的大小。</strong></p>
<p>比如下面这张图片，我需要你检测葫芦娃的脸，如图1所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-a1843eda0e8f2c6ea8cddb5180da1da3_720w.jpg" alt="img">图1：检测葫芦娃的脸</p>
<p>我们可以对边框的区域进行二分类：属于头或者不属于头。</p>
<p>你先预设一个框的大小，然后在图片上遍历这个框，比如第一行全都不是头。第4个框只有一部分目标在，也不算。第5号框算是一个头，我们记住它的位置。这样不断地滑动，就是遍历性地分类。</p>
<p>接下来要遍历框的大小：因为你刚才是预设一个框的大小，但葫芦娃的头有大有小，你还得遍历框的大小，如下图2所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-9c3d5b1c85adf80791780697b9ad8ecb_720w.jpg" alt="img">图2：遍历框的大小</p>
<p>还没有结束，刚才滑窗时是挨个滑，但其实没有遍历所有的位置，更精确的遍历方法应该如下图3所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-eda9abf77cb3597a41cc460c6f4f6d0d_720w.jpg" alt="img"></p>
<p>图3：更精确地遍历框的位置</p>
<p>这种方法其实就是RCNN全家桶的初衷，专业术语叫做：<strong>滑动窗口分类方法</strong>。</p>
<p>现在需要你思考一个问题：<strong>这种方法的精确和什么因素有关？</strong></p>
<p><strong>答案是：遍历得彻不彻底</strong>。遍历得越精确，检测器的精度就越高。所以这也就带来一个问题就是：<strong>检测的耗时非常大</strong>。</p>
<p>举个例子：比如输入图片大小是(800,1000)也就意味着有800000个位置。窗口大小最小 <img src="https://www.zhihu.com/equation?tex=%281%5Ctimes1%29" alt="[公式]"> ,最大 <img src="https://www.zhihu.com/equation?tex=%28800%5Ctimes1000%29" alt="[公式]"> ，所以这个遍历的次数是<strong>无限次</strong>。我们看下伪代码：</p>
<p><img src="https://pic4.zhimg.com/80/v2-72224eb25b6bcfce14d31a5a571422f3_720w.jpg" alt="img"></p>
<p>滑动窗口分类方法伪代码</p>
<p>那这种方法如何训练呢？</p>
<p>本质上还是训练一个二分类器。这个二分类器的<strong>输入是一个框的内容，输出是(前景/背景)</strong>。</p>
<p><strong>第1个问题：</strong></p>
<p>框有不同的大小，对于不同大小的框，输入到相同的二分类器中吗？</p>
<p>是的。要先把不同大小的input归一化到统一的大小。</p>
<p><strong>第2个问题：</strong></p>
<p>背景图片很多，前景图片很少：二分类样本不均衡。</p>
<p>确实是这样，你看看一张图片有多少框对应的是背景，有多少框才是葫芦娃的头。</p>
<p>以上就是传统检测方法的主要思路：</p>
<ul>
<li>耗时。</li>
<li>操作复杂，需要手动生成大量的样本。</li>
</ul>
<p><strong>到现在为止，我们用分类的算法设计了一个检测器，它存在着各种各样的问题，现在是优化的时候了(接下来正式进入YOLO系列方法了)：</strong></p>
<p>YOLO的作者当时是这么想的：你分类器输出一个one-hot vector，那我把它换成**(x,y,w,h,c)**，c表示confidence置信度，把问题转化成一个回归问题，直接回归出Bounding Box的位置不就好了吗？</p>
<p>刚才的分类器是：img <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp16<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp32<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp64<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp128<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> …<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> fc256-fc[10]</p>
<p>现在我变成：img <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp16<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp32<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp64<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp128<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> …<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> fc256-fc[5]，这个输出是**(x,y,w,h,c)**，不就变成了一个检测器吗？</p>
<p><strong>本质上都是矩阵映射到矩阵，只是代表的意义不一样而已。</strong></p>
<blockquote>
<p>传统的方法为什么没有这么做呢？我想肯定是效果不好，终其原因是算力不行，conv操作还没有推广。</p>
</blockquote>
<p>好，现在模型是：</p>
<p>img <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp16<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp32<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp64<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp128<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> …<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> fc256-fc[5] <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> c,x,y,w,h</p>
<p><img src="https://pic2.zhimg.com/80/v2-833df61bdd1b13d0647acf5fbe139339_720w.jpg" alt="img"></p>
<p>那如何组织训练呢？找1000张图片，把label设置为 <img src="https://www.zhihu.com/equation?tex=%281%2Cx%5E%7B%2A%7D%2Cy%5E%7B%2A%7D%2Cw%5E%7B%2A%7D%2Ch%5E%7B%2A%7D%29" alt="[公式]"> 。这里 <img src="https://www.zhihu.com/equation?tex=x%5E%7B%2A%7D" alt="[公式]"> 代表真值。有了数据和标签，就完成了设计。</p>
<p>我们会发现，这种方法比刚才的<strong>滑动窗口分类方法</strong>简单太多了。这一版的思路我把它叫做<strong>YOLO v0</strong>，因为它是<strong>You Only Look Once</strong>最简单的版本。</p>
<hr>
<h2 id="4-YOLO-v1终于诞生">4 YOLO v1终于诞生</h2>
<ul>
<li><strong>需求1：YOLO v0只能输出一个目标，那比如下图4的多个目标怎么办呢？</strong></li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-bdf872e1bc2ac749425d3e45b123b3ec_720w.jpg" alt="img"></p>
<p>图4：多个目标情况</p>
<p>你可能会回答：我输出N个向量不就行了吗？但具体输出多少个合适呢？图4有7个目标，那有的图片有几百个目标，你这个N又该如何调整呢？</p>
<p><strong>答：为了保证所有目标都被检测到，我们应该输出尽量多的目标。</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-de3aec28683d4e58fca1218ddfaff670_720w.jpg" alt="img">输出尽量多的目标</p>
<p>但这种方法也不是最优的，最优的应该是下图这样：</p>
<p><img src="https://pic3.zhimg.com/80/v2-0c9a23e1c1740d62ccf50cdc97b0bcb2_720w.jpg" alt="img"></p>
<p>图5：用一个(c,x,y,w,h)去负责image某个区域的目标</p>
<p>如图5所示：用一个(c,x,y,w,h)去负责image某个区域的目标。</p>
<p>比如说图片设置为16个区域，每个区域用1个(c,x,y,w,h)去负责：</p>
<p><img src="https://pic2.zhimg.com/80/v2-a6230457e694322c04998b7d0536a889_720w.jpg" alt="img">图6：图片设置为16个区域</p>
<p>就可以一次输出16个框，每个框是1个(c,x,y,w,h)，如图6所示。</p>
<p>为什么这样子更优？因为conv操作是位置强相关的，就是原来的目标在哪里，你conv之后的feature map上还在哪里，所以图片划分为16个区域，结果也应该分布在16个区域上，所以我们的<strong>结果(Tensor)的维度size是：(5,4,4)</strong>。</p>
<p>那现在你可能会问：<strong>c的真值该怎么设置呢？</strong></p>
<p>**答：**看葫芦娃的大娃，他的脸跨了4个区域(grid)，但只能某一个grid的c=1，其他的c=0。那么该让哪一个grid的c=1呢？就看他的脸的中心落在了哪个grid里面。根据这一原则，c的真值为下图7所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-02a45bc027de8ff65693bcf1ae21c2e3_720w.jpg" alt="img">图7：c的label值</p>
<p>但是你发现7个葫芦娃只有6个1，原因是某一个grid里面有2个目标，确实如此，第三行第三列的grid既有<strong>水娃</strong>又有<strong>隐身娃</strong>。这种一个区域有多个目标的情况我们<strong>目前没法解决，因为我们的模型现在能力就这么大，只能在一个区域中检测出一个目标，如何改进我们马上就讨论，你可以现在先自己想一想。</strong></p>
<p>总之现在我们设计出了模型的输出结果，那距离完成模型的设计还差一个损失函数，那Loss咋设计呢？看下面的伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss = 0</span><br><span class="line">for img in img_all:</span><br><span class="line">   for i in range(4):</span><br><span class="line">      for j in range(4):</span><br><span class="line">         loss_ij = lamda_1*(c_pred-c_label)**2 + c_label*(x_pred-x_label)**2 +\</span><br><span class="line">                     c_label*(y_pred-y_label)**2 + c_label*(w_pred-w_label)**2 + \</span><br><span class="line">                     c_label*(h_pred-h_label)**2</span><br><span class="line">         loss += loss_ij</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>遍历所有图片，遍历所有位置，计算loss。</p>
</blockquote>
<ul>
<li>好现在模型设计完了，回到刚才的问题：模型现在能力就这么大，只能在一个区域中检测出一个目标，如何改进？</li>
</ul>
<p>**答：**刚才区域是 <img src="https://www.zhihu.com/equation?tex=4%5Ctimes4" alt="[公式]"> ，现在变成 <img src="https://www.zhihu.com/equation?tex=40%5Ctimes40" alt="[公式]"> ，或者更大，使区域更密集，就可以缓解多个目标的问题，但无法从根本上去解决。</p>
<ul>
<li>另一个问题，按上面的设计你检测得到了16个框，可是图片上只有7个葫芦娃的脸，怎么从16个结果中筛选出7个我们要的呢？</li>
</ul>
<p><strong>答：</strong></p>
<p>**法1：聚类。**聚成7类，在这7个类中，选择confidence最大的框。听起来挺好。</p>
<p>**法1的bug：**2个目标本身比较近聚成了1个类怎么办？如果不知道到底有几个目标呢？为何聚成7类？不是3类？</p>
<p>**法2：NMS(非极大值抑制)。**2个框重合度很高，大概率是一个目标，那就只取一个框。</p>
<p>重合度的计算方法：交并比IoU=两个框的交集面积/两个框的并集面积。</p>
<p>具体算法：</p>
<p><img src="https://pic2.zhimg.com/80/v2-f2cf821fc64f14d2b60ae2f61409e3f5_720w.jpg" alt="img"></p>
<p>面试必考的NMS</p>
<p>**法1的bug：**2个目标本身比较近怎么办？依然没有解决。</p>
<p>如果不知道到底有几个目标呢？NMS自动解决了这个问题。</p>
<p>**面试的时候会问这样一个问题：**NMS的适用情况是什么？</p>
<p>**答：**1图多目标检测时用NMS。</p>
<p>到现在为止我们终于解决了第4节开始提出的多个目标的问题，现在又有了新的需求：</p>
<ul>
<li><strong>需求2：多类的目标怎么办呢？</strong></li>
</ul>
<p>比如说我现在既要你检测<strong>葫芦娃的脸</strong>，又要你检测<strong>葫芦娃的葫芦</strong>，怎么设计？</p>
<p>img <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp16<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp32<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp64<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp128<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> …<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> fc256-fc[5+2]*N <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> [c,x,y,w,h,one-hot]*N</p>
<p>2个类，one-hot就是[0,1],[1,0]这样子，如下图8所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-38ce7b4f57bc0833a66a6baf5559da99_720w.jpg" alt="img"></p>
<p>图8：多类的目标的label</p>
<p>伪代码依然是：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">loss = 0</span><br><span class="line">for img in img_all:</span><br><span class="line">   for i in range(3):</span><br><span class="line">      for j in range(4):</span><br><span class="line">         c_loss = lamda_1*(c_pred-c_label)**2</span><br><span class="line">         geo_loss = c_label*(x_pred-x_label)**2 +\</span><br><span class="line">                     c_label*(y_pred-y_label)**2 + c_label*(w_pred-w_label)**2 + \</span><br><span class="line">                     c_label*(h_pred-h_label)**2</span><br><span class="line">         class_loss = 1/m * mse_loss(p_pred, p_label)</span><br><span class="line">         loss_ij =c_loss  + geo_loss + class_loss</span><br><span class="line">         loss += loss_ij</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<p>至此，多个类的问题也解决了，现在又有了新的需求：</p>
<ul>
<li><strong>需求3：小目标检测怎么办呢？</strong></li>
</ul>
<p>小目标总是检测不佳，所以我们专门设计神经元去拟合小目标。</p>
<p><img src="https://pic2.zhimg.com/80/v2-efb58a2c9d3e881df099789f640471ad_720w.jpg" alt="img"></p>
<p>图9：多类的小目标的label，分别预测大目标和小目标</p>
<p>对于每个区域，我们用2个五元组(c,x,y,w,h)，一个负责回归大目标，一个负责回归小目标，同样添加one-hot vector，one-hot就是[0,1],[1,0]这样子，来表示属于哪一类(葫芦娃的头or葫芦娃的葫芦)。</p>
<p>伪代码变为了：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">loss = 0</span><br><span class="line">for img in img_all:</span><br><span class="line">   for i in range(3):</span><br><span class="line">      for j in range(4):</span><br><span class="line">         c_loss = lamda_1*(c_pred-c_label)**2</span><br><span class="line">         geo_loss = c_label_big*(x_big_pred-x_big_label)**2 +\</span><br><span class="line">                     c_label_big*(y_big_pred-y_big_label)**2 + c_label_big*(w_big_pred-w_big_label)**2 + \</span><br><span class="line">                     c_label_big*(h_big_pred-h_big_label)**2 +\</span><br><span class="line">                     c_label_small*(x_small_pred-x_small_label)**2 +\</span><br><span class="line">                     c_label_small*(y_small_pred-y_small_label)**2 + c_label_small*(w_small_pred-w_small_label)**2 + \</span><br><span class="line">                     c_label_small*(h_small_pred-h_small_label)**2</span><br><span class="line">         class_loss = 1/m * mse_loss(p_pred, p_label)</span><br><span class="line">         loss_ij =c_loss  + geo_loss + class_loss</span><br><span class="line">         loss += loss_ij</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<p>至此，小目标的问题也有了解决方案。</p>
<p>到这里，我们设计的检测器其实就是YOLO v1，只是有的参数跟它不一样，我们看论文里的图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-ce26d13cfd3b7145f4594524435a9b92_720w.jpg" alt="img"></p>
<p>YOLO v1</p>
<p><img src="https://pic4.zhimg.com/80/v2-3af308f7096bda4c621c077302b90533_720w.jpg" alt="img"></p>
<p>YOLO v1</p>
<p>YOLO v1其实就是把我们划分的16个区域变成了 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="[公式]"> 个区域，我们预测16个目标，YOLO v1预测49个目标。我们是2类(葫芦娃的头or葫芦娃的葫芦)，YOLO v1是20类。</p>
<p><img src="https://www.zhihu.com/equation?tex=30%3D5%2A2%28c_%7Bbig%7D%2Cx_%7Bbig%7D%2Cy_%7Bbig%7D%2Cw_%7Bbig%7D%2Ch_%7Bbig%7D%2C+c_%7Bsmall%7D%2Cx_%7Bsmall%7D%2Cy_%7Bsmall%7D%2Cw_%7Bsmall%7D%2Ch_%7Bsmall%7D%29%2B20+classes" alt="[公式]"></p>
<p>backbone也是一堆卷积+检测头(FC层)，所以说设计到现在，我们其实是把YOLO v1给设计出来了。</p>
<p>再看看作者的解释：</p>
<p><img src="https://pic2.zhimg.com/80/v2-6b90a56a5c57d2ed05ec620f4230ca45_720w.png" alt="img"></p>
<p>发现train的时候用的小图片，检测的时候用的是大图片(肯定是经过了无数次试验证明了效果好)。</p>
<p>结构学完了，再看loss函数，并比较下和我们设计的loss函数有什么区别。</p>
<p><img src="https://pic4.zhimg.com/80/v2-f81c565d41b681263689626331325ac3_720w.jpg" alt="img"></p>
<p>YOLO v1 loss函数</p>
<p><strong>解读一下这个损失函数：</strong></p>
<p>我们之前说的损失函数是设计了3个for循环，而作者为了方便写成了求和的形式：</p>
<ul>
<li>前2行计算前景的geo_loss。</li>
<li>第3行计算前景的confidence_loss。</li>
<li>第4行计算背景的confidence_loss。</li>
<li>第5行计算分类损失class_loss。</li>
</ul>
<p>伪代码上面已经有了，现在我们总体看一下这个模型：</p>
<p>img <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp192<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp256<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp512<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> cbrp1024<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> …<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> fc4096-fc[5+2]*N <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes30" alt="[公式]"></p>
<p>检测层的设计：回归坐标值+one-hot分类</p>
<p><img src="https://pic2.zhimg.com/80/v2-bf46a1073c877181e06cba19b8c20995_720w.jpg" alt="img"></p>
<p>检测层的设计</p>
<ul>
<li><strong>样本不均衡的问题解决了吗？</strong></li>
</ul>
<p>没有计算背景的geo_loss，只计算了前景的geo_loss，这个问题YOLO v1回避了，依然存在。</p>
<p><strong>最后我们解读下YOLO v1的代码：</strong></p>
<p><strong>1.模型定义：</strong></p>
<p>定义特征提取层：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">class VGG(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">       super(VGG,self).__init__()</span><br><span class="line">       # the vgg&#x27;s layers</span><br><span class="line">       #self.features = features</span><br><span class="line">       cfg = [64,64,&#x27;M&#x27;,128,128,&#x27;M&#x27;,256,256,256,&#x27;M&#x27;,512,512,512,&#x27;M&#x27;,512,512,512,&#x27;M&#x27;]</span><br><span class="line">       layers= []</span><br><span class="line">       batch_norm = False</span><br><span class="line">       in_channels = 3</span><br><span class="line">       for v in cfg:</span><br><span class="line">           if v == &#x27;M&#x27;:</span><br><span class="line">               layers += [nn.MaxPool2d(kernel_size=2,stride = 2)]</span><br><span class="line">           else:</span><br><span class="line">               conv2d = nn.Conv2d(in_channels,v,kernel_size=3,padding = 1)</span><br><span class="line">               if batch_norm:</span><br><span class="line">                   layers += [conv2d,nn.Batchnorm2d(v),nn.ReLU(inplace=True)]</span><br><span class="line">               else:</span><br><span class="line">                   layers += [conv2d,nn.ReLU(inplace=True)]</span><br><span class="line">               in_channels = v</span><br><span class="line">       # use the vgg layers to get the feature</span><br><span class="line">       self.features = nn.Sequential(*layers)</span><br><span class="line">       # 全局池化</span><br><span class="line">       self.avgpool = nn.AdaptiveAvgPool2d((7,7))</span><br><span class="line">       # 决策层：分类层</span><br><span class="line">       self.classifier = nn.Sequential(</span><br><span class="line">           nn.Linear(512*7*7,4096),</span><br><span class="line">           nn.ReLU(True),</span><br><span class="line">           nn.Dropout(),</span><br><span class="line">           nn.Linear(4096,4096),</span><br><span class="line">           nn.ReLU(True),</span><br><span class="line">           nn.Dropout(),</span><br><span class="line">           nn.Linear(4096,1000),</span><br><span class="line">       )</span><br><span class="line"></span><br><span class="line">       for m in self.modules():</span><br><span class="line">           if isinstance(m,nn.Conv2d):</span><br><span class="line">               nn.init.kaiming_normal_(m.weight,mode=&#x27;fan_out&#x27;,nonlinearity=&#x27;relu&#x27;)</span><br><span class="line">               if m.bias is not None: </span><br><span class="line">                   nn.init.constant_(m.bias,0)</span><br><span class="line">           elif isinstance(m,nn.BatchNorm2d):</span><br><span class="line">               nn.init.constant_(m.weight,1)</span><br><span class="line">               nn.init.constant_(m.bias,1)</span><br><span class="line">           elif isinstance(m,nn.Linear):</span><br><span class="line">               nn.init.normal_(m.weight,0,0.01)</span><br><span class="line">               nn.init.constant_(m.bias,0)</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">         x = self.features(x)</span><br><span class="line">         x_fea = x</span><br><span class="line">         x = self.avgpool(x)</span><br><span class="line">         x_avg = x</span><br><span class="line">         x = x.view(x.size(0),-1)</span><br><span class="line">         x = self.classifier(x)</span><br><span class="line">         return x,x_fea,x_avg</span><br><span class="line">    def extractor(self,x):</span><br><span class="line">         x = self.features(x)</span><br><span class="line">         return x</span><br></pre></td></tr></table></figure>
<p>定义检测头：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">self.detector = nn.Sequential(</span><br><span class="line">   nn.Linear(512*7*7,4096),</span><br><span class="line">   nn.ReLU(True),</span><br><span class="line">   nn.Dropout(),</span><br><span class="line">   nn.Linear(4096,1470),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>整体模型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">class YOLOV1(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">       super(YOLOV1,self).__init__()</span><br><span class="line">       vgg = VGG()</span><br><span class="line">       self.extractor = vgg.extractor</span><br><span class="line">       self.avgpool = nn.AdaptiveAvgPool2d((7,7))</span><br><span class="line">       # 决策层：检测层</span><br><span class="line">       self.detector = nn.Sequential(</span><br><span class="line">          nn.Linear(512*7*7,4096),</span><br><span class="line">          nn.ReLU(True),</span><br><span class="line">          nn.Dropout(),</span><br><span class="line">          #nn.Linear(4096,1470),</span><br><span class="line">          nn.Linear(4096,245),</span><br><span class="line">          #nn.Linear(4096,5),</span><br><span class="line">       )</span><br><span class="line">       for m in self.modules():</span><br><span class="line">           if isinstance(m,nn.Conv2d):</span><br><span class="line">               nn.init.kaiming_normal_(m.weight,mode=&#x27;fan_out&#x27;,nonlinearity=&#x27;relu&#x27;)</span><br><span class="line">               if m.bias is not None: </span><br><span class="line">                   nn.init.constant_(m.bias,0)</span><br><span class="line">           elif isinstance(m,nn.BatchNorm2d):</span><br><span class="line">               nn.init.constant_(m.weight,1)</span><br><span class="line">               nn.init.constant_(m.bias,1)</span><br><span class="line">           elif isinstance(m,nn.Linear):</span><br><span class="line">               nn.init.normal_(m.weight,0,0.01)</span><br><span class="line">               nn.init.constant_(m.bias,0)</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.extractor(x)</span><br><span class="line">        #import pdb</span><br><span class="line">        #pdb.set_trace()</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.size(0),-1)</span><br><span class="line">        x = self.detector(x)</span><br><span class="line">        b,_ = x.shape</span><br><span class="line">        #x = x.view(b,7,7,30)</span><br><span class="line">        x = x.view(b,7,7,5)</span><br><span class="line">        </span><br><span class="line">        #x = x.view(b,1,1,5)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>
<p>主函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    vgg = VGG()</span><br><span class="line">    x  = torch.randn(1,3,512,512)</span><br><span class="line">    feature,x_fea,x_avg = vgg(x)</span><br><span class="line">    print(feature.shape)</span><br><span class="line">    print(x_fea.shape)</span><br><span class="line">    print(x_avg.shape)</span><br><span class="line"> </span><br><span class="line">    yolov1 = YOLOV1()</span><br><span class="line">    feature = yolov1(x)</span><br><span class="line">    # feature_size b*7*7*30</span><br><span class="line">    print(feature.shape)</span><br></pre></td></tr></table></figure>
<p><strong>2.模型训练：</strong></p>
<p>主函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    train()</span><br></pre></td></tr></table></figure>
<p>下面看train()函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def train():</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        ts = time.time()</span><br><span class="line">        for iter, batch in enumerate(train_loader):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            # 取图片</span><br><span class="line">            inputs = input_process(batch)</span><br><span class="line">            # 取标注</span><br><span class="line">            labels = target_process(batch)</span><br><span class="line">            </span><br><span class="line">            # 获取得到输出</span><br><span class="line">            outputs = yolov1_model(inputs)</span><br><span class="line">            #import pdb</span><br><span class="line">            #pdb.set_trace()</span><br><span class="line">            #loss = criterion(outputs, labels)</span><br><span class="line">            loss,lm,glm,clm = lossfunc_details(outputs,labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            #print(torch.cat([outputs.detach().view(1,5),labels.view(1,5)],0).view(2,5))</span><br><span class="line">            if iter % 10 == 0:</span><br><span class="line">            #    print(torch.cat([outputs.detach().view(1,5),labels.view(1,5)],0).view(2,5))</span><br><span class="line">                print(&quot;epoch&#123;&#125;, iter&#123;&#125;, loss: &#123;&#125;, lr: &#123;&#125;&quot;.format(epoch, iter, loss.data.item(),optimizer.state_dict()[&#x27;param_groups&#x27;][0][&#x27;lr&#x27;]))</span><br><span class="line">        </span><br><span class="line">        #print(&quot;Finish epoch &#123;&#125;, time elapsed &#123;&#125;&quot;.format(epoch, time.time() - ts))</span><br><span class="line">        #print(&quot;*&quot;*30)</span><br><span class="line">        #val(epoch)</span><br><span class="line">        scheduler.step()</span><br></pre></td></tr></table></figure>
<p>训练过程比较常规，先取1个batch的训练数据，分别得到inputs和labels，依次计算loss，反传，step等。</p>
<p>下面说下2个训练集的数据处理函数：</p>
<p>input_process：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def input_process(batch):</span><br><span class="line">    #import pdb</span><br><span class="line">    #pdb.set_trace()</span><br><span class="line">    batch_size=len(batch[0])</span><br><span class="line">    input_batch= torch.zeros(batch_size,3,448,448)</span><br><span class="line">    for i in range(batch_size):</span><br><span class="line">        inputs_tmp = Variable(batch[0][i])</span><br><span class="line">        inputs_tmp1=cv2.resize(inputs_tmp.permute([1,2,0]).numpy(),(448,448))</span><br><span class="line">        inputs_tmp2=torch.tensor(inputs_tmp1).permute([2,0,1])</span><br><span class="line">        input_batch[i:i+1,:,:,:]= torch.unsqueeze(inputs_tmp2,0)</span><br><span class="line">    return input_batch </span><br></pre></td></tr></table></figure>
<blockquote>
<p>batch[0]为image，batch[1]为label，batch_size为1个batch的图片数量。<br>
batch[0][i]为这个batch的第i张图片，inputs_tmp2为尺寸变成了3,448,448之后的图片，再经过unsqueeze操作拓展1维，size=[1,3,448,448]，存储在input_batch中。</p>
</blockquote>
<p>最后，返回的是size=[batch_size,3,448,448]的输入数据。</p>
<p>target_process：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def target_process(batch,grid_number=7):</span><br><span class="line">    # batch[1]表示label</span><br><span class="line">    # batch[0]表示image</span><br><span class="line">    batch_size=len(batch[0])</span><br><span class="line">    target_batch= torch.zeros(batch_size,grid_number,grid_number,30)</span><br><span class="line">    #import pdb</span><br><span class="line">    #pdb.set_trace()</span><br><span class="line">    for i in range(batch_size):</span><br><span class="line">        labels = batch[1]</span><br><span class="line">        batch_labels = labels[i]</span><br><span class="line">        #import pdb</span><br><span class="line">        #pdb.set_trace()</span><br><span class="line">        number_box = len(batch_labels[&#x27;boxes&#x27;])</span><br><span class="line">        for wi in range(grid_number):</span><br><span class="line">            for hi in range(grid_number):</span><br><span class="line">                # 遍历每个标注的框</span><br><span class="line">                for bi in range(number_box):</span><br><span class="line">                    bbox=batch_labels[&#x27;boxes&#x27;][bi]</span><br><span class="line">                    _,himg,wimg = batch[0][i].numpy().shape</span><br><span class="line">                    bbox = bbox/ torch.tensor([wimg,himg,wimg,himg])</span><br><span class="line">                    #import pdb</span><br><span class="line">                    #pdb.set_trace()</span><br><span class="line">                    center_x= (bbox[0]+bbox[2])*0.5</span><br><span class="line">                    center_y= (bbox[1]+bbox[3])*0.5</span><br><span class="line">                    #print(&quot;[%s,%s,%s],[%s,%s,%s]&quot;%(wi/grid_number,center_x,(wi+1)/grid_number,hi/grid_number,center_y,(hi+1)/grid_number))</span><br><span class="line">                    if center_x&lt;=(wi+1)/grid_number and center_x&gt;=wi/grid_number and center_y&lt;=(hi+1)/grid_number and center_y&gt;= hi/grid_number:</span><br><span class="line">                        #pdb.set_trace()</span><br><span class="line">                        cbbox =  torch.cat([torch.ones(1),bbox])</span><br><span class="line">                        # 中心点落在grid内，</span><br><span class="line">                        target_batch[i:i+1,wi:wi+1,hi:hi+1,:] = torch.unsqueeze(cbbox,0)</span><br><span class="line">                    #else:</span><br><span class="line">                        #cbbox =  torch.cat([torch.zeros(1),bbox])</span><br><span class="line">                #import pdb</span><br><span class="line">                #pdb.set_trace()</span><br><span class="line">                #rint(target_batch[i:i+1,wi:wi+1,hi:hi+1,:])</span><br><span class="line">                #target_batch[i:i+1,wi:wi+1,hi:hi+1,:] = torch.unsqueeze(cbbox,0)</span><br><span class="line">    return target_batch</span><br></pre></td></tr></table></figure>
<blockquote>
<p>要从batch里面获得label，首先要想清楚label(就是bounding box)应该是什么size，输出的结果应该是 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes30" alt="[公式]"> 的，所以label的size应该是：[batch_size,7,7,30]。在这个程序里我们实现的是输出 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes5" alt="[公式]"> 。这个 <img src="https://www.zhihu.com/equation?tex=5" alt="[公式]"> 就是x,y,w,h，所以label的size应该是：[batch_size,7,7,5]<br>
batch_labels表示这个batch的第i个图片的label，number_box表示这个图有几个真值框。<br>
接下来3重循环遍历每个grid的每个框，bbox表示正在遍历的这个框。<br>
bbox = bbox/ torch.tensor([wimg,himg,wimg,himg])表示对x,y,w,h进行归一化。<br>
接下来if语句得到confidence的真值，存储在target_batch中返回。</p>
</blockquote>
<p>最后是loss函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">def lossfunc_details(outputs,labels):</span><br><span class="line">    # 判断维度</span><br><span class="line">    assert ( outputs.shape == labels.shape),&quot;outputs shape[%s] not equal labels shape[%s]&quot;%(outputs.shape,labels.shape)</span><br><span class="line">    #import pdb</span><br><span class="line">    #pdb.set_trace()</span><br><span class="line">    b,w,h,c = outputs.shape</span><br><span class="line">    loss = 0</span><br><span class="line">    #import pdb</span><br><span class="line">    #pdb.set_trace()</span><br><span class="line">    conf_loss_matrix = torch.zeros(b,w,h)</span><br><span class="line">    geo_loss_matrix = torch.zeros(b,w,h)</span><br><span class="line">    loss_matrix = torch.zeros(b,w,h)</span><br><span class="line">    </span><br><span class="line">    for bi in range(b):</span><br><span class="line">        for wi in range(w):</span><br><span class="line">            for hi in range(h):</span><br><span class="line">                #import pdb</span><br><span class="line">                #pdb.set_trace()</span><br><span class="line">                # detect_vector=[confidence,x,y,w,h]</span><br><span class="line">                detect_vector = outputs[bi,wi,hi]</span><br><span class="line">                gt_dv = labels[bi,wi,hi]</span><br><span class="line">                conf_pred = detect_vector[0]</span><br><span class="line">                conf_gt = gt_dv[0]</span><br><span class="line">                x_pred = detect_vector[1]</span><br><span class="line">                x_gt = gt_dv[1]</span><br><span class="line">                y_pred = detect_vector[2]</span><br><span class="line">                y_gt = gt_dv[2]</span><br><span class="line">                w_pred = detect_vector[3]</span><br><span class="line">                w_gt = gt_dv[3]</span><br><span class="line">                h_pred = detect_vector[4]</span><br><span class="line">                h_gt = gt_dv[4]</span><br><span class="line">                loss_confidence = (conf_pred-conf_gt)**2 </span><br><span class="line">                #loss_geo = (x_pred-x_gt)**2 + (y_pred-y_gt)**2 + (w_pred**0.5-w_gt**0.5)**2 + (h_pred**0.5-h_gt**0.5)**2</span><br><span class="line">            </span><br><span class="line">                loss_geo = (x_pred-x_gt)**2 + (y_pred-y_gt)**2 + (w_pred-w_gt)**2 + (h_pred-h_gt)**2</span><br><span class="line">                loss_geo = conf_gt*loss_geo</span><br><span class="line">                loss_tmp = loss_confidence + 0.3*loss_geo</span><br><span class="line">                #print(&quot;loss[%s,%s] = %s,%s&quot;%(wi,hi,loss_confidence.item(),loss_geo.item()))</span><br><span class="line">                loss += loss_tmp</span><br><span class="line">                conf_loss_matrix[bi,wi,hi]=loss_confidence</span><br><span class="line">                geo_loss_matrix[bi,wi,hi]=loss_geo</span><br><span class="line">                loss_matrix[bi,wi,hi]=loss_tmp</span><br><span class="line">    #打印出batch中每张片的位置loss,和置信度输出</span><br><span class="line">    print(geo_loss_matrix)</span><br><span class="line">    print(outputs[0,:,:,0]&gt;0.5)</span><br><span class="line">    return loss,loss_matrix,geo_loss_matrix,conf_loss_matrix</span><br></pre></td></tr></table></figure>
<blockquote>
<p>首先需要注意：label和output的size应该是：[batch_size,7,7,5]。<br>
outputs[bi,wi,hi]就是一个5位向量： <img src="https://www.zhihu.com/equation?tex=%28c%5E%7Bpred%7D%2Cx%5E%7Bpred%7D%2Cy%5E%7Bpred%7D%2Cw%5E%7Bpred%7D%2Ch%5E%7Bpred%7D%29" alt="[公式]"> 。<br>
我们分别计算了loss_confidence和loss_geo，因为我们实现的这个模型只检测1个类，所以没有class_loss。</p>
</blockquote>
<p>为了使本文<strong>尽量生动有趣</strong>，我仍然用葫芦娃作为例子展示YOLO的过程(真的是尽力了。。。)。</p>
<p><img src="https://pic2.zhimg.com/80/v2-6f33f7b365e952d0cb1436c540135a0d_720w.jpg" alt="img">葫芦娃</p>
<p>下面进入正题，首先回顾下YOLO v1的模型结构，忘记了的同学请看上面的文章<strong>并点赞</strong>，如下面2图所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-adc371c1de58af018e9b097f73105b4c_720w.jpg" alt="img">YOLO</p>
<p><img src="https://pic2.zhimg.com/80/v2-3c018cd25d7e0f696bdf78e64da76655_720w.jpg" alt="img">YOLO</p>
<p>我们认为，<strong>检测模型=特征提取器+检测头</strong></p>
<p>在YOLO v1的模型中检测头就是最后的2个全连接层(Linear in PyTorch)，它们是参数量最大的2个层，也是最值得改进的2个层。后面的YOLO模型都对这里进行改进：</p>
<p>YOLO v1一共预测49个目标，一共98个框。</p>
<h2 id="5-YOLO-v2">5 YOLO v2</h2>
<ul>
<li><strong>检测头的改进：</strong></li>
</ul>
<p>YOLO v1虽然快，但是预测的框不准确，很多目标找不到：</p>
<ul>
<li><strong>预测的框不准确：准确度不足。</strong></li>
<li><strong>很多目标找不到：recall不足。</strong></li>
</ul>
<p>我们一个问题一个问题解决，首先第1个：</p>
<ul>
<li><strong>问题1：预测的框不准确：</strong></li>
</ul>
<p><strong>当时别人是怎么做的？</strong></p>
<p>同时代的检测器有R-CNN，人家预测的是偏移量。</p>
<p>什么是偏移量？</p>
<p><img src="https://pic4.zhimg.com/80/v2-4883b178ed0e2bb95f1d504dc6bed6a7_720w.jpg" alt="img">YOLO v2</p>
<p>之前YOLO v1直接预测x,y,w,h，范围比较大，现在我们想预测一个稍微小一点的值，来增加准确度。</p>
<p>不得不先介绍2个新概念：<strong>基于grid的偏移量和基于anchor的偏移量</strong>。什么意思呢？</p>
<p><strong>基于anchor的偏移量</strong>的意思是，anchor的位置是固定的，<strong>偏移量=目标位置-anchor的位置</strong>。</p>
<p><strong>基于grid的偏移量</strong>的意思是，grid的位置是固定的，<strong>偏移量=目标位置-grid的位置</strong>。</p>
<p><strong>Anchor是什么玩意？</strong></p>
<p>Anchor是R-CNN系列的一个概念，你可以把它理解为一个预先定义好的框，它的位置，宽高都是已知的，是一个参照物，供我们预测时参考。</p>
<p>上面的图就是YOLO v2给出的改进，你可能现在看得一脸懵逼，我先解释下各个字母的含义：</p>
<p><img src="https://www.zhihu.com/equation?tex=b_%7Bx%7D%2Cb_%7By%7D%2Cb_%7Bw%7D%2Cb_%7Bh%7D" alt="[公式]"> ：模型最终得到的的检测结果。</p>
<p><img src="https://www.zhihu.com/equation?tex=t_%7Bx%7D%2Ct_%7By%7D%2Ct_%7Bw%7D%2Ct_%7Bh%7D" alt="[公式]"> ：模型要预测的值。</p>
<p><img src="https://www.zhihu.com/equation?tex=c_%7Bx%7D%2Cc_%7By%7D" alt="[公式]"> ：grid的左上角坐标，如下图所示。</p>
<p><img src="https://www.zhihu.com/equation?tex=p_%7Bw%7D%2Cp_%7Bh%7D" alt="[公式]"> ：Anchor的宽和高，这里的anchor是人为定好的一个框，宽和高是固定的。</p>
<p><img src="https://pic1.zhimg.com/80/v2-cc3885ebc9f24892cf30c58c564044c0_720w.jpg" alt="img"></p>
<p>通过这样的定义我们从<strong>直接预测位置</strong>改为<strong>预测一个偏移量</strong>，基于<strong>Anchor框的宽和高</strong>和<strong>grid的先验位置</strong>的<strong>偏移量</strong>，得到最终目标的位置，这种方法也叫作<strong>location prediction</strong>。</p>
<p><strong>这里还涉及到一个尺寸问题：</strong></p>
<p>刚才说到 <img src="https://www.zhihu.com/equation?tex=t_%7Bx%7D%2Ct_%7By%7D%2Ct_%7Bw%7D%2Ct_%7Bh%7D" alt="[公式]"> 是模型要预测的值，这里 <img src="https://www.zhihu.com/equation?tex=c_%7Bx%7D%2Cc_%7By%7D" alt="[公式]"> 为grid的坐标，画个图就明白了：</p>
<p><img src="https://pic3.zhimg.com/80/v2-e31634384bb34bbbb1de0e8ddfc5ddc2_720w.jpg" alt="img">图1：原始值</p>
<p>如图1所示，假设此图分为9个grid，GT如红色的框所示，Anchor如紫色的框所示。图中的数字为image的真实信息。</p>
<p>我们首先会对这些值<strong>归一化</strong>，结果如下图2所示：</p>
<p><img src="https://pic3.zhimg.com/80/v2-63ca4ed417c5db83b18c95a42a7f60f2_720w.jpg" alt="img">图2：要预测的值</p>
<p>归一化之后你会发现，要预测的值就变为了：</p>
<p><img src="https://www.zhihu.com/equation?tex=t_%7Bx%7D%2Ct_%7By%7D%2Ct_%7Bw%7D%2Ct_%7Bh%7D%3D0.172%2C-0.148%2C-0.340%2C-0.326" alt="[公式]"></p>
<p>这是一个偏移量，且值很小，有利于神经网络的学习。</p>
<p>**你可能会有疑问：**为什么YOLO v2改预测偏移量而不是直接去预测 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%2Cw%2Ch%29" alt="[公式]"> ？</p>
<p>上面我说了作者看到了同时代的R-CNN，人家预测的是偏移量。另一个重要的原因是：直接预测位置会导致神经网络在一开始训练时不稳定，使用偏移量会使得训练过程更加稳定，性能指标提升了5%左右。</p>
<p>位置上不使用Anchor框，宽高上使用Anchor框。以上就是YOLO v2的一个改进。用了YOLO v2的改进之后确实是更准确了，但别激动，上面还有一个问题呢~</p>
<ul>
<li><strong>问题2：很多目标找不到：</strong></li>
</ul>
<p>你还记得上一篇讲得<strong>YOLO v1一次能检测多少个目标吗</strong>？答案是<strong>49个目标</strong>，98个框，并且2个框对应一个类别。可以是大目标也可以是小目标。因为输出的尺寸是：[N, 7, 7, 30]。式中N为图片数量，7,7为49个区域(grid)。</p>
<p><img src="https://www.zhihu.com/equation?tex=30%3D2%5Ctimes5%28c%2Cx%2Cy%2Cw%2Ch%29%2B1%5Ctimes20+classes" alt="[公式]"></p>
<p>YOLO v2首先把 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="[公式]"> 个区域改为 <img src="https://www.zhihu.com/equation?tex=13%5Ctimes13" alt="[公式]"> 个区域，每个区域有5个anchor，且每个anchor对应着1个类别，那么，输出的尺寸就应该为：[N,13,13,125]。</p>
<p><img src="https://www.zhihu.com/equation?tex=125%3D5%5Ctimes5%28c%2Cx%2Cy%2Cw%2Ch%29%2B5%5Ctimes20+classes" alt="[公式]"></p>
<p>这里面有个bug，就是YOLO v2先对每个区域得到了5个anchor作为参考，那你就会问2个问题：</p>
<p><strong>1.为什么要用Anchor呢？</strong></p>
<p><strong>答：一开始YOLO v1的初始训练过程很不稳定</strong>，在YOLO v2中，作者观察了很多图片的所有Ground Truth，发现：比如车，GT都是矮胖的长方形，再比如行人，GT都是瘦高的长方形，且宽高比具有相似性。那**能不能根据这一点，从数据集中预先准备几个几率比较大的bounding box，再以它们为基准进行预测呢？**这就是Anchor的初衷。</p>
<p><strong>2.每个区域的5个anchor是如何得到的？</strong></p>
<p>下图可以回答你的问题：</p>
<p><img src="https://pic1.zhimg.com/80/v2-7e157d61f41ca02634f06b0b78c71684_720w.jpg" alt="img">methods to get the 5 anchor</p>
<p>方法：对于任意一个数据集，就比如说COCO吧(紫色的anchor)，先对训练集的GT bounding box进行聚类，聚成几类呢？作者进行了实验之后发现<strong>5</strong>类的<strong>recall vs. complexity</strong>比较好，现在聚成了<strong>5</strong>类，当然9类的mAP最好，预测的最全面，但是在复杂度上升很多的同时对模型的准确度提升不大，所以采用了一个比较折中的办法选取了5个聚类簇，即使用5个先验框。</p>
<p>所以到现在为止，有了anchor再结合刚才的 <img src="https://www.zhihu.com/equation?tex=t_%7Bx%7D%2Ct_%7By%7D%2Ct_%7Bw%7D%2Ct_%7Bh%7D" alt="[公式]"> ，就可以求出目标位置。</p>
<p><strong>anchor是从数据集中统计得到的(Faster-RCNN中的Anchor的宽高和大小是手动挑选的)。</strong></p>
<ul>
<li><strong>损失函数为：</strong></li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-faed5df5818795d5fc047815f0338768_720w.jpg" alt="img">YOLO v2损失函数</p>
<blockquote>
<p>这里的W=13,H=13,A=5。<br>
每个 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> 都是一个权重值。c表示类别，r表示rectangle，即(x,y,w,h)。<br>
第1,4行是confidence_loss，注意这里的真值变成了0和IoU(GT, anchor)的值，你看看这些细节。。。<br>
第5行是class_loss。<br>
第2,3行：t是迭代次数，即前12800步我们计算这个损失，后面不计算了。这部分意义何在？<br>
意思是：前12800步我们会优化<strong>预测的(x,y,w,h)与anchor的(x,y,w,h)的距离+预测的(x,y,w,h)与GT的(x,y,w,h)的距离</strong>，12800步之后就只优化<strong>预测的(x,y,w,h)与GT的(x,y,w,h)的距离，为啥？因为这时的预测结果已经较为准确了，anchor已经满足我了我们了，而在一开始预测不准的时候，用上anchor可以加速训练。</strong><br>
<strong>你看看这操作多么的细节。。。</strong><br>
<img src="https://www.zhihu.com/equation?tex=1_%7Bk%7D%5E%7Btruth%7D" alt="[公式]"> 是什么？第k个anchor与所有GT的IoU的maximum，如果大于一个阈值，就 <img src="https://www.zhihu.com/equation?tex=1_%7Bk%7D%5E%7Btruth%7D%3D1" alt="[公式]"> ，否则的话： <img src="https://www.zhihu.com/equation?tex=1_%7Bk%7D%5E%7Btruth%7D%3D0" alt="[公式]"> 。</p>
</blockquote>
<p>好，到现在为止，YOLO v2做了这么多改进，整体性能大幅度提高，但是小目标检测仍然是YOLO v2的痛。直到kaiming大神的ResNet出现，backbone可以更深了，所以darknet53诞生。</p>
<p>最后我们做个比较：</p>
<p><img src="https://pic1.zhimg.com/80/v2-e4dce0794b6d4aa5a67133633baed6b4_720w.jpg" alt="img">YOLO v1和v2的比较</p>
<hr>
<h2 id="6-YOLO-v3">6 YOLO v3</h2>
<ul>
<li><strong>检测头的改进：</strong></li>
</ul>
<p>之前在说小目标检测仍然是YOLO v2的痛，YOLO v3是如何改进的呢？如下图所示。</p>
<p><img src="https://pic1.zhimg.com/80/v2-4cf1b6f6afec393122305ca2bb2725a4_720w.jpg" alt="img">YOLO v3</p>
<p>我们知道，YOLO v2的检测头已经由YOLO v1的 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="[公式]"> 变为 <img src="https://www.zhihu.com/equation?tex=13%5Ctimes13" alt="[公式]">了，我们看YOLO v3检测头分叉了，分成了3部分：</p>
<ul>
<li>13<em>13</em>3*(4+1+80)</li>
<li>26<em>26</em>3*(4+1+80)</li>
<li>52<em>52</em>3*(4+1+80)</li>
</ul>
<p>预测的框更多更全面了，并且分级了。</p>
<p>我们发现3个分支分别为<strong>32倍下采样，16倍下采样，8倍下采样</strong>，分别取预测<strong>大，中，小目标</strong>。为什么这样子安排呢？</p>
<p>因为<strong>32倍下采样</strong>每个点感受野更大，所以去预测<strong>大目标，8倍下采样</strong>每个点感受野最小，所以去预测<strong>小目标。专人专事。</strong></p>
<p><strong>发现预测地更准确了，性能又提升了。</strong></p>
<p>又有人会问，你现在是3个分支，我改成5个，6个分支会不会更好？</p>
<p>理论上会，但还是那句话，作者遵循recall vs. complexity的trade off。</p>
<p>图中的123456789是什么意思？</p>
<p><strong>答：框</strong>。每个grid设置9个先验框，3个大的，3个中的，3个小的。</p>
<p>每个分支预测3个框，每个框预测5元组+80个one-hot vector类别，所以一共size是：</p>
<p><strong>3*(4+1+80)</strong></p>
<p><strong>每个分支的输出size为：</strong></p>
<ul>
<li><strong>[13,13,3*(4+1+80)]</strong></li>
<li><strong>[26,26,3*(4+1+80)]</strong></li>
<li><strong>[52,52,3*(4+1+80)]</strong></li>
</ul>
<p><strong>当然你也可以用5个先验框，这时每个分支的输出size为：</strong></p>
<ul>
<li><strong>[13,13,5*(4+1+80)]</strong></li>
<li><strong>[26,26,5*(4+1+80)]</strong></li>
<li><strong>[52,52,5*(4+1+80)]</strong></li>
</ul>
<p><strong>读到这里，请你数一下YOLO v3可以预测多少个bounding box？</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=%2813%5Ctimes13%2B26%5Ctimes26%2B52%5Ctimes52%29%5Ctimes3%3D10467%28YOLO+v3%29%3E%3E845%28YOLO+v2%29%3D13%5Ctimes13%5Ctimes5" alt="[公式]"></p>
<p>多了这么多可以预测的bounding box，模型的能力增强了。</p>
<p><strong>为确定priors</strong>，YOLO v3 应用<strong>k均值聚类</strong>。然后它<strong>预先选择9个聚类簇</strong>。</p>
<p>对于<strong>COCO数据集</strong>，锚定框的宽度和高为**(10 ×13)，(16 ×30)，(33 ×23)，(30 ×61)，(62 ×45)，(59 ×119)，(116 × 90)，( 156 ×198)，(373373×326326)**。这应该是按照输入图像的尺寸416×416 计算得到的。这 9个priors根据它们的尺度分为3个不同的组。在检测目标时，给一特定的特征图分配一个组。</p>
<p>就对应了下面这个图：</p>
<p><img src="https://pic4.zhimg.com/80/v2-24ff3cd5062021493490e28e7d73d6bf_720w.jpg" alt="img">YOLO v3</p>
<p>检测头是<strong>DBL</strong>，定义在图上，没有了FC。</p>
<p>还有一种画法，更加直观一点：</p>
<p><img src="https://pic3.zhimg.com/80/v2-085b6d95dc53894e5de4fe95d2249b06_720w.jpg" alt="img"></p>
<p><img src="https://pic2.zhimg.com/80/v2-151886b99cf8a93f116cd845a4524c89_720w.jpg" alt="img">YOLO v3 head</p>
<p><strong>anchor和YOLO v2一样，依然是从数据集中统计得到的。</strong></p>
<ul>
<li><strong>损失函数为：</strong></li>
</ul>
<p><img src="https://pic4.zhimg.com/80/v2-1714579e2a7f9ca88335bdaeae9e1c4f_720w.jpg" alt="img">YOLO v3损失函数</p>
<blockquote>
<p>第4行说明：loss分3部分组成：<br>
第1行代表geo_loss，S代表13,26,52，就是grid是几乘几的。B=5。<br>
第2行代表confidence_loss，和YOLO v2一模一样。<br>
第3行代表class_loss，和YOLO v2的区别是改成了交叉熵。</p>
</blockquote>
<ul>
<li><strong>边界框预测和代价函数的计算：</strong></li>
</ul>
<p>YOLO v3使用多标签分类，用多个独立的logistic分类器代替softmax函数，以计算输入属于特定标签的可能性。在计算分类损失进行训练时，YOLO v3对每个标签使用二元交叉熵损失。</p>
<p><strong>正负样本的确定：</strong></p>
<blockquote>
<p>如果<strong>边界框先验（锚定框）与 GT 目标比其他目标重叠多</strong>，则相应的<strong>目标性得分应为 1</strong>。<br>
对于<strong>重叠大于等于0.5的其他先验框(anchor)</strong>，忽略，<strong>不算损失</strong>。<br>
每个 GT 目标<strong>仅与一个先验边界框相关联</strong>。 如果没有分配先验边界框，则不会导致<strong>分类和定位损失，只会有目标性的置信度损失。</strong><br>
使用**tx和ty(<strong>而不是 bx 和by</strong>)**来计算损失。</p>
</blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-1e31f33f86d3f2bb9c7d516efd974765_720w.jpg" alt="img">使用tx和ty(而不是 bx 和by)来计算损失</p>
<p><img src="https://pic1.zhimg.com/80/v2-5b4352b9a3138190717febf40e5f0384_720w.png" alt="img">交叉熵损失</p>
<p><strong>总结起来就是下面4句话：</strong></p>
<ul>
<li>正样本：与GT的IOU最大的框。</li>
<li>负样本：与GT的IOU&lt;0.5 的框。</li>
<li>忽略的样本：与GT的IOU&gt;0.5 但不是最大的框。</li>
<li>使用 tx 和ty （而不是 bx 和by ）来计算损失。</li>
</ul>
<p>最后我们做个比较：</p>
<p><img src="https://pic4.zhimg.com/80/v2-e35154c0fe812a210295ca8c3cd92df3_720w.jpg" alt="img">YOLO v1 v2和v3的比较</p>
<hr>
<h2 id="7-疫情都挡不住的YOLO-v4">7 疫情都挡不住的YOLO v4</h2>
<p>第一次看到YOLO v4公众号发文是在疫情期间，那时候还来不了学校。不得不说疫情也挡不住作者科研的动力。</p>
<ul>
<li><strong>检测头的改进：</strong></li>
</ul>
<p>YOLO v4的作者换成了Alexey Bochkovskiy大神，检测头总的来说还是多尺度的，3个尺度，分别负责大中小目标。只不过多了一些细节的改进：</p>
<p><strong>1.Using multi-anchors for single ground truth</strong></p>
<p>之前的YOLO v3是1个anchor负责一个GT，YOLO v4中用多个anchor去负责一个GT。方法是：对于 <img src="https://www.zhihu.com/equation?tex=GT_%7Bj%7D" alt="[公式]"> 来说，只要 <img src="https://www.zhihu.com/equation?tex=IoU%28anchor_%7Bi%7D%2CGT_%7Bj%7D%29%3Ethreshold" alt="[公式]"> ，就让 <img src="https://www.zhihu.com/equation?tex=anchor_%7Bi%7D" alt="[公式]"> 去负责 <img src="https://www.zhihu.com/equation?tex=GT_%7Bj%7D" alt="[公式]"> 。</p>
<p>这就相当于你anchor框的数量没变，但是选择的<strong>正样本</strong>的比例增加了，就<strong>缓解了正负样本不均衡的问题</strong>。</p>
<p><strong>2.Eliminate_grid sensitivity</strong></p>
<p>还记得之前的YOLO v2的这幅图吗？YOLO v2，YOLO v3都是预测4个这样的偏移量</p>
<p><img src="https://pic3.zhimg.com/80/v2-63ca4ed417c5db83b18c95a42a7f60f2_720w.jpg" alt="img">图3：YOLO v2，YOLO v3要预测的值</p>
<p>这里其实还隐藏着一个问题：</p>
<p>模型预测的结果是： <img src="https://www.zhihu.com/equation?tex=t_%7Bx%7D%2Ct_%7By%7D%2Ct_%7Bw%7D%2Ct_%7Bh%7D" alt="[公式]"> ，那么最终的结果是： <img src="https://www.zhihu.com/equation?tex=b_%7Bx%7D%2Cb_%7By%7D%2Cb_%7Bw%7D%2Cb_%7Bh%7D" alt="[公式]"> 。这个 <img src="https://www.zhihu.com/equation?tex=b" alt="[公式]"> 按理说应该能取到一个grid里面的任意位置。但是实际上边界的位置是取不到的，因为sigmoid函数的值域是： <img src="https://www.zhihu.com/equation?tex=%280%2C1%29" alt="[公式]"> ，它不是 <img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]"> 。所以作者提出的Eliminate_grid sensitivity的意思是：将 <img src="https://www.zhihu.com/equation?tex=b_%7Bx%7D%2Cb_%7By%7D" alt="[公式]"> 的计算公式改为：</p>
<p><img src="https://www.zhihu.com/equation?tex=b_%7Bx%7D%3D1.1%5Ccdot%5Csigma%28t_%7Bx%7D%29%2Bc_%7Bx%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=b_%7By%7D%3D1.1%5Ccdot%5Csigma%28t_%7By%7D%29%2Bc_%7By%7D" alt="[公式]"></p>
<p>这里的1.1就是一个示例，你也可以是1.05,1.2等等，反正要乘上一个略大于1的数，作者发现经过这样的改动以后效果会再次提升。</p>
<p><strong>3.CIoU-loss</strong></p>
<p>之前的YOLO v2，YOLO v3在计算geo_loss时都是用的MSE Loss，之后人们开始使用IoU Loss。</p>
<p><img src="https://www.zhihu.com/equation?tex=L_%7BIoU%7D%3D1-%5Cfrac%7B%7CB%5Ccap+B_%7Bgt%7D%7C%7D%7B%7CB%5Ccup+B_%7Bgt%7D%7C%7D" alt="[公式]"> 它可以反映预测检测框与真实检测框的检测效果。</p>
<p>但是问题也很多：不能反映两者的距离大小（重合度）。同时因为loss=0，**当GT和bounding box不挨着时，没有梯度回传，无法进行学习训练。**如下图4所示，三种情况IoU都相等，但看得出来他们的重合度是不一样的，左边的图回归的效果最好，右边的最差：</p>
<p><img src="https://pic2.zhimg.com/80/v2-a52e8fc7166b29c08b80de1ead22ec79_720w.jpg" alt="img">图4：IoU Loss不能反映两者的距离大小</p>
<ul>
<li><strong>所以接下来的改进是：</strong></li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=L_%7BGIoU%7D%3D1-IoU%2B%5Cfrac%7B%7CC-B%5Ccup+B_%7Bgt%7D%7C%7D%7B%7CC%7C%7D" alt="[公式]"> , <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 为同时包含了预测框和真实框的最小框的面积。</p>
<p><img src="https://pic1.zhimg.com/80/v2-4ccbf64fa4eefb0e321809a803f90c74_720w.jpg" alt="img">图4：GIoU Loss</p>
<p>GIoU Loss可以解决上面IoU Loss对距离不敏感的问题。但是GIoU Loss存在训练过程中发散等问题。</p>
<ul>
<li><strong>接下来的改进是：</strong></li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=L_%7BDIoU%7D+%3D+1-IoU+%2B+%5Cfrac%7B%5Crho%5E%7B2%7D%28b%2Cb%5E%7Bgt%7D%29%7D%7Bc%5E%7B2%7D%7D" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=b" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=b%5E%7Bgt%7D" alt="[公式]"> 分别代表了预测框和真实框的中心点，且<img src="https://www.zhihu.com/equation?tex=%5Crho" alt="[公式]">代表的是计算两个中心点间的欧式距离。<img src="https://www.zhihu.com/equation?tex=c" alt="[公式]">代表的是能够同时包含预测框和真实框的<strong>最小闭包区域</strong>的对角线距离。</p>
<p><img src="https://pic1.zhimg.com/80/v2-2345aacc478cc5523d439ffcd84958ac_720w.jpg" alt="img">图5：DIoU Loss</p>
<p><strong>DIoU loss</strong>可以直接最小化两个目标框的距离，因此比GIoU loss收敛快得多。</p>
<p><strong>DIoU loss除了这一点之外，还有一个好处是：</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-3db202166e0c206001ca03e191489532_720w.jpg" alt="img">IoU Loss和GIoU loss都一样时</p>
<p>如上图所示，此3种情况IoU Loss和GIoU loss都一样，但是DIoU Loss右图最小，中间图次之，左图最大。</p>
<p>这里就是一道面试题：<strong>请总结DIoU loss的好处是？</strong></p>
<p><strong>答：</strong></p>
<ul>
<li>收敛快(需要的epochs少)。</li>
<li>缓解了Bounding box全包含GT问题。</li>
</ul>
<p><strong>但是</strong>DIoU loss只是<strong>缓解了Bounding box全包含GT问题</strong>，<strong>依然没有彻底解决包含的问题</strong>，即：</p>
<p><img src="https://pic4.zhimg.com/80/v2-96838980b7fd4443661cf0019802ea7b_720w.jpg" alt="img"></p>
<p>这2种情况<img src="https://www.zhihu.com/equation?tex=b" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=b%5E%7Bgt%7D" alt="[公式]">是重合的，DIoU loss的第3项没有区别，所以在这个意义上DIoU loss依然存在问题。</p>
<ul>
<li><strong>接下来的改进是：</strong></li>
</ul>
<p>惩罚项如下面公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BR%7D_%7BC+I+o+U%7D%3D%5Cfrac%7B%5Crho%5E%7B2%7D%5Cleft%28%5Cmathbf%7Bb%7D%2C+%5Cmathbf%7Bb%7D%5E%7Bg+t%7D%5Cright%29%7D%7Bc%5E%7B2%7D%7D%2B%5Calpha+v" alt="[公式]"> 其中 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 是权重函数，</p>
<p>而 <img src="https://www.zhihu.com/equation?tex=%5Cnu" alt="[公式]"> 用来度量长宽比的相似性，定义为<img src="https://www.zhihu.com/equation?tex=v%3D%5Cfrac%7B4%7D%7B%5Cpi%5E%7B2%7D%7D%5Cleft%28%5Carctan+%5Cfrac%7Bw%5E%7Bg+t%7D%7D%7Bh%5E%7Bg+t%7D%7D-%5Carctan+%5Cfrac%7Bw%7D%7Bh%7D%5Cright%29%5E%7B2%7D" alt="[公式]"></p>
<p>完整的 CIoU 损失函数定义：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7BC+I+o+U%7D%3D1-I+o+U%2B%5Cfrac%7B%5Crho%5E%7B2%7D%5Cleft%28%5Cmathbf%7Bb%7D%2C+%5Cmathbf%7Bb%7D%5E%7Bg+t%7D%5Cright%29%7D%7Bc%5E%7B2%7D%7D%2B%5Calpha+v" alt="[公式]"></p>
<p>最后，CIoU loss的梯度类似于DIoU loss，但还要考虑 <img src="https://www.zhihu.com/equation?tex=%5Cnu" alt="[公式]"> 的梯度。在长宽在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]"> 的情况下， <img src="https://www.zhihu.com/equation?tex=w%5E%7B2%7D%2Bh%5E%7B2%7D" alt="[公式]"> 的值通常很小，会导致梯度爆炸，因此在 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bw%5E%7B2%7D%2Bh%5E%7B2%7D%7D" alt="[公式]"> 实现时将替换成1。</p>
<p>CIoU loss实现代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">def bbox_overlaps_ciou(bboxes1, bboxes2):</span><br><span class="line">    rows = bboxes1.shape[0]</span><br><span class="line">    cols = bboxes2.shape[0]</span><br><span class="line">    cious = torch.zeros((rows, cols))</span><br><span class="line">    if rows * cols == 0:</span><br><span class="line">        return cious</span><br><span class="line">    exchange = False</span><br><span class="line">    if bboxes1.shape[0] &gt; bboxes2.shape[0]:</span><br><span class="line">        bboxes1, bboxes2 = bboxes2, bboxes1</span><br><span class="line">        cious = torch.zeros((cols, rows))</span><br><span class="line">        exchange = True</span><br><span class="line"></span><br><span class="line">    w1 = bboxes1[:, 2] - bboxes1[:, 0]</span><br><span class="line">    h1 = bboxes1[:, 3] - bboxes1[:, 1]</span><br><span class="line">    w2 = bboxes2[:, 2] - bboxes2[:, 0]</span><br><span class="line">    h2 = bboxes2[:, 3] - bboxes2[:, 1]</span><br><span class="line"></span><br><span class="line">    area1 = w1 * h1</span><br><span class="line">    area2 = w2 * h2</span><br><span class="line"></span><br><span class="line">    center_x1 = (bboxes1[:, 2] + bboxes1[:, 0]) / 2</span><br><span class="line">    center_y1 = (bboxes1[:, 3] + bboxes1[:, 1]) / 2</span><br><span class="line">    center_x2 = (bboxes2[:, 2] + bboxes2[:, 0]) / 2</span><br><span class="line">    center_y2 = (bboxes2[:, 3] + bboxes2[:, 1]) / 2</span><br><span class="line"></span><br><span class="line">    inter_max_xy = torch.min(bboxes1[:, 2:],bboxes2[:, 2:])</span><br><span class="line">    inter_min_xy = torch.max(bboxes1[:, :2],bboxes2[:, :2])</span><br><span class="line">    out_max_xy = torch.max(bboxes1[:, 2:],bboxes2[:, 2:])</span><br><span class="line">    out_min_xy = torch.min(bboxes1[:, :2],bboxes2[:, :2])</span><br><span class="line"></span><br><span class="line">    inter = torch.clamp((inter_max_xy - inter_min_xy), min=0)</span><br><span class="line">    inter_area = inter[:, 0] * inter[:, 1]</span><br><span class="line">    inter_diag = (center_x2 - center_x1)**2 + (center_y2 - center_y1)**2</span><br><span class="line">    outer = torch.clamp((out_max_xy - out_min_xy), min=0)</span><br><span class="line">    outer_diag = (outer[:, 0] ** 2) + (outer[:, 1] ** 2)</span><br><span class="line">    union = area1+area2-inter_area</span><br><span class="line">    u = (inter_diag) / outer_diag</span><br><span class="line">    iou = inter_area / union</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        arctan = torch.atan(w2 / h2) - torch.atan(w1 / h1)</span><br><span class="line">        v = (4 / (math.pi ** 2)) * torch.pow((torch.atan(w2 / h2) - torch.atan(w1 / h1)), 2)</span><br><span class="line">        S = 1 - iou</span><br><span class="line">        alpha = v / (S + v)</span><br><span class="line">        w_temp = 2 * w1</span><br><span class="line">    ar = (8 / (math.pi ** 2)) * arctan * ((w1 - w_temp) * h1)</span><br><span class="line">    cious = iou - (u + alpha * ar)</span><br><span class="line">    cious = torch.clamp(cious,min=-1.0,max = 1.0)</span><br><span class="line">    if exchange:</span><br><span class="line">        cious = cious.T</span><br><span class="line">    return cious</span><br></pre></td></tr></table></figure>
<p>所以最终的演化过程是：</p>
<p><strong>MSE Loss <img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> IoU Loss<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> GIoU Loss<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> DIoU Loss<img src="https://www.zhihu.com/equation?tex=%5Crightarrow" alt="[公式]"> CIoU Loss</strong></p>
<ul>
<li><strong>YOLO v4损失函数：</strong></li>
</ul>
<p>所以基于上面的改进，YOLO v4的损失函数为：</p>
<p><img src="https://pic3.zhimg.com/80/v2-310600feb7204188d9738fbd062567ca_720w.jpg" alt="img">YOLO v4的损失函数</p>
<hr>
<h2 id="8-代码比论文都早的YOLO-v5">8 代码比论文都早的YOLO v5</h2>
<ul>
<li><strong>检测头的改进：</strong></li>
</ul>
<p><strong>head部分没有任何改动</strong>，和yolov3和yolov4完全相同，也是三个输出头，stride分别是8,16,32，大输出特征图检测小物体，小输出特征图检测大物体。</p>
<p>但采用了<strong>自适应anchor，而且这个功能还可以手动打开/关掉，具体是什么意思呢？</strong></p>
<p>加上了自适应anchor的功能，个人感觉YOLO v5其实变成了2阶段方法。</p>
<p><strong>先回顾下之前的检测器得到anchor的方法：</strong></p>
<p><strong>Yolo v2 v3 v4：聚类得到anchor</strong>，不是完全基于anchor的，w,h是基于anchor的，而x,y是基于grid的坐标，所以人家叫<strong>location prediction</strong>。</p>
<p><strong>R-CNN系列：手动指定</strong>anchor的位置。</p>
<p><strong>基于anchor的方法是怎么用的：</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-5dc0cdfb531add5071abf2abc7399467_720w.jpg" alt="img">anchor是怎么用的</p>
<p>有了anchor的 <img src="https://www.zhihu.com/equation?tex=%28x_%7BA%7D%2Cy_%7BA%7D%2Cw_%7BA%7D%2Ch_%7BA%7D%29" alt="[公式]"> ,和我们预测的偏移量 <img src="https://www.zhihu.com/equation?tex=t_%7Bx%7D%2Ct_%7By%7D%2Ct_%7Bw%7D%2Ct_%7Bh%7D" alt="[公式]"> ，就可以计算出最终的output： <img src="https://www.zhihu.com/equation?tex=b_%7Bx%7D%2Cb_%7By%7D%2Cb_%7Bw%7D%2Cb_%7Bh%7D" alt="[公式]"> 。</p>
<p>之前anchor是固定的，自适应anchor利用网络的学习功能，让 <img src="https://www.zhihu.com/equation?tex=%28x_%7BA%7D%2Cy_%7BA%7D%2Cw_%7BA%7D%2Ch_%7BA%7D%29" alt="[公式]"> 也是可以学习的。我个人觉得自适应anchor策略，影响应该不是很大，除非是<strong>刚开始设置的anchor是随意设置的</strong>，一般我们都会基于实际项目数据重新运用<strong>kmean算法聚类得到anchor</strong>，这一步本身就不能少。</p>
<p>最后总结一下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-34554f1fe4165e6b85d4905b925faa79_720w.jpg" alt="img">YOLO series的比较</p>
<p>本文只介绍了<strong>YOLO v2 v3 v4 v5</strong>对于<strong>检测头head</strong>和<strong>损失函数loss</strong>的优化，剩下的<strong>backbone</strong>方面和<strong>输入端的优化</strong>实在是写不动了，放到下一篇吧。</p>
<p>下面进入正题，目标检测器模型的结构如下图1所示，之前看过了YOLO v2 v3 v4 v5对于检测头和loss函数的改进，如下图2所示，下面着重介绍backbone的改进：</p>
<p><img src="https://pic1.zhimg.com/80/v2-8c5bac2266cf9b9b857529805511221c_720w.jpg" alt="img">图1：检测器的结构</p>
<p><img src="https://pic2.zhimg.com/80/v2-b641aebaf5d009504d30a1d17a5f7019_720w.jpg" alt="img">图2：YOLO系列比较</p>
<p>我们发现YOLO v1只是把最后的特征分成了 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="[公式]"> 个grid，到了YOLO v2就变成了 <img src="https://www.zhihu.com/equation?tex=13%5Ctimes13" alt="[公式]"> 个grid，再到YOLO v3 v4 v5就变成了多尺度的**(strides=8,16,32)<strong>，更加复杂了。那</strong>为什么一代比一代检测头更加复杂呢？答案是：因为它们的提特征网络更加强大了，能够支撑起检测头做更加复杂的操作。**换句话说，如果没有backbone方面的优化，你即使用这么复杂的检测头，可能性能还会更弱。所以引出了今天的话题：</p>
<ul>
<li><strong>backbone的改进：</strong></li>
</ul>
<h2 id="YOLO-v1"><strong>YOLO v1</strong></h2>
<p>我们先看看YOLO v1的backbone长什么样子：</p>
<p><img src="https://pic2.zhimg.com/80/v2-70005c21ddd8470ae5587b231987cb7d_720w.jpg" alt="img">YOLO v1 backbone</p>
<p>最后2层是全连接层，其他使用了大量的卷积层，网络逐渐变宽，是非常标准化的操作。注意这里面试官可能会问你一个问题：为什么都是卷积，图上要分开画出来，不写在一起？答案是：按照feature map的分辨率画出来。分辨率A变化到分辨率B的所有卷积画在了一起。因为写代码时经常会这么做，所以问这个问题的意图是看看你是否经常写代码。</p>
<p>然后我们看下检测类网络的结构，如下图3所示，这个图是YOLO v4中总结的：</p>
<p><img src="https://pic1.zhimg.com/80/v2-d2fb92212cbfb1d29bc395000602231c_720w.jpg" alt="img">图3：检测类网络的结构</p>
<p>YOLO v1没有Neck，Backbone是GoogLeNet，属于Dense Prediction。<strong>1阶段的检测器属于Dense Prediction，而2阶段的检测器既有Dense Prediction，又有Sparse Prediction</strong>。</p>
<hr>
<h2 id="YOLO-v2"><strong>YOLO v2</strong></h2>
<p>为了进一步提升性能，YOLO v2重新训练了一个darknet-19，如下图4所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-3ea70cd81cb6103ad41e9bb01b3114e4_720w.jpg" alt="img">图4：darknet-19</p>
<p>仔细观察上面的backbone的结构(双横线上方)，提出3个问题：</p>
<ul>
<li>为什么没有 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="[公式]"> 卷积了？只剩下了 <img src="https://www.zhihu.com/equation?tex=3%5Ctimes3" alt="[公式]"> 卷积和 <img src="https://www.zhihu.com/equation?tex=1%5Ctimes1" alt="[公式]"> 卷积了？</li>
</ul>
<p>**答：**vgg net论文得到一个结论，<img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="[公式]"> 卷积可以用更小的卷积代替，且<img src="https://www.zhihu.com/equation?tex=3%5Ctimes3" alt="[公式]"> 卷积更加节约参数，使模型更小。</p>
<p>网络可以做得更深，更好地提取到特征。为什么？因为每做一次卷积，后面都会接一个非线性的激活函数，更深意味着非线性的能力更强了。所以，你可能以后再也见不到 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="[公式]"> 卷积了。</p>
<p>另外还用了bottleneck结构(红色框)：</p>
<p><img src="https://www.zhihu.com/equation?tex=3%5Ctimes3" alt="[公式]"> 卷积负责扩大感受野， <img src="https://www.zhihu.com/equation?tex=1%5Ctimes1" alt="[公式]"> 卷积负责减少参数量。</p>
<ul>
<li>为什么没有FC层了？</li>
</ul>
<p>**答：**使用了GAP(Global Average Pooling)层，把 <img src="https://www.zhihu.com/equation?tex=1000%5Ctimes7%5Ctimes7" alt="[公式]"> 映射为 <img src="https://www.zhihu.com/equation?tex=1000%5Ctimes1" alt="[公式]"> ，满足了输入不同尺度的image的需求。你不管输入图片是 <img src="https://www.zhihu.com/equation?tex=224%5Ctimes224" alt="[公式]"> 还是 <img src="https://www.zhihu.com/equation?tex=256%5Ctimes256" alt="[公式]"> ，最后都给你映射为 <img src="https://www.zhihu.com/equation?tex=1000%5Ctimes1" alt="[公式]">。</p>
<p>这对提高检测器的性能有什么作用呢？</p>
<p>对于小目标的检测，之前输入图片是固定的大小的，小目标很难被检测准确；现在允许多尺度输入图片了，只要把图片放大，小目标就变成了大目标，提高检测的精度。</p>
<ul>
<li>为什么最后一层是softmax？</li>
</ul>
<p>**答：**因为backbone网络darknet-19是单独train的，是按照分类网络去train的，用的数据集是imagenet，是1000个classes，所以最后加了一个softmax层，使用cross entropy loss。</p>
<p><strong>接下来总结下YOLO v2的网络结构：</strong></p>
<ul>
<li>图4中的双横线的上半部分(第0-22层)为backbone，train的方法如上文。</li>
<li>后面的结构如下图5所示：</li>
</ul>
<p><img src="https://pic3.zhimg.com/80/v2-1498b3c3c5ebebbb31b871329bc97b2e_720w.jpg" alt="img">图5：YOLO v2网络结构</p>
<ul>
<li>从第23层开始为检测头，其实是3个 3 * 3 * 1024 的卷积层。</li>
<li>同时增加了一个 passthrough 层(27层)，最后使用 1 * 1 卷积层输出预测结果，输出结果的size为 <img src="https://www.zhihu.com/equation?tex=13%5Ctimes13%5Ctimes125" alt="[公式]"> 。</li>
<li>route层的作用是进行层的合并(concat)，后面的数字指的是合并谁和谁。</li>
<li>passthrough 层可以把 <img src="https://www.zhihu.com/equation?tex=26%5Ctimes26%5Ctimes64%5Crightarrow13%5Ctimes13%5Ctimes256" alt="[公式]"> 。</li>
</ul>
<p><strong>YOLO2 的训练主要包括三个阶段：</strong></p>
<ol>
<li>先在 <strong>ImageNet 分类数据集</strong>上<strong>预训练 Darknet-19</strong>，此时模型输入为 <strong>224 * 224</strong> ，共训练 160 个 epochs。(为什么可以这样训练？因为有GAP)</li>
<li>将网络的输入调整为 <strong>448 * 448</strong>（注意在测试的时候使用 <strong>416 * 416</strong> 大小） ，继续在 ImageNet 数据集上 <strong>finetune 分类模型</strong>，训练 10 个 epochs。注意为什么测试的时候使用大小？<strong>答案是：<strong>将输入图像大小从</strong>448 ×448</strong> 更改为 <strong>416 ×416</strong> 。这将创建<strong>奇数空间维度(<strong>7×7 v.s 8 ×8 <strong>grid cell)</strong>。 图片的中心通常被大目标占据。 对于奇数网格单元，可以更容易确定目标所属的位置。对于一些大目标，它们中心点往落入图片中心位置，此时使用特征图中心的</strong>1个cell</strong>去预测这些目标的bounding box相对容易些，否则就要用中间<strong>4个Cells</strong>来进行预测。</li>
<li>修改 Darknet-19 分类模型为检测模型为图5形态，即：移除最后一个卷积层、global avgpooling 层以及 softmax 层，并且新增了3个 3 * 3 * 1024 卷积层，同时增加了一个 passthrough 层，最后使用 1 * 1 卷积层输出预测结果，并在<strong>检测数据集</strong>上继续<strong>finetune</strong> 网络。</li>
<li>YOLO v1的训练：先使用<strong>224 * 224</strong>的分辨率训练分类网络，再切换到<strong>448 * 448</strong>的分辨率训练检测网络。而YOLO v2在使用<strong>224 * 224</strong>的分辨率训练分类网络<strong>160 epochs</strong>之后，先使用<strong>448 * 448</strong>的分辨率finetune分类网络<strong>10 epochs</strong>，再使用<strong>448 * 448</strong>的分辨率训练检测网络。可提升4%mAP。**为什么可以改变输入维度了？**因为YOLO v2中使用了GAP(Global Average Pooling)不论输入图片多大最后都给你从 <img src="https://www.zhihu.com/equation?tex=1000%5Ctimes7%5Ctimes7%5Crightarrow1000%5Ctimes1" alt="[公式]"> ，进行分类训练。</li>
</ol>
<blockquote>
<p>注意这里图5有个地方得解释一下：第25层把第16层进行reorg，即passthrough操作，得到的结果为27层，再与第24层进行route，即concat操作，得到第28层。</p>
</blockquote>
<p><strong>可视化的图为：</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-33a0c926dc46085ac913b5262edb3a0a_720w.jpg" alt="img">图5：YOLO v2可视化结构</p>
<h2 id="YOLO-v3">YOLO v3</h2>
<p>先看下YOLO v3的backbone，如下图6所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-2bc3e653ec7744e36e45960d0cc060c9_720w.jpg" alt="img">图6：YOLO v3 backbone</p>
<p>先声明下darknet 53指的是convolution层有52层+1个conv层把1024个channel调整为1000个，你会发现YOLO v2中使用的GAP层在YOLO v3中还在用，他还是在ImageNet上先train的backbone，</p>
<p>观察发现依然是有bottleneck的结构和<strong>残差网络</strong>。</p>
<p><strong>为什么</strong>YOLO v3敢用3个检测头**？因为**他的backbone更强大了。</p>
<p><strong>为什么</strong>更强大了**？因为**当时已经出现了ResNet结构。</p>
<p>所以YOLO v3的提高，有一部分功劳应该给ResNet。</p>
<p>再观察发现YOLO v3没有Pooling layer了，用的是conv(stride = 2)进行下采样，<strong>为什么？</strong></p>
<p><strong>因为</strong>Pooling layer，不管是MaxPooling还是Average Pooling，本质上都是下采样减少计算量，本质上就是不更新参数的conv，但是他们会损失信息，所以用的是conv(stride = 2)进行下采样。</p>
<p>下图7是YOLO v3的网络结构：</p>
<p><img src="https://pic2.zhimg.com/80/v2-997f8c0bc3bde01329a52ba6afa49cfd_720w.jpg" alt="img">图7：YOLO v3 Structure</p>
<p><img src="https://pic4.zhimg.com/80/v2-3946e19705fcabb730daf30f9cd1408f_720w.jpg" alt="img">图8：YOLO v3 Structure</p>
<p>特征融合的方式更加直接，没有YOLO v2的passthrough操作，直接上采样之后concat在一起。</p>
<hr>
<h2 id="YOLO-v4">YOLO v4</h2>
<p>图9,10展示了YOLO v4的结构：</p>
<p><img src="https://pic2.zhimg.com/80/v2-0ea4884cd89aaf5b87ba1464d02f358d_720w.jpg" alt="img">图9：YOLO v4 Structure</p>
<p><img src="https://pic3.zhimg.com/80/v2-05a713ffc8cee40dbaa3b808aa63dcea_720w.jpg" alt="img">图10：YOLO v4 Structure</p>
<p><img src="https://pic1.zhimg.com/80/v2-c70a13fe3c658d20e9d11a1880619e70_720w.jpg" alt="img">图11：YOLO v4 Structure</p>
<p>Yolov4的结构图和Yolov3相比，因为多了<strong>CSP结构，PAN结构</strong>，如果单纯看可视化流程图，会觉得很绕，不过在绘制出上面的图形后，会觉得豁然开朗，其实整体架构和Yolov3是相同的，不过使用各种新的算法思想对各个子结构都进行了改进。</p>
<p><strong>YOLOv4使用了CSPDarknet53作为backbone，加上SPP模块，PANET作为neck，以及YOLO v3的head。</strong></p>
<p><strong>Yolov4的五个基本组件</strong>：</p>
<ol>
<li>**CBM：**Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。</li>
<li>**CBL：**由Conv+Bn+Leaky_relu激活函数三者组成。</li>
<li>**Res unit：**借鉴Resnet网络中的残差结构，让网络可以构建的更深。</li>
<li>**CSPX：**借鉴CSPNet网络结构，由三个卷积层和X个Res unint模块Concate组成。</li>
<li>**SPP：**采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。</li>
</ol>
<p><strong>其他基础操作：</strong></p>
<ol>
<li>**Concat：**张量拼接，维度会扩充，和Yolov3中的解释一样，对应于cfg文件中的route操作。</li>
<li>**add：**张量相加，不会扩充维度，对应于cfg文件中的shortcut操作。</li>
</ol>
<p><strong>Backbone中卷积层的数量：</strong></p>
<p>和Yolov3一样，再来数一下Backbone里面的卷积层数量。</p>
<p>每个CSPX中包含3+2<em>X个卷积层，因此整个主干网络Backbone中一共包含2+（3+2</em>1）+2+（3+2<em>2）+2+（3+2</em>8）+2+（3+2<em>8）+2+（3+2</em>4）+1=72。</p>
<ul>
<li><strong>输入端的改进：</strong></li>
</ul>
<p>YOLO v4对输入端进行了改进，主要包括<strong>数据增强Mosaic、cmBN、SAT自对抗训练</strong>，使得在卡不是很多时也能取得不错的结果。</p>
<p>这里介绍下数据增强Mosaic：</p>
<p><img src="https://pic1.zhimg.com/80/v2-9ddb5f309a28e99f6c1901ec908923e4_720w.jpg" alt="img">Mosaic数据增强</p>
<p><strong>CutMix</strong>只使用了两张图片进行拼接，而<strong>Mosaic数据增强</strong>则采用了4张图片，<strong>随机缩放、随机裁剪、随机排布</strong>的方式进行拼接。</p>
<p>Yolov4的作者采用了<strong>Mosaic数据增强</strong>的方式。</p>
<p>主要有几个优点：</p>
<ol>
<li><strong>丰富数据集：<strong>随机使用</strong>4张图片</strong>，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。</li>
<li>**减少GPU：**可能会有人说，随机缩放，普通的数据增强也可以做，但作者考虑到很多人可能只有一个GPU，因此Mosaic增强训练时，可以直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果。</li>
</ol>
<p>cmBN的方法如下图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-bc8cfe6198d8c2a0d8a3a646a27a6e0d_720w.jpg" alt="img">cmBN的方法</p>
<hr>
<h2 id="YOLO-v5">YOLO v5</h2>
<p>图12,13展示了YOLO v5的结构：</p>
<p><img src="https://pic2.zhimg.com/80/v2-2ca7dfc26a64e7ca0e5e8b21c87ed13d_720w.jpg" alt="img">图12：YOLO v5 Structure</p>
<p><img src="https://pic2.zhimg.com/80/v2-31ca3528d3bb53273f74a23b5b59ab51_720w.jpg" alt="img">图13：YOLO v5 Structure</p>
<p>检测头的结构基本上是一样的，融合方法也是一样。</p>
<p><strong>Yolov5的基本组件</strong>：</p>
<ol>
<li>**Focus：**基本上就是YOLO v2的passthrough。</li>
<li>**CBL：**由Conv+Bn+Leaky_relu激活函数三者组成。</li>
<li>**CSP1_X：**借鉴CSPNet网络结构，由三个卷积层和X个Res unint模块Concate组成。</li>
<li>**CSP2_X：**不再用Res unint模块，而是改为CBL。</li>
<li>**SPP：**采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合，如图13所示。</li>
</ol>
<p>提特征的网络变短了，速度更快。YOLO v5的结构没有定下来，作者的代码还在持续更新。</p>
<h2 id="YOLO-v5的四种结构的深度："><strong>YOLO v5的四种结构的深度：</strong></h2>
<p>下图展示了YOLO v5的四种结构：</p>
<p><img src="https://pic3.zhimg.com/80/v2-6c383a7d5fdafda4a18799b1a1e7585e_720w.jpg" alt="img">图14：YOLO四种结构深度</p>
<p>Yolov5代码中，每个网络结构的两个参数：</p>
<p><strong>（1）Yolov5s.yaml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">depth_multiple: 0.33  # model depth multiple</span><br><span class="line">width_multiple: 0.50  # layer channel multiple</span><br></pre></td></tr></table></figure>
<p><strong>（2）Yolov5m.yaml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">depth_multiple: 0.67  # model depth multiple</span><br><span class="line">width_multiple: 0.75  # layer channel multiple</span><br></pre></td></tr></table></figure>
<p><strong>（3）Yolov5l.yaml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">depth_multiple: 1.0  # model depth multiple</span><br><span class="line">width_multiple: 1.0  # layer channel multiple</span><br></pre></td></tr></table></figure>
<p><strong>（4）Yolov5x.yaml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">depth_multiple: 1.33  # model depth multiple</span><br><span class="line">width_multiple: 1.25  # layer channel multiple</span><br></pre></td></tr></table></figure>
<p>四种结构就是通过上面的两个参数，来进行控制网络的<strong>深度</strong>和<strong>宽度</strong>。其中<strong>depth_multiple</strong>控制网络的<strong>深度</strong>，<strong>width_multiple</strong>控制网络的<strong>宽度</strong>。</p>
<h3 id="Yolov5网络结构">Yolov5网络结构</h3>
<p>四种结构的yaml文件中，下方的网络架构代码都是一样的。</p>
<p>将Backbone部分提取出来，讲解如何控制网络的宽度和深度，yaml文件中的Head部分也是同样的原理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># YOLOv5 backbone</span><br><span class="line">backbone:</span><br><span class="line">  # [from, number, module, args]</span><br><span class="line">  [[-1, 1, Focus, [64, 3]],  # 0-P1/2</span><br><span class="line">   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4</span><br><span class="line">   [-1, 3, BottleneckCSP, [128]],</span><br><span class="line">   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8</span><br><span class="line">   [-1, 9, BottleneckCSP, [256]],</span><br><span class="line">   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16</span><br><span class="line">   [-1, 9, BottleneckCSP, [512]],</span><br><span class="line">   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32</span><br><span class="line">   [-1, 1, SPP, [1024, [5, 9, 13]]],</span><br><span class="line">   [-1, 3, BottleneckCSP, [1024, False]],  # 9</span><br><span class="line">  ]</span><br></pre></td></tr></table></figure>
<p>在对网络结构进行解析时，yolo.py中下方的这一行代码将四种结构的<strong>depth_multiple</strong>，<strong>width_multiple</strong>提取出，赋值给<strong>gd，gw</strong>。后面主要对这<strong>gd，gw</strong>这两个参数进行讲解。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">anchors, nc, gd, gw = d[&#x27;anchors&#x27;], d[&#x27;nc&#x27;], d[&#x27;depth_multiple&#x27;], d[&#x27;width_multiple&#x27;]</span><br></pre></td></tr></table></figure>
<p>下面再细致的剖析下，看是如何控制每种结构，深度和宽度的。</p>
<p><strong>1) 不同网络的深度</strong></p>
<p>在上图中有2种结构：CSP1和CSP2，其中CSP1结构主要应用于<strong>Backbone</strong>中，CSP2结构主要应用于<strong>Neck</strong>中。</p>
<p><strong>需要注意的是，四种网络结构中每个CSP结构的深度都是不同的。</strong></p>
<p>a.以yolov5s为例，第一个CSP1中，使用了<strong>1个残差组件</strong>，因此是<strong>CSP1_1</strong>。而在Yolov5m中，则增加了网络的深度，在第一个CSP1中，使用了<strong>2个残差组件</strong>，因此是<strong>CSP1_2</strong>。</p>
<p>而Yolov5l中，同样的位置，则使用了<strong>3个残差组件</strong>，Yolov5x中，使用了<strong>4个残差组件</strong>。</p>
<p>其余的第二个CSP1和第三个CSP1也是同样的原理。</p>
<p>b.在第二种CSP2结构中也是同样的方式，以第一个CSP2结构为例，Yolov5s组件中使用了<strong>1组</strong>卷积，因此是<strong>CSP2_1</strong>。</p>
<p>而Yolov5m中使用了<strong>2组</strong>，Yolov5l中使用了<strong>3组</strong>，Yolov5x中使用了<strong>4组。</strong></p>
<p>其他的四个CSP2结构，也是同理。</p>
<p>Yolov5中，网络的不断加深，也在不断<strong>增加网络特征提取</strong>和<strong>特征融合</strong>的能力。</p>
<p><strong>2) 控制深度的代码</strong></p>
<p>控制四种网络结构的核心代码是<strong><a target="_blank" rel="noopener" href="http://yolo.py">yolo.py</a></strong>中下面的代码，存在两个变量，<strong>n和gd</strong>。</p>
<p>我们再将<strong>n和gd</strong>带入计算，看每种网络的变化结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n = max(round(n * gd), 1) if n &gt; 1 else n  # depth gain</span><br></pre></td></tr></table></figure>
<p><strong>3) 验证控制深度的有效性</strong></p>
<p>我们选择<strong>最小的yolov5s.yaml</strong>和中间的<strong>yolov5l.yaml</strong>两个网络结构，将**gd(height_multiple)**系数带入，看是否正确。</p>
<p><img src="https://pic1.zhimg.com/80/v2-2bc10e474354757a4d7947e219f56cc0_720w.jpg" alt="img"></p>
<p><strong>a. yolov5x.yaml</strong></p>
<p>其中<strong>height_multiple=0.33</strong>，即<strong>gd=0.33</strong>，而n则由上面红色框中的信息获得。</p>
<p>以上面网络框图中的第一个CSP1为例，即上面的第一个红色框。n等于第二个数值3。</p>
<p>而<strong>gd=0.33</strong>，带入（2）中的计算代码，结果n=1。因此第一个CSP1结构内只有1个残差组件，即CSP1_1。</p>
<p>第二个CSP1结构中，n等于第二个数值9，而<strong>gd=0.33</strong>，带入（2）中计算，结果<strong>n=3</strong>，因此第二个CSP1结构中有3个残差组件，即CSP1_3。</p>
<p>第三个CSP1结构也是同理，这里不多说。</p>
<p><strong>b. yolov5l.xml</strong></p>
<p>其中<strong>height_multiple=1</strong>，即<strong>gd=1</strong></p>
<p>和上面的计算方式相同，第一个CSP1结构中，n=1，带入代码中，结果n=3，因此为CSP1_3。</p>
<p>下面第二个CSP1和第三个CSP1结构都是同样的原理。</p>
<h2 id="YOLO-v5的四种结构的宽度：">YOLO v5的四种结构的宽度：</h2>
<p><img src="https://pic3.zhimg.com/80/v2-76ec41e464f5ce3ce103136837937112_720w.jpg" alt="img">图15：YOLO四种结构宽度</p>
<p><strong>1) 不同网络的宽度:</strong></p>
<p>如上图表格中所示，四种yolov5结构在不同阶段的卷积核的数量都是不一样的，因此也直接影响卷积后特征图的第三维度，即<strong>宽度</strong>。</p>
<p>a.以Yolo v5s结构为例，第一个Focus结构中，最后卷积操作时，卷积核的数量是32个，因此经过<strong>Focus结构</strong>，特征图的大小变成<strong>304*304*32</strong>。</p>
<p>而Yolo v5m的<strong>Focus结构</strong>中的卷积操作使用了48个卷积核，因此<strong>Focus结构</strong>后的特征图变成3<strong>04*304*48</strong>。yolov5l，yolov5x也是同样的原理。</p>
<p>b. 第二个卷积操作时，Yolo v5s使用了64个卷积核，因此得到的特征图是<strong>152*152*64</strong>。而yolov5m使用96个特征图，因此得到的特征图是<strong>152*152*96</strong>。Yolo v5l，Yolo v5x也是同理。</p>
<p>c. 后面三个卷积下采样操作也是同样的原理。</p>
<p>四种不同结构的卷积核的数量不同，这也直接影响网络中，比如<strong>CSP1，CSP2等结构</strong>，以及各个普通卷积，卷积操作时的卷积核数量也同步在调整，影响整体网络的计算量。</p>
<p>当然卷积核的数量越多，特征图的厚度，即<strong>宽度越宽</strong>，网络提取特征的<strong>学习能力也越强</strong>。</p>
<ol start="2">
<li><strong>控制宽度的代码</strong></li>
</ol>
<p>在Yolo v5的代码中，控制宽度的核心代码是<strong><a target="_blank" rel="noopener" href="http://yolo.py">yolo.py</a></strong>文件里面的这一行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c2 = make_divisible(c2 * gw, 8) if c2 != no else c2</span><br></pre></td></tr></table></figure>
<p>它所调用的子函数<strong>make_divisible</strong>是一个很出名的函数，在各大任务中均会使用，作用是：</p>
<p>**make_divisible(A,B)：**找到比A大的，能整除B的最小整数。</p>
<blockquote>
<p>make_divisible(54,8) = 56</p>
</blockquote>
<p><strong>3) 验证控制宽度的有效性</strong></p>
<p>我们还是选择<strong>最小的Yolo v5s</strong>和<strong>中间的Yolo v5l</strong>两个网络结构，将<strong>width_multiple</strong>系数带入，看是否正确。</p>
<p><img src="https://pic2.zhimg.com/80/v2-7544dc64d6cfce2fb0dbe69bec73e3e1_720w.jpg" alt="img">图16：backbone</p>
<p><strong>a. Yolo v5x.yaml</strong></p>
<p>其中<strong>width_multiple=0.5</strong>，即<strong>gw=0.5</strong>。</p>
<p>以第一个卷积下采样为例，即Focus结构中下面的卷积操作。</p>
<p>按照上面Backbone的信息，我们知道Focus中，标准的c2=64，而<strong>gw=0.5</strong>，代入（2）中的计算公式，最后的结果=32。即Yolo v5s的Focus结构中，卷积下采样操作的卷积核数量为<strong>32个。</strong></p>
<p>再计算后面的第二个卷积下采样操作，标准c2的值=128，<strong>gw=0.5</strong>，代入（2）中公式，最后的结果=64，也是正确的。</p>
<p><strong>b. Yolo v5l.yaml</strong></p>
<p>其中<strong>width_multiple=1</strong>，即<strong>gw=1</strong>，而标准的<strong>c2=64</strong>，代入上面（2）的计算公式中，可以得到Yolo v5l的Focus结构中，卷积下采样操作的卷积核的数量为64个，而第二个卷积下采样的卷积核数量是128个。</p>
<p>另外的三个卷积下采样操作，以及<strong>Yolo v5m，Yolo v5x结构</strong>也是同样的计算方式。</p>
<p>比如：YOLO V5s默认depth_multiple=0.33， width_multiple=0.50。即BottleneckCSP中Bottleneck的数量为<strong>默认的1/3</strong>，而所有卷积操作的卷积核个数均为<strong>默认的1/2。</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-c1422e55da6ba5030371f0111d8d4e8f_720w.jpg" alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-4c47b276ff2f51fb59a056e961666f67_720w.jpg" alt="img">图17：SPP结构</p>
<p><strong>Focus操作如下图所示：</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-560db4a00dd21975f64e7addff498ecc_720w.jpg" alt="img">图18：Focus操作</p>
<p><strong>Focus</strong>的<strong>slice</strong>操作如下图所示：</p>
<p><img src="https://pic3.zhimg.com/80/v2-eeec8b5b35b9ab4f096dfbfb229ea1fa_720w.jpg" alt="img">图19：slice操作</p>
<p>这个其实就是<strong>Yolo v2</strong>里面的<strong>ReOrg+Conv</strong>操作，也是亚像素卷积的反向操作版本，简单来说就是把数据切分为4份，每份数据都是相当于2倍下采样得到的，然后在channel维度进行拼接，最后进行卷积操作。其最大好处是可以<strong>最大程度的减少信息损失而进行下采样操作</strong>。</p>
<p><strong>YOLO v5s</strong>默认<strong>3x640x640</strong>的输入，复制四份，然后通过切片操作将这个四个图片切成了四个<strong>3x320x320</strong>的切片，接下来使用concat从深度上连接这四个切片，输出为<strong>12x320x320</strong>，之后再通过卷积核数为32的卷积层，生成<strong>32x320x320</strong>的输出，最后经过batch_borm 和leaky_relu将结果输入到下一个卷积层。</p>
<p><strong>Focus的代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Focus(nn.Module):</span><br><span class="line">    # Focus wh information into c-space</span><br><span class="line">    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups</span><br><span class="line">        super(Focus, self).__init__()</span><br><span class="line">        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):  # x(b,c,w,h) -&gt; y(b,4c,w/2,h/2)</span><br><span class="line">        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))</span><br></pre></td></tr></table></figure>
<p>这里解释以下<strong>PANET</strong>结构是什么意思，PAN结构来自论文Path Aggregation Network，可视化结果如图19所示：</p>
<p><img src="https://pic3.zhimg.com/80/v2-93d93db2c7f60ebf3fa5d894d1b446f2_720w.jpg" alt="img">图20：PAN结构</p>
<p>可以看到包含了自底向上和自顶向下的连接，值得注意的是这里的红色虚线和绿色虚线：</p>
<p>FPN的结构把浅层特征传递给顶层要经历<strong>几十甚至上百层</strong>，显然经过这么多层的传递，浅层信息(小目标)丢失比较厉害。这里的<strong>红色虚线</strong>就象征着ResNet的几十甚至上百层。</p>
<p><strong>自下而上的路径</strong>由<strong>不到10层</strong>组成，<strong>浅层特征</strong>经过FPN的<strong>laterial connection</strong>连接到 <img src="https://www.zhihu.com/equation?tex=P_%7B2%7D" alt="[公式]"> ，再经过<strong>bottom-up path augmentation</strong>连接到顶层，经过的层数不到10层，能较好地保留浅层的信息。这里的<strong>绿色虚线</strong>就象征着自下而上的路径的不到10层。</p>
<p>YOLO V5借鉴了YOLO V4的<strong>修改版PANET</strong>结构。</p>
<p><strong>PANET</strong>通常使用自适应特征池将相邻层<strong>加</strong>在一起，以进行掩模预测。但是，当在YOLO v4中使用PANET时，此方法略麻烦，因此，YOLO v4的作者没有使用自适应特征池添加相邻层，而是对其进行<strong>Concat</strong>操作，从而提高了预测的准确性。</p>
<p><img src="https://pic4.zhimg.com/80/v2-98fce9f04482ad10b4fb0c04281362df_720w.jpg" alt="img">图21：Modified PAN</p>
<ul>
<li><strong>输入端的改进：</strong></li>
</ul>
<p><strong>1.Mosaic数据增强，和YOLO v4一样。</strong></p>
<p><strong>2.自适应锚框计算：</strong></p>
<p>在Yolo算法中，针对不同的数据集，都会有<strong>初始设定长宽的锚框</strong>。</p>
<p>在网络训练中，网络在初始锚框的基础上输出预测框，进而和<strong>真实框groundtruth</strong>进行比对，计算两者差距，再反向更新，<strong>迭代网络参数</strong>。</p>
<p>因此初始锚框也是比较重要的一部分，比如Yolov5在Coco数据集上初始设定的锚框：</p>
<p><img src="https://pic1.zhimg.com/80/v2-f8a55d48eb2003b47629f4e92d7ab5a8_720w.jpg" alt="img">Yolov5在Coco数据集上初始设定的锚框</p>
<p>在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。</p>
<p>但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。</p>
<p>当然，如果觉得计算的锚框效果不是很好，也可以在代码中将自动计算锚框功能<strong>关闭</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&#x27;--noautoanchor&#x27;, action=&#x27;store_true&#x27;, help=&#x27;disable autoanchor check&#x27;)</span><br></pre></td></tr></table></figure>
<p>设置成<strong>False</strong>，每次训练时，不会自动计算。</p>
<p><strong>3.自适应图片缩放</strong></p>
<p>在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。</p>
<p>比如Yolo算法中常用<strong>416*416，608*608</strong>等尺寸，比如对下面<strong>800*600</strong>的图像进行缩放:，如图15所示：</p>
<p><img src="https://pic3.zhimg.com/80/v2-21213f06cfd4b9025ac0aec29e2e8a82_720w.jpg" alt="img">图22：Yolo算法的缩放填充</p>
<p>但<strong>Yolov5代码</strong>中对此进行了改进，也是<strong>Yolov5推理速度</strong>能够很快的一个不错的trick。</p>
<p>作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果<strong>填充的比较多</strong>，则存在<strong>信息冗余</strong>，影响<strong>推理速度</strong>。</p>
<p>因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像<strong>自适应的添加最少的黑边</strong>，如图16所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-d9d3685aae6ba2f99e1c7c27a4f1ae53_720w.jpg" alt="img">图22：YOLO v5的自适应填充</p>
<p>图像高度上两端的黑边变少了，在推理时，计算量也会减少，即目标检测速度会得到提升。</p>
<p>通过这种简单的改进，推理速度得到了37%的提升，可以说效果很明显。</p>
<p>Yolov5中填充的是灰色，即（114,114,114），都是一样的效果，且训练时没有采用缩减黑边的方式，还是采用传统填充的方式，即缩放到416*416大小。只是在测试，使用模型推理时，才采用缩减黑边的方式，提高目标检测，推理的速度。</p>
<p><strong>关于样本：</strong></p>
<p>yolov5的很大区别就是在于正样本区域的定义。<strong>在yolov3中，每一个</strong> <img src="https://www.zhihu.com/equation?tex=GT" alt="[公式]"> **只有1个对应的anchor，**设定的规则是取 <img src="https://www.zhihu.com/equation?tex=IoU%28anchor%2CGT%29" alt="[公式]"> 最大的anchor。而且某个 <img src="https://www.zhihu.com/equation?tex=GT" alt="[公式]"> 一定不可能在三个预测层的某几层上同时进行匹配。这样就会导致一个问题：正样本的anchor太少了，导致整体收敛比较慢。后面的FCOS等工作研究表明：<strong>增加高质量正样本anchor可以显著加速收敛</strong>。</p>
<p>YOLO v5和前YOLO系列相比，特点应该是：</p>
<ul>
<li><strong>(1) 增加了正样本：方法是邻域的正样本anchor匹配策略。</strong></li>
<li><strong>(2) 通过灵活的配置参数，可以得到不同复杂度的模型</strong></li>
<li><strong>(3) 通过一些内置的超参优化策略，提升整体性能</strong></li>
<li><strong>(4) 和yolov4一样，都用了mosaic增强，提升小物体检测性能</strong></li>
</ul>
<hr>
<p>最后我们对YOLO series总的做个比较，结束这个系列的解读，喜欢的同学点赞关注评论3连：</p>
<p><img src="https://pic1.zhimg.com/80/v2-ba712736fadbf364cc2fec97dd374f98_720w.jpg" alt="img"></p>
<p>YOLO v5 series的比较</p>
<h1>写给小白的Yolo介绍</h1>
<h2 id="YOLO是什么？">YOLO是什么？</h2>
<p>YOLO是目标检测模型。</p>
<p>目标检测是计算机视觉中比较简单的任务，用来在一张图篇中找到<strong>某些特定的物体</strong>，目标检测不仅要求我们识别这些物体的<strong>种类</strong>，同时要求我们标出这些物体的<strong>位置</strong>。</p>
<p>显然，类别是离散数据，位置是连续数据。</p>
<p><img src="https://pic1.zhimg.com/80/v2-88ca6ec106e7647a0390ad3d93625444_720w.jpg" alt="img"></p>
<p>上面的图片中，分别是计算机视觉的三类任务：分类，目标检测，实例分割。</p>
<p>很显然，整体上这三类任务从易到难，我们要讨论的目标检测位于中间。前面的分类任务是我们做<strong>目标检测的基础</strong>，至于<strong>像素级别</strong>的实例分割，太难了别想了。</p>
<p>YOLO在2016年被提出，发表在计算机视觉顶会<strong>CVPR</strong>(Computer Vision and Pattern Recognition)上，论文的国内镜像在这里：<a href="https://link.zhihu.com/?target=http%3A//xxx.itp.ac.cn/pdf/1506.02640v5">1</a></p>
<p>YOLO的全称是you only look once，指<strong>只需要</strong>浏览一次就可以识别出图中的物体的类别和位置。</p>
<p>因为只需要看一次，YOLO被称为Region-free方法，相比于Region-based方法，YOLO不需要提前找到<strong>可能存在目标的Region</strong>。</p>
<p>也就是说，一个典型的Region-base方法的流程是这样的：先通过计算机图形学（或者深度学习）的方法，对图片进行分析，找出若干个可能存在物体的区域，将这些区域裁剪下来，放入一个图片分类器中，由分类器分类。</p>
<p>因为YOLO这样的Region-free方法只需要一次扫描，也被称为<strong>单阶段</strong>（1-stage）模型。Region-based方法方法也被称为<strong>两阶段</strong>（2-stage）方法。</p>
<hr>
<h2 id="YOLO之前的世界">YOLO之前的世界</h2>
<p>YOLO之前的世界，额，其实是R-CNN什么的，也就是我们前面说的Region-based方法，但是感觉还是太高端了。我们从用脚都能想到的目标检测方法开始讲起。</p>
<p>如果我们现在有一个<strong>分类器：</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-21d1a2b0e0381fa30e69eeaa7f24384e_720w.jpg" alt="img"></p>
<p>这只猫咪好可爱，想撸。</p>
<p>现在我们的追求升级了，我们不仅仅想处理这种<strong>一张图片中只有一个物体</strong>的图片，我们现在想处理有多个物体的图片。</p>
<p>我们该什么做呢？</p>
<p>首先有几点我们要实现想到：<strong>首先</strong>物体的位置是不确定的，你没办法保证物体一定在最中间；<strong>其次</strong>，物体的大小是不确定的，有的物体比较大，也有的物体比较小，注意，这里不是说大象一定更大，猫咪一定更小，毕竟还有近大远小嘛；<strong>然后</strong>，我们还没办法保证物体的种类，假设我们有一个可以识别100中物体的分类器，那么起码图片中出现了这100种物体我们都要识别出来。</p>
<p>比如说这样：</p>
<p><img src="https://pic1.zhimg.com/80/v2-e56c54e419838b8e45ad211df5e38058_720w.jpg" alt="img"></p>
<p>挺难的，是吧？</p>
<p>最naive的方法是<strong>滑窗法</strong>，就是用滑动窗口去识别一个个物体。</p>
<p>比如这样：</p>
<p><img src="https://pic1.zhimg.com/80/v2-b99bfd4ae6466b6a5c9274b40c544d10_720w.jpg" alt="img"></p>
<p>上图的红色框框就是所谓的滑窗。如果一个物体<strong>正好</strong>出现在一个滑窗中，那么我们就可以把它检测出来了，这个滑窗的位置也就是我们认为<strong>这个物体所在的位置</strong>。</p>
<p>等下，如果物体没有正好出现在一个滑窗中呢？</p>
<p>我们管滑窗每次滑动的距离叫做<strong>步长</strong>，如果我们把步长设置的特别小，如果步长仅仅为一个像素点，那<strong>一定可以</strong>保证物体可以<strong>正好</strong>出现在某个窗口中了。</p>
<p>那如果某个物体特别大，或者特别小呢？</p>
<p>例如在上图中，每个窗口和汽车差不多大小，但是如果我们要识别一辆卡车，一个窗口可能就不够大了。</p>
<p>显然，我们可以设计不同大小的窗口，我们可以设计几十中不同大小的窗口，让他们按照最小的步长滑动，把窗口里的所有图片都放入分类器中。</p>
<p>但是这样太太太浪费时间了。</p>
<p>到这里R-CNN同学出现了，他说，你这样用滑窗法可能最后得到了几十万个窗口，而我可以提前扫描一下图片，得到2000个左右的Region（其实就是前面的窗口），这样不就节省了很多时间？</p>
<p><a href="https://link.zhihu.com/?target=http%3A//xxx.itp.ac.cn/pdf/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentationxxx.itp.ac.cn/pdf/1311.2524</a></p>
<p>R-CNN同学管这个叫做Region Proposal，并且提出了一个叫做Selective Search的算法。（吐槽一下这个名字起得太大众了）</p>
<p>但是R-CNN被YOLO打脸了，YOLO说，我更快。</p>
<hr>
<h2 id="YOLO原理">YOLO原理</h2>
<p>在这之前，我们再重申一下我们的任务。我们的目的是在一张图片中找出物体，并给出它的类别和位置。目标检测是基于监督学习的，每张图片的监督信息是它所包含的N个物体，每个物体的信息有<strong>五个</strong>，分别是物体的<strong>中心位置(x,y)<strong>和它的</strong>高(h)<strong>和</strong>宽(w)</strong>，最后是它的类别。</p>
<p>YOLO 的预测是基于整个图片的，并且它会一次性输出所有检测到的目标信息，包括类别和位置。</p>
<p>就好像捕鱼一样，R-CNN是先选好哪里可能出现鱼，而YOLO是直接一个大网下去，把所有的鱼都捞出来。</p>
<p>先假设我们处理的图片是一个<strong>正方形</strong>。</p>
<p>YOLO的第一步是<strong>分割图片</strong>，它将图片分割为 <img src="https://www.zhihu.com/equation?tex=s%5E2" alt="[公式]"> 个grid，每个grid的大小都是相等的，像这样：</p>
<p><img src="https://pic1.zhimg.com/80/v2-01c11f4d423a698271159fcc98524894_720w.jpg" alt="img"></p>
<p>如果我们让每个框只能识别出一个物体，且要求这个物体必须在这个框之内，那YOLO就变成了很蠢的滑窗法了。</p>
<p>YOLO的聪明之处在于，它只要求这个物体的<strong>中心</strong>落在这个框框之中。</p>
<p>这意味着，我们不用设计非常非常<strong>大</strong>的框，因为我们只需要让物体的中心在这个框中就可以了，而不是必须要让整个物体都在这个框中。</p>
<p>具体怎么实现呢？</p>
<p>我们要让这个 <img src="https://www.zhihu.com/equation?tex=S%5E2" alt="[公式]"> 个框每个都预测出<strong>B个bounding boxs</strong>，这个bounding boxs有5个量，分别是物体的<strong>中心位置(x,y)<strong>和它的</strong>高(h)<strong>和</strong>宽(w)</strong>，以及这次预测的<strong>置信度</strong>。</p>
<p>每个框框不仅只预测B个bounding boxs，它还要负责预测这个框框中的物体<strong>是什么类别</strong>的，这里的类别用one-hot编码表示。</p>
<p>注意，虽然一个框框有多个bounding boxes，但是只能识别出一个物体，因此每个框框需要预测物体的类别，<strong>而bounding box不需要</strong>。</p>
<p>也就是说，如果我们有 <img src="https://www.zhihu.com/equation?tex=S%5E2" alt="[公式]"> 个框框，每个框框的bounding boxes个数为B，分类器可以识别出C种不同的物体，那么所有整个ground truth的长度为：</p>
<p><img src="https://www.zhihu.com/equation?tex=S+%5Ctimes+S++%5Ctimes+%28B++%5Ctimes+5++%2BC%29" alt="[公式]"></p>
<p>先看这些bounding boxs显示出来是什么样的：</p>
<p><img src="https://pic3.zhimg.com/80/v2-019dcd8dae979fa96f48fc23e70e25a2_720w.jpg" alt="img"></p>
<p>在上面的例子中，图片被分成了49个框，每个框预测2个bounding boxs，因此上面的图中有98个bounding boxs。</p>
<p>可以看到大致上每个框里确实有两个bounding boxs。</p>
<p>可以看到这些BOX中有的边框比较粗，有的比较细，这是<strong>置信度</strong>不同的表现，置信度高的比较粗，置信度低的比较细。</p>
<hr>
<p>在详细的介绍confidence之前，我们先来说一说关于bounding boxs的细节。</p>
<p>bounding boxs可以锁定物体的位置，这要求它输出四个关于位置的值，分别是x,y,h和w。我们在处理输入的图片的时候想让图片的大小<strong>任意</strong>，这一点对于卷积神经网络来说不算太难，但是，如果输出的位置坐标是一个<strong>任意的正实数</strong>，模型很可能在<strong>大小不同</strong>的物体上<strong>泛化能力</strong>有很大的差异。</p>
<p>这时候当然有一个常见的套路，就是对数据进行归一化，让连续数据的值位于0和1之间。</p>
<p>对于x和y而言，这相对比较容易，毕竟x和y是物体的中心位置，既然物体的中心位置在这个grid之中，那么只要让真实的<strong>x除以grid的宽度</strong>，让真实的<strong>y除以grid的高度</strong>就可以了。</p>
<p>但是h和w就不能这么做了，因为一个物体很可能远大于grid的大小，预测物体的高和宽很可能大于bounding boxs的高和宽，这样w除以bounding boxs的宽度，h除以bounding boxs的高度依旧<strong>不在0和1之间</strong>。</p>
<p>解决方法是让<strong>w除以整张图片的宽度</strong>，<strong>h除以整张图片的高度</strong>。</p>
<p>下面的例子是一个448<em>448的图片，有3</em>3的grid，展示了计算x,y,w,h的真实值（ground truth）的过程：</p>
<p><img src="https://pic1.zhimg.com/80/v2-8fbe9f9fbcdfeac54f36e4174f579e8c_720w.jpg" alt="img"></p>
<p>接下来，我们好好说道说道这个confidence。</p>
<p>confidence的计算公式是：</p>
<p><img src="https://www.zhihu.com/equation?tex=C%3DP+r%28o+b+j%29+%2A+I+O+U_%7Bt+r+u+t+h%7D%5E%7Bp+r+e+d%7D" alt="[公式]"></p>
<p>这个IOU的全称是intersection over union，也就是交并比，它反应了两个框框的相似度。</p>
<p><img src="https://pic2.zhimg.com/80/v2-95ed394b96949c248c7668377d4b3889_720w.jpg" alt="img"></p>
<p><img src="https://www.zhihu.com/equation?tex=I+O+U_%7Bt+r+u+t+h%7D%5E%7Bp+r+e+d%7D" alt="[公式]"> 的意思是预测的bounding box和真实的物体位置的交并比。</p>
<p><img src="https://www.zhihu.com/equation?tex=P+r%28o+b+j%29+" alt="[公式]"> 是一个grid有物体的概率，在有物体的时候ground truth为1，没有物体的时候ground truth为0.</p>
<p>这个<img src="https://www.zhihu.com/equation?tex=I+O+U_%7Bt+r+u+t+h%7D%5E%7Bp+r+e+d%7D" alt="[公式]"> 非常有意思，因为它的groun truth不是确定的，这导致虽然 <img src="https://www.zhihu.com/equation?tex=P+r%28o+b+j%29+" alt="[公式]"> 的ground truth是确定的，但是bounding box的confidence的ground truth是不确定的。</p>
<p>一个不确定的ground truth有什么用呢？</p>
<p>想象这样的问题：老师问小明1+1等于几。小明说等于2，老师又问你有多大的把握你的回答是对的，小明说有80%</p>
<p>这里的80%就是confidence。</p>
<p>confidence主要有两个作用，在后面我会一一介绍。</p>
<p>现在，我们根据上面大雁的图片计算一下样本的groun truth：</p>
<p><img src="https://pic1.zhimg.com/80/v2-8fbe9f9fbcdfeac54f36e4174f579e8c_720w.jpg" alt="img"></p>
<p>首先，这里有9个grid，每个grid有两个bounding box，每个bounding box有5个预测值，假设分类器可以识别出3中物体，那么ground truth的总长度为 <img src="https://www.zhihu.com/equation?tex=S+%5Ctimes+S++%5Ctimes+%28B++%5Ctimes+5++%2BC%29+%3D+3+%5Ctimes+3++%5Ctimes+%282++%5Ctimes+5++%2B3%29+%3D+117" alt="[公式]"></p>
<p>我们假定大雁的类别的one-hot为100，另外两个是火鸡和特朗普，分别是010和001.</p>
<p>我们规定每个grid的ground truth的顺序是confidence, x, y, w, h, c1, c2, c3</p>
<p>那么第一个（左上角）grid的ground truth应该是：0, ?, ?, ?, ?, ?, ?, ?</p>
<p>实际上除了最中间的grid以外，其他的grid的ground truth都是这样的。</p>
<p>这里的&quot;?&quot;的意思是，随便是多少都行，我不在乎。在下面我们会看到，我们<strong>不会</strong>对这些值计算损失函数。</p>
<p>中间的ground truth应该是：</p>
<p>iou, 0.48, 0.28, 0.50, 0.32, 1, 0, 0</p>
<p>iou要根据x, y, w, h的预测值现场计算。</p>
<hr>
<p>这样看似可以让每个grid找到负责的物体，并把它识别出来了。但是还存在一个不得不考虑的问题，如果物体很大，而框框又很小，一个物体被多个框框识别了怎么办？</p>
<p>这里，我们要用到一个叫做<strong>非极大值抑制</strong>Non-maximal suppression(NMS)的技术。</p>
<p>这个NMS还是基于交并比实现的。</p>
<p><img src="https://pic2.zhimg.com/80/v2-afecf4b1039563a528da44282c14703d_720w.jpg" alt="img"></p>
<p>例如在上面狗狗的图里，B1,B2,B3,B4这四个框框可能都说狗狗在我的框里，但是最后的输出应该只有一个框，那怎么把其他框删除呢？</p>
<p>这里就用到了我们之前讲的confidence了，confidence预测<strong>有多大的把握这个物体在我的框里</strong>，我们在同样是检测狗狗的框里，也就是B1,B2,B3,B4中，选择confidence最大的，把其余的都删掉。</p>
<p>也就是只保留B1.</p>
<p>但是这里还有一个引人深思的问题，为什么confidence的定义是 <img src="https://www.zhihu.com/equation?tex=C%3DP+r%28o+b+j%29+%2A+I+O+U_%7Bt+r+u+t+h%7D%5E%7Bp+r+e+d%7D" alt="[公式]"> ，直接用 <img src="https://www.zhihu.com/equation?tex=P+r%28o+b+j%29+" alt="[公式]"> 不行吗，直接用 <img src="https://www.zhihu.com/equation?tex=P+r%28o+b+j%29+" alt="[公式]"> 的话就可以把ground truth确定下来，训练的时候就方便多了。</p>
<p>这里有一个非常非常鸡贼的技巧！</p>
<p>理论上只用 <img src="https://www.zhihu.com/equation?tex=P+r%28o+b+j%29+" alt="[公式]"> 也可以选出应该负责识别物体的grid，但是可能会<strong>不太精确</strong>。这里我们训练的目标是预测 <img src="https://www.zhihu.com/equation?tex=P+r%28o+b+j%29+%2A+I+O+U_%7Bt+r+u+t+h%7D%5E%7Bp+r+e+d%7D" alt="[公式]"> ，我们的想法是让本来不应该预测物体的grid的confidence尽可能的小，既然 <img src="https://www.zhihu.com/equation?tex=P+r%28o+b+j%29+" alt="[公式]"> 的效果不太理想，那我就让 <img src="https://www.zhihu.com/equation?tex=I+O+U_%7Bt+r+u+t+h%7D%5E%7Bp+r+e+d%7D" alt="[公式]"> <strong>尽可能小</strong>。</p>
<p>为什么<strong>真正的最中间的grid</strong>的confidence往往会比较大呢？</p>
<p>因为我们的bounding boxes是用<strong>中点坐标+宽高</strong>表示的，每个grid预测的bounding box都要求其中心在这个grid内，那么如果不是最中间的grid，其他的grid的<strong>IOU自然而言就会比较低</strong>了，因此相应的confidence就降下来了。</p>
<p>现在，我们知道了哪个是应该保留的bounding boxes了，但是还有一个问题，我们是怎么判断出这几个bounding boxes<strong>识别的是同一个物体</strong>的呢？</p>
<p>这里用到NMS的技巧，我们首先判断这几个grid的类别是不是相同的，假设上面的B1，B2，B3和B4识别的都是狗狗，那么进入下一步，我们保留B1，然后判断B2，B3和B4要不要删除。</p>
<p>我们把B1成为<strong>极大bounding box</strong>，计算极大bounding box和其他几个bounding box的IOU，如果超过一个阈值，例如0.5，就认为这<strong>两个bounding box实际上预测的是同一个物体</strong>，就把其中<strong>confidence比较小</strong>的删除。</p>
<p>最后，我们结合极大bounding box和grid识别的种类，判断图片中有什么物体，它们分别是什么，它们分别在哪。</p>
<p><img src="https://pic1.zhimg.com/80/v2-9e17c9c77db99a335deae64157bd80b8_720w.jpg" alt="img"></p>
<p>我们刚才说confidence有两个功能，一个是用来极大值抑制，另一个就是在最后输出结果的时候，将某个bounding box的confidencd和这个bounding box所属的grid的类别概率相乘，然后输出。</p>
<p>举个例子，比如某个grid中的某个bounding box预测到了一个物体，将这个bounding box送入神经网络（其实是整张图片一起送进去的，我们这样说是为了方便），然后神经网络<strong>对bounding box</strong>说，你这里有一个物体的概率是0.8.然后神经网络又<strong>对grid</strong>说，你这个grid里物体最可能是狗，概率是0.7。</p>
<p>那最后这里是狗的概率就是 <img src="https://www.zhihu.com/equation?tex=0.8+%5Ctimes+0.7+%3D+0.56" alt="[公式]"> 。</p>
<p>我们在这里就不细讲yolo的网络结构了，相比之下我认为yolo的<strong>损失函数</strong>的设计更有创见。</p>
<p>yolo的损失函数是这样的：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+loss++%26%3D++%5Clambda_%7B%5Ctext+%7Bcoord+%7D%7D+%5Csum_%7Bi%3D0%7D%5E%7BS%5E%7B2%7D%7D+%5Csum_%7Bj%3D0%7D%5E%7BB%7D+%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bobj+%7D%7D%5Cleft%5B%5Cleft%28x_%7Bi%7D-%5Chat%7Bx%7D_%7Bi%7D%5Cright%29%5E%7B2%7D%2B%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5Cright%29%5E%7B2%7D%5Cright%5D+%5C%5C++%26%2B%5Clambda_%7B%5Ctext+%7Bcoord+%7D%7D+%5Csum_%7Bi%3D0%7D%5E%7BS%5E%7B2%7D%7D+%5Csum_%7Bj%3D0%7D%5E%7BB%7D+%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bobj+%7D%7D%5Cleft%5B%28%5Csqrt%7Bw_%7Bi%7D%7D-%5Csqrt%7B%5Chat%7Bw%7D_%7Bi%7D%7D%29%5E%7B2%7D%2B%28%5Csqrt%7Bh_%7Bi%7D%7D-%5Csqrt%7B%5Chat%7Bh%7D_%7Bi%7D%7D%29%5E%7B2%7D%5Cright%5D+%5C%5C++%26%2B%5Csum_%7Bi%3D0%7D%5E%7BS%5E%7B2%7D%7D+%5Csum_%7Bj%3D0%7D%5E%7BB%7D+%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bobj+%7D%7D%5Cleft%28C_%7Bi%7D-%5Chat%7BC%7D_%7Bi%7D%5Cright%29%5E%7B2%7D+%5C%5C++%26%2B%5Clambda_%7B%5Ctext+%7Bnoobj+%7D%7D+%5Csum_%7Bi%3D0%7D%5E%7BS%5E%7B2%7D%7D+%5Csum_%7Bj%3D0%7D%5E%7BB%7D+%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bnoobj+%7D%7D%5Cleft%28C_%7Bi%7D-%5Chat%7BC%7D_%7Bi%7D%5Cright%29%5E%7B2%7D+%5C%5C+++%26%2B%5Csum_%7Bi%3D0%7D%5E%7BS%5E%7B2%7D%7D+%5Cmathbb%7B1%7D_%7Bi%7D%5E%7B%5Ctext+%7Bobj+%7D%7D+%5Csum_%7Bc+%5Cin+%5Ctext+%7B+classes+%7D%7D%5Cleft%28p_%7Bi%7D%28c%29-%5Chat%7Bp%7D_%7Bi%7D%28c%29%5Cright%29%5E%7B2%7D+%5Cend%7Baligned%7D" alt="[公式]"></p>
<p>首先， <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bobj+%7D%7D" alt="[公式]"> 代表的是这个grid里有没有物体，如果这个grid没有物体， <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bobj+%7D%7D%3D0" alt="[公式]"> ，反之 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bobj+%7D%7D%3D1" alt="[公式]"> 。</p>
<p>与 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bobj+%7D%7D" alt="[公式]"> 相反的是 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bnoobj+%7D%7D" alt="[公式]"> ，如果没有物体， <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bnoobj+%7D%7D%3D1" alt="[公式]"> ，反之 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7B1%7D_%7Bi+j%7D%5E%7B%5Ctext+%7Bnoobj+%7D%7D%3D0" alt="[公式]"> 。</p>
<p>说白了就是，当一个grid有物体的时候，损失函数计算第1，2，3，5项，当grid里没有物体的时候计算第4项。</p>
<p>损失函数一共有5项，我们一项一项的分析。</p>
<p>首先的是中心坐标的损失函数，用了我们最熟悉的均方误差MSE，这个很好理解。</p>
<p>然后是高和宽，没有简单的用MSE，而是用平方根的MSE，这是为什么呢？</p>
<p>第一个原因是更容易优化，但是还有更重要的原因：</p>
<p>看下面的表格：</p>
<img src="https://pic4.zhimg.com/80/v2-a0a7833719a274bfce50aed5ed977217_720w.jpg" alt="img" style="zoom:50%;" />
<p>首先，我们只考虑var1和var2在0和1之间。当var1和var2都很小的时候，也即是w和h都很小，意味着这个物体很小，那么我们应该尽量放大一些损失函数，让模型在识别小物体的时候准确一点。当var1和var2都很大，意味着这个物体也很大，甚至可能已经布满整张图片了，这时我么可以减小一些损失函数，毕竟很大的物体不需要很高的精度。</p>
<p>一句话，使用平方根的MSE而不是MSE其实就是像让模型<strong>对小尺度的物体更敏感</strong>。或者说，对大的和小的物体同样敏感。</p>
<p>接下来的三项都使用了MSE，其实用交叉熵可能会更好，但是这些细节就不追究了。</p>
<p>其中第4项是用来判断一个bounding box中究竟有没有物体的。</p>
<p>接下来的问题是 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bcoord%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bnoobj%7D" alt="[公式]"> 都应该取怎样的值，为什么这样设计？</p>
<p>论文中给出的答案是 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bcoord%7D%3D5" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bnoobj%7D+%3D+0.5" alt="[公式]"> ，也就是放大第一项和第二项的损失函数，缩小第四项的损失函数。</p>
<p>这样做的原因是让梯度更稳定，如果grid中不含有物体，它对1，2，3，5项没有影响，如果调节第四项，会让含有物体的grid的confidence发生改变，这可能使其他项的梯度剧烈变化，从而带来模型上的不稳定。</p>
<p>因此，我们放大第一项和第二项，缩小第四项。</p>
<p>YOLO的设计虽然精巧，但是还有许多不足的地方，比如一个grid只能识别出一种物体。我们会在YOLO v2和YOLO v3中看到更巧妙的设计。</p>
<p>此外，YOLO的bounding box的预测准确度是不如R-CNN这样的Region-based方法的，看YOLO v2里作者们怎么报复回来。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io">CJH</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io/2022/09/08/YOLO%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">http://cjh0220.github.io/2022/09/08/YOLO%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://cjh0220.github.io" target="_blank">CJH's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/YOLO/">YOLO</a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/08/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"><img class="prev-cover" src="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">模拟退火算法</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/08/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%8B%E7%BB%8D/"><img class="next-cover" src="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">目标检测介绍</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJH</div><div class="author-info__description">Hello,my friends</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">59</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cjh0220"><i class="fab fa-github"></i><span>Gihhub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cjh0220" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1005741898@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">推荐博客</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">YOLO（You Only Look Once）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-%E5%89%8D%E8%A8%80"><span class="toc-number">2.1.</span> <span class="toc-text">0 前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%85%88%E4%BB%8E%E4%B8%80%E6%AC%BE%E5%BC%BA%E5%A4%A7%E7%9A%84app%E8%AF%B4%E8%B5%B7"><span class="toc-number">2.2.</span> <span class="toc-text">1 先从一款强大的app说起</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%B0%88%E7%9A%84%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.3.</span> <span class="toc-text">2 不得不谈的分类模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-YOLO%E7%B3%BB%E5%88%97%E6%80%9D%E6%83%B3%E7%9A%84%E9%9B%8F%E5%BD%A2%EF%BC%9AYOLO-v0"><span class="toc-number">2.4.</span> <span class="toc-text">3 YOLO系列思想的雏形：YOLO v0</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-YOLO-v1%E7%BB%88%E4%BA%8E%E8%AF%9E%E7%94%9F"><span class="toc-number">2.5.</span> <span class="toc-text">4 YOLO v1终于诞生</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-YOLO-v2"><span class="toc-number">2.6.</span> <span class="toc-text">5 YOLO v2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-YOLO-v3"><span class="toc-number">2.7.</span> <span class="toc-text">6 YOLO v3</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%96%AB%E6%83%85%E9%83%BD%E6%8C%A1%E4%B8%8D%E4%BD%8F%E7%9A%84YOLO-v4"><span class="toc-number">2.8.</span> <span class="toc-text">7 疫情都挡不住的YOLO v4</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%BB%A3%E7%A0%81%E6%AF%94%E8%AE%BA%E6%96%87%E9%83%BD%E6%97%A9%E7%9A%84YOLO-v5"><span class="toc-number">2.9.</span> <span class="toc-text">8 代码比论文都早的YOLO v5</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-v1"><span class="toc-number">2.10.</span> <span class="toc-text">YOLO v1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-v2"><span class="toc-number">2.11.</span> <span class="toc-text">YOLO v2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-v3"><span class="toc-number">2.12.</span> <span class="toc-text">YOLO v3</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-v4"><span class="toc-number">2.13.</span> <span class="toc-text">YOLO v4</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-v5"><span class="toc-number">2.14.</span> <span class="toc-text">YOLO v5</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-v5%E7%9A%84%E5%9B%9B%E7%A7%8D%E7%BB%93%E6%9E%84%E7%9A%84%E6%B7%B1%E5%BA%A6%EF%BC%9A"><span class="toc-number">2.15.</span> <span class="toc-text">YOLO v5的四种结构的深度：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Yolov5%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">2.15.1.</span> <span class="toc-text">Yolov5网络结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-v5%E7%9A%84%E5%9B%9B%E7%A7%8D%E7%BB%93%E6%9E%84%E7%9A%84%E5%AE%BD%E5%BA%A6%EF%BC%9A"><span class="toc-number">2.16.</span> <span class="toc-text">YOLO v5的四种结构的宽度：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">写给小白的Yolo介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">3.1.</span> <span class="toc-text">YOLO是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO%E4%B9%8B%E5%89%8D%E7%9A%84%E4%B8%96%E7%95%8C"><span class="toc-number">3.2.</span> <span class="toc-text">YOLO之前的世界</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO%E5%8E%9F%E7%90%86"><span class="toc-number">3.3.</span> <span class="toc-text">YOLO原理</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/10/31/%E5%A4%A9%E6%B1%A0%E6%B1%BD%E8%BD%A6%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90%E6%94%BB%E7%95%A5/" title="天池汽车竞品分析攻略"><img src="https://bbsfiles.vivo.com.cn/vivobbs/attachment/forum/201804/01/185444kptaz7wspdasss7y.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="天池汽车竞品分析攻略"/></a><div class="content"><a class="title" href="/2022/10/31/%E5%A4%A9%E6%B1%A0%E6%B1%BD%E8%BD%A6%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90%E6%94%BB%E7%95%A5/" title="天池汽车竞品分析攻略">天池汽车竞品分析攻略</a><time datetime="2022-10-31T01:59:47.000Z" title="发表于 2022-10-31 09:59:47">2022-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/30/Titanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E6%94%BB%E7%95%A5/" title="Titanic生存预测攻略"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fn.sinaimg.cn%2Fsinakd10116%2F680%2Fw1920h1160%2F20200504%2Ff8f5-iteyfwv0171312.jpg&amp;refer=http%3A%2F%2Fn.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1669771764&amp;t=76c184dd81120e988a7ab54f65b051d1" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Titanic生存预测攻略"/></a><div class="content"><a class="title" href="/2022/10/30/Titanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E6%94%BB%E7%95%A5/" title="Titanic生存预测攻略">Titanic生存预测攻略</a><time datetime="2022-10-30T02:57:10.000Z" title="发表于 2022-10-30 10:57:10">2022-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/30/python%E6%A8%A1%E6%8B%9F%E7%BC%93%E5%86%B2%E5%8C%BA%E8%BF%9B%E5%BA%A6%E6%9D%A1/" title="python模拟缓冲区进度条"><img src="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python模拟缓冲区进度条"/></a><div class="content"><a class="title" href="/2022/10/30/python%E6%A8%A1%E6%8B%9F%E7%BC%93%E5%86%B2%E5%8C%BA%E8%BF%9B%E5%BA%A6%E6%9D%A1/" title="python模拟缓冲区进度条">python模拟缓冲区进度条</a><time datetime="2022-10-30T01:17:08.000Z" title="发表于 2022-10-30 09:17:08">2022-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/29/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="集成学习"><img src="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="集成学习"/></a><div class="content"><a class="title" href="/2022/10/29/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="集成学习">集成学习</a><time datetime="2022-10-29T02:54:37.000Z" title="发表于 2022-10-29 10:54:37">2022-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/29/K%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/" title="K折交叉验证"><img src="https://s1.ax1x.com/2022/09/08/vqpCng.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="K折交叉验证"/></a><div class="content"><a class="title" href="/2022/10/29/K%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/" title="K折交叉验证">K折交叉验证</a><time datetime="2022-10-29T02:54:23.000Z" title="发表于 2022-10-29 10:54:23">2022-10-29</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s1.ax1x.com/2022/09/08/vqpCng.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By CJH</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">欢迎欢迎，热烈欢迎</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>