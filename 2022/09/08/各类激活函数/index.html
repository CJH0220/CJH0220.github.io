<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>各类激活函数 | CJH's blog</title><meta name="keywords" content="激活函数"><meta name="author" content="CJH"><meta name="copyright" content="CJH"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="激活函数 为什么要使用激活函数？ 激活函数用来怎加非线性因素的，提高模型拟合能力。如果不存在激活函数，神经网络的每一层的输入都是对前面输入的线性变化，就算把网络加到很深也无法去拟合任意函数的。 激活函数具有的特性 虽然我们常用激活函数不是很多，那是否只有这些函数能作为激活函数呢？我们从神经网络的工作过程中看，激活函数具有什么样的性质能够更好的帮助神经网络的训练。  非线性：数，激活函数必须是非线性">
<meta property="og:type" content="article">
<meta property="og:title" content="各类激活函数">
<meta property="og:url" content="http://cjh0220.github.io/2022/09/08/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/index.html">
<meta property="og:site_name" content="CJH&#39;s blog">
<meta property="og:description" content="激活函数 为什么要使用激活函数？ 激活函数用来怎加非线性因素的，提高模型拟合能力。如果不存在激活函数，神经网络的每一层的输入都是对前面输入的线性变化，就算把网络加到很深也无法去拟合任意函数的。 激活函数具有的特性 虽然我们常用激活函数不是很多，那是否只有这些函数能作为激活函数呢？我们从神经网络的工作过程中看，激活函数具有什么样的性质能够更好的帮助神经网络的训练。  非线性：数，激活函数必须是非线性">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png">
<meta property="article:published_time" content="2022-09-08T11:39:11.000Z">
<meta property="article:modified_time" content="2022-09-08T11:40:01.074Z">
<meta property="article:author" content="CJH">
<meta property="article:tag" content="激活函数">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png"><link rel="shortcut icon" href="/img/CJH.png"><link rel="canonical" href="http://cjh0220.github.io/2022/09/08/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '各类激活函数',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-08 19:40:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">116</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">90</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">CJH's blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">各类激活函数</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2022-09-08T11:39:11.000Z" title="发表于 2022-09-08 19:39:11">2022-09-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="各类激活函数"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>激活函数</h1>
<h2 id="为什么要使用激活函数？">为什么要使用激活函数？</h2>
<p>激活函数用来怎加非线性因素的，提高模型拟合能力。如果不存在激活函数，神经网络的每一层的输入都是对前面输入的线性变化，就算把网络加到很深也无法去拟合任意函数的。</p>
<h2 id="激活函数具有的特性">激活函数具有的特性</h2>
<p>虽然我们常用激活函数不是很多，那是否只有这些函数能作为激活函数呢？我们从神经网络的工作过程中看，激活函数具有什么样的性质能够更好的帮助神经网络的训练。</p>
<ul>
<li>非线性：数，激活函数必须是非线性的。</li>
<li>计算简单：神经元都要经过激活运算的，在随着网络结构越来越庞大、参数量越来越多，激活函数如果计算量小就节约了大量的资源。</li>
<li>f ( x ) ≈ x:在向前传播时，如果参数的初始化是随机量的最小值，神经网络的训练很高效。在训练的时候不会出现输出的幅度随着不断训练发生倍数的增长，是网络更加的稳定，同时也使得梯度更容易回传。</li>
<li>可微：因为神经网络要通过反向传播来跟新参数，如果激活函数不可微，就无法根据损失函数对权重求偏导，也就无法更新权重。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响。</li>
<li>非饱和性：（饱和函数有Sigmoid、Tanh等，非饱和函数ReLU等）例如Sigmoid函数求导以后的值很小，两端的值接近为零在反向传播的时候，如果网络的层次过大便会发生梯度消失的问题，使得浅层的参数无法更新。（梯度消失后面会介绍）</li>
<li>单调性：当激活函数单调时，单层网络保证是凸函数。</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的Learning Rate。</li>
</ul>
<h1>sigmoid</h1>
<p>sigmoid函数，也就是s型曲线函数，如下：</p>
<img src="http://r.photo.store.qq.com/psc?/V546geDp2QZW0L1vGz2K43S5DM3mjCJ4/45NBuzDIW489QBoVep5mcf*48pd3sw1EG6ASFdDywaGGCr4lxdd5LhJ4BRqpK06sV0ipiHOzf97Pdrhqvqvj5D6MUjEuN03e2lZZ5vA8.2g!/r" style="zoom: 50%;" />
<p><strong>原函数图像：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20190828171615342.jpg#pic_center" alt="img"></p>
<p><strong>函数性质</strong></p>
<ul>
<li>非线性函数</li>
<li>求导简单，函数求导后为<em>f</em>′(<em>x</em>)=<em>f</em>(<em>x</em>)(1−<em>f</em>(<em>x</em>))</li>
<li>不满足f ( x ) ≈ x</li>
<li>在定义域内处处可导</li>
<li><strong>饱和激活函数</strong></li>
<li>函数为单调函数</li>
<li>函数的输出区间在（0，1）之间，函数定义域为负无穷到正无穷</li>
</ul>
<p><strong>倒数及其导数图像</strong>：</p>
<p><img src="https://img-blog.csdnimg.cn/20190828174210519.jpg#pic_center" alt="img"></p>
<p><strong>优点和缺点</strong></p>
<ul>
<li>优点：平滑、容易求导</li>
<li>缺点：
<ul>
<li>激活函数运算量大（包含幂的运算）</li>
<li>函数输出不关于原点对称，使得权重更新效率变低，同时这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布</li>
<li>由图像知道导数的取值范围[0,0.25]，非常的小。在进行反向传播计算的时候就会乘上一个很小的值，如果网络层次过深，就会发生“梯度消失”的现象了，无法更新浅层网络的参数了。</li>
</ul>
</li>
</ul>
<h2 id="hard-sigmoid">hard sigmoid</h2>
<p><strong>函数公式：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191106153422324.png#pic_center" alt="img"></p>
<p>解释：当 x &lt; -2.5输出<strong>0</strong>，当 x &gt; 2.5时，输出<strong>1</strong>，当 -2.5 &lt; x &amp; x &lt; 2.5时，输出为 <strong>(2x+5) / 10</strong>，线性函数。<br>
那么其导数，当 x &lt; -2.5输出<strong>0</strong>，当 x &gt; 2.5时，输出<strong>0</strong>，当 -2.5 &lt; x &amp; x &lt; 2.5时，输出为 <strong>1 / 5</strong>。</p>
<p><strong>hard-Sigmoid</strong>函数时<strong>Sigmoid</strong>激活函数的分段线性近似。从公示和曲线上来看，其更易计算，因此会提高训练的效率，不过同时会导致一个问题：就是首次派生值为零可能会导致神经元died或者过慢的学习率。</p>
<p><strong>函数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/20191106153649526.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<h2 id="Tanh-双曲正切函数-激活函数">Tanh(双曲正切函数)激活函数</h2>
<p><strong>函数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/20190828191851823.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25lb19sY3g=,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p><strong>函数公式：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo fence="true">)</mo></mrow><mo>=</mo><mfrac><mrow><mo fence="true">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup><mo fence="true">)</mo></mrow><mrow><mo fence="true">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup><mo fence="true">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">f\left( x\right) =\dfrac{\left( e^{x}-e^{-x}\right) }{\left( e^{x}+e^{-x}\right) }
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">x</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3843em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4483em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5904em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p><strong>导数公式</strong>:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo fence="true">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><msup><mrow><mo fence="true">(</mo><mi>f</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f\left( x\right) =1-\left( f\left( x\right) \right) ^{2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">x</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.204em;vertical-align:-0.25em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">x</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><strong>倒数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/20190828192827207.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25lb19sY3g=,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p><strong>函数性质</strong></p>
<ul>
<li>非线性函数</li>
<li>求导简单</li>
<li>不满足f ( x ) ≈ x</li>
<li><strong>饱和激活函数</strong></li>
<li>函数为单调函数</li>
<li>函数的输出区间在（-1，1）之间，函数定义域为负无穷到正无穷</li>
</ul>
<p><strong>优点与缺点：</strong></p>
<ul>
<li>优点：
<ul>
<li>解决了Sigmoid的输出不关于零点对称的问题</li>
<li>也具有Sigmoid的优点平滑，容易求导</li>
</ul>
</li>
<li>缺点：
<ul>
<li>激活函数运算量大（包含幂的运算）</li>
<li>Tanh的导数图像虽然最大之变大，使得梯度消失的问题得到一定的缓解，但是不能根本解决这个问题</li>
</ul>
</li>
</ul>
<h2 id="ReLU激活函数">ReLU激活函数</h2>
<p>ReLU函数代表的的是“修正线性单元”，它是带有卷积图像的输入x的最大函数(x,o)。ReLU函数将矩阵x内所有负值都设为零，其余的值不变</p>
<p><strong>函数公式：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo fence="true">)</mo></mrow><mo>=</mo><mi>max</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">f\left( x\right) =\max \left( 0,x\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">x</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span></p>
<p><strong>函数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/20190828195811214.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25lb19sY3g=,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p><strong>激活函数的性质：</strong></p>
<ul>
<li>非线性函数（虽然单侧是线性函数）</li>
<li>计算简单是真的简单(不管是在神经网络向前计算过程中还是反向传播的时候)</li>
<li>右侧满足f ( x ) ≈ x {\rm{f}}(x) \approx xf(x)≈x</li>
<li>右侧为单调函数</li>
<li>输出为（0，+无穷）</li>
</ul>
<p><strong>优点和缺点：</strong></p>
<ul>
<li>
<p>优点</p>
<ul>
<li>
<p>计算量小，相对于之前使用sigmoid和Tanh激活函数需要进行指数运算，使用ReLu的计算量小很多，在使用反向传播计算的时候也要收敛更更快。</p>
</li>
<li>
<p>缓解了在深层网络中使用sigmoid和Tanh激活函数造成了梯度消失的现象（右侧导数恒为1）</p>
</li>
<li>
<p>缓解过拟合的问题。由于函数的会使小于零的值变成零，使得一部分神经元的输出为0，造成网络的稀疏</p>
<p>性，减少参数相互依赖的关系缓解过拟合的问题（<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/29021768/answer/43488153">请问人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid function?</a>）</p>
</li>
</ul>
</li>
<li>
<p>缺点</p>
<ul>
<li>造成神经元的“死亡”（详细的介绍见连接<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/67151971/answer/434079498">深度学习中，使用relu存在梯度过大导致神经元“死亡”，怎么理解？</a>）</li>
</ul>
<p>解决方法：优化函数，采用较小学习速率，采用momentum based 优化算法</p>
<ul>
<li>ReLU的输出不是0均值的</li>
</ul>
</li>
</ul>
<h2 id="Leaky-ReLU-变种激活函数">Leaky ReLU 变种激活函数</h2>
<p><strong>函数公式：</strong></p>
<p>f ( x ) = max ⁡ ( α x , x )</p>
<p><strong>函数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/2019083013484538.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25lb19sY3g=,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p>函数图像跟之前的ReLu图像很像，同样的PReLU和ELU激活函数也是在ReLu的基础上针对ReLU在训练时神经元容易死亡做出了优化，基本的思路就是让函数小于0的部分不直接为0，而是等于一个很小的数，使得负轴的信息不至于完全丢弃。</p>
<h2 id="softmax">softmax</h2>
<p>softmax函数，又称**归一化指数函数。**它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。</p>
<img src="https://img-blog.csdnimg.cn/20181128162309759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6X3BldGVy,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />
<p>下面为大家解释一下为什么softmax是这种形式。</p>
<p>首先，我们知道概率有两个性质：1）预测的概率为非负数；2）各种预测结果概率之和等于1。</p>
<p>softmax就是将在负无穷到正无穷上的预测结果按照这两步转换为概率的。</p>
<p><strong>1）将预测结果转化为非负数</strong></p>
<p>下图为y=exp(x）的图像，我们可以知道<strong>指数函数</strong>的值域取值范围是零到正无穷。<strong>softmax第一步就是将模型的预测结果转化到指数函数上，这样保证了概率的非负性。</strong></p>
<img src="https://img-blog.csdnimg.cn/2019111910482712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6X3BldGVy,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 67%;" />
<p>2）各种预测结果概率之和等于1</p>
<p>为了确保各个预测结果的概率之和等于1。我们只需要将转换后的结果进行归一化处理。方法就是将转化后的结果除以所有转化后结果之和，可以理解为转化后结果占总数的百分比。这样就得到近似的概率。</p>
<p>下面为大家举一个例子，假如模型对一个三分类问题的预测结果为-3、1.5、2.7。我们要用softmax将模型结果转为概率。步骤如下：</p>
<p>1）将预测结果转化为非负数</p>
<p>y1 = exp(x1) = exp(-3) = 0.05</p>
<p>y2 = exp(x2) = exp(1.5) = 4.48</p>
<p>y3 = exp(x3) = exp(2.7) = 14.88</p>
<p>2）各种预测结果概率之和等于1</p>
<p>z1 = y1/(y1+y2+y3) = 0.05/(0.05+4.48+14.88) = 0.0026</p>
<p>z2 = y2/(y1+y2+y3) = 4.48/(0.05+4.48+14.88) = 0.2308</p>
<p>z3 = y3/(y1+y2+y3) = 14.88/(0.05+4.48+14.88) = 0.7666</p>
<p>总结一下softmax如何将多分类输出转换为概率，可以分为两步：</p>
<p><strong>1）分子：通过指数函数，将实数输出映射到零到正无穷。</strong></p>
<p><strong>2）分母：将所有结果相加，进行归一化。</strong></p>
<img src="https://img-blog.csdnimg.cn/20181212223827495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6X3BldGVy,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />
<h2 id="SELU（可伸缩的指数线性单元）">SELU（可伸缩的指数线性单元）</h2>
<p>SELU Scaled Exponential Linear Unit等同于：<code>scale * elu(x, alpha)</code>，其中 alpha 和 scale 是预定义的常量。只要正确初始化权重（参见 <code>lecun_normal</code> 初始化方法）并且输入的数量「足够大」（参见参考文献获得更多信息），选择合适的 alpha 和 scale 的值，就可以在两个连续层之间保留输入的均值和方差。</p>
<p>函数形式：</p>
<p><img src="https://ftp.bmp.ovh/imgs/2021/07/510ee62a38ceee09.png" alt=""></p>
<p>深度学习在卷积神经网络和循环神经网络取得很大突破，但标准前馈网络的成功消息却很少。因此引入自归一化的神经网络，来尝试进行高级抽象表示。<br>
这种自归一化的神经网络的激活函数就是selu，它也是一种基于激活函数的正则化方案。它具有自归一化特点，即使加入噪声也能收敛到均值为0、方差为1或方差具有上下界。<br>
优点：在全连接层效果好，可以避免梯度消失和爆炸。<br>
缺点：在卷积网络效果尚未证明。可能引起过拟合。</p>
<h2 id="ELU（指数线性单元）">ELU（指数线性单元）</h2>
<p>其将激活函数的平均值接近零，从而加快学习的速度。同时，还可以通过正值的标识来避免梯度消失的问题。根据一些研究，ELU的分类 精确度要高于Relu。</p>
<p><strong>函数公式：</strong></p>
<img src="https://img-blog.csdnimg.cn/20191030135243499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p><strong>函数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/20191030140332643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<ul>
<li>融合了sigmoid和ReLU，左侧具有软饱和性，右侧无饱和性。</li>
<li>右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。</li>
<li>ELU的输出均值接近于零，所以收敛速度更快。</li>
<li>在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU<br>
网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU<br>
网络在Fan-in/Fan-out下都能收敛。</li>
</ul>
<h2 id="SELU（给ELU乘个系数）">SELU（给ELU乘个系数）</h2>
<p><strong>函数公式：</strong></p>
<img src="https://img-blog.csdnimg.cn/20191030142451758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p><strong>函数图像：</strong></p>
<p>lambda为系数</p>
<img src="https://img-blog.csdnimg.cn/20191030153053550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:33%;" />
<h2 id="P-Relu（参数化修正线性单元）">P-Relu（参数化修正线性单元）</h2>
<p>可以看作是Leaky ReLU的一个变体，不同的是，P-ReLU中的负值部分的斜率是根据数据来定的，即a的值并不是一个常数。</p>
<h2 id="R-Relu（随机纠正线性单元）">R-Relu（随机纠正线性单元）</h2>
<p>R-ReLU也是Leaky ReLU的一个变体，只不过在这里负值部分的斜率在训练的时候是随机的，即在一个范围内随机抽取a的值，不过这个值在测试环节会固定下来。</p>
<h2 id="Swish">Swish</h2>
<p><strong>函数公式：</strong></p>
<img src="https://img-blog.csdnimg.cn/20191030142614595.png#pic_center" alt="img" style="zoom:50%;" />
<p><strong>导数公式：</strong></p>
<img src="https://img-blog.csdnimg.cn/20191030151728500.png#pic_center" alt="img" style="zoom:50%;" />
<p><strong>函数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/2019103014450670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p><strong>导数图像：</strong></p>
<img src="https://img-blog.csdnimg.cn/20191030151508307.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p>当β = 0时,Swish变为线性函数f(x)=x2f(x)=x2.<br>
β → ∞, σ(x)=(1+exp(−x))−1σ(x)=(1+exp⁡(−x))−1为0或1. Swish变为ReLU: f(x)=2max(0,x)<br>
所以Swish函数可以看做是介于线性函数与ReLU函数之间的平滑函数.</p>
<h2 id="Mish">Mish</h2>
<p><strong>函数公式</strong></p>
<img src="https://img-blog.csdnimg.cn/20191101121536815.png#pic_center" alt="img" style="zoom:50%;" />
<p><strong>公式推导：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191101121647443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p>
<p>Mish和Swish中参数=1的曲线对比：（第一张是原始函数，第二张是导数）</p>
<img src="https://img-blog.csdnimg.cn/20191101121931980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom: 50%;" />
<img src="https://img-blog.csdnimg.cn/20191101121949780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<p><strong>优点</strong></p>
<p>以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。理论上对负值的轻微允许允许更好的梯度流，而不是像ReLU中那样的硬零边界。</p>
<p>最后，可能也是最重要的，目前的想法是，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。</p>
<p>要区别可能是Mish函数在曲线上几乎所有点上的平滑度</p>
<h2 id="Maxout">Maxout</h2>
<p>与常规的激活函数不同，Maxout是一个可以学习的分段线性函数。<br>
其可以看做是在深度学习网络中加入了一层激活函数层，包含一个参数k，这一层相比ReLU，Sigmoid等，其在于增加了k个神经元，然后输出激活值最大的值。<br>
其需要学习的参数就是k个神经元中的权值和偏置，这就相当于常规的激活函数一层，而Maxout是两层，而且参数个数增加了K倍。<br>
其可以有效的原理是，任何ReLU及其变体等激活函数都可以看成分段的线性函数，而Maxout加入的一层神经元正是一个可以学习参数的分段线性函数。</p>
<p><strong>优点</strong>：其拟合能力很强，理论上可以拟合任意的凸函数；<br>
具有ReLU的所有优点，线性和非饱和性；<br>
同时没有ReLU的一些缺点，如神经元的死亡；<br>
<strong>缺点</strong>：导致整体参数的激增。</p>
<p>网络图片：</p>
<img src="https://img-blog.csdnimg.cn/20191030153907257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />
<h2 id="关于激活函数统一说明">关于激活函数统一说明</h2>
<p>ELU在正半轴取输入x奸情了梯度弥散情况（正半轴导数处处为1），而这一点特性，基本上除了swish其他非饱和激活函数都具有这个特性。而只有ReLU在负半轴的输出没有复制，所以ReLU的输出均值一定是大于0的，而当激活值的均值非0时，会对下一层造成以bias，也就是下一层的激活单元会出现bias shift现象，通过不断的层数叠加，bias shift会变得非常大。而ELU可以让激活函数的输出均值尽可能接近0，类似于BN操作，但是计算复杂度更低。而且虽然Leaky ReLU和PReLU等都有负值，但是它们不保证在不激活的状态对噪声鲁棒，这里的不激活指的是负半轴。而ELU在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性。Swish和ELU都是可以取负值，同时在负半轴具有软饱和的性能，提高了对噪声的鲁棒性，SELU效果比ELU效果还要更好。</p>
<p><img src="https://img-blog.csdnimg.cn/20191030152606832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pza19sZWFybmVy,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io">CJH</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://cjh0220.github.io/2022/09/08/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">http://cjh0220.github.io/2022/09/08/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://cjh0220.github.io" target="_blank">CJH's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">激活函数</a></div><div class="post_share"><div class="social-share" data-image="https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/08/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%8B%E7%BB%8D/"><img class="prev-cover" src="https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">特征工程介绍</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/08/%E5%90%84%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"><img class="next-cover" src="https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">各类损失函数</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s1.ax1x.com/2022/09/08/vbOo6J.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJH</div><div class="author-info__description">Hello,my friends</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">116</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">90</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cjh0220"><i class="fab fa-github"></i><span>Gihhub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cjh0220" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1005741898@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">为什么要使用激活函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%85%B7%E6%9C%89%E7%9A%84%E7%89%B9%E6%80%A7"><span class="toc-number">1.2.</span> <span class="toc-text">激活函数具有的特性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">sigmoid</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#hard-sigmoid"><span class="toc-number">2.1.</span> <span class="toc-text">hard sigmoid</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tanh-%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">Tanh(双曲正切函数)激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.</span> <span class="toc-text">ReLU激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Leaky-ReLU-%E5%8F%98%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.</span> <span class="toc-text">Leaky ReLU 变种激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax"><span class="toc-number">2.5.</span> <span class="toc-text">softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SELU%EF%BC%88%E5%8F%AF%E4%BC%B8%E7%BC%A9%E7%9A%84%E6%8C%87%E6%95%B0%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%89"><span class="toc-number">2.6.</span> <span class="toc-text">SELU（可伸缩的指数线性单元）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ELU%EF%BC%88%E6%8C%87%E6%95%B0%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%89"><span class="toc-number">2.7.</span> <span class="toc-text">ELU（指数线性单元）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SELU%EF%BC%88%E7%BB%99ELU%E4%B9%98%E4%B8%AA%E7%B3%BB%E6%95%B0%EF%BC%89"><span class="toc-number">2.8.</span> <span class="toc-text">SELU（给ELU乘个系数）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P-Relu%EF%BC%88%E5%8F%82%E6%95%B0%E5%8C%96%E4%BF%AE%E6%AD%A3%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%89"><span class="toc-number">2.9.</span> <span class="toc-text">P-Relu（参数化修正线性单元）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#R-Relu%EF%BC%88%E9%9A%8F%E6%9C%BA%E7%BA%A0%E6%AD%A3%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%89"><span class="toc-number">2.10.</span> <span class="toc-text">R-Relu（随机纠正线性单元）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Swish"><span class="toc-number">2.11.</span> <span class="toc-text">Swish</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mish"><span class="toc-number">2.12.</span> <span class="toc-text">Mish</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Maxout"><span class="toc-number">2.13.</span> <span class="toc-text">Maxout</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%BB%9F%E4%B8%80%E8%AF%B4%E6%98%8E"><span class="toc-number">2.14.</span> <span class="toc-text">关于激活函数统一说明</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/09/04/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89SFM/" title="三维重建学习（七）SFM"><img src="https://s3.bmp.ovh/imgs/2024/09/02/facd3eeb2161346a.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="三维重建学习（七）SFM"/></a><div class="content"><a class="title" href="/2024/09/04/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89SFM/" title="三维重建学习（七）SFM">三维重建学习（七）SFM</a><time datetime="2024-09-04T11:17:15.000Z" title="发表于 2024-09-04 19:17:15">2024-09-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/27/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80-%E5%85%AD%EF%BC%89%E4%BC%A0%E7%BB%9F%E8%A7%86%E8%A7%89%E5%87%A0%E4%BD%95/" title="三维重建学习（一~六）传统视觉几何"><img src="https://s3.bmp.ovh/imgs/2024/08/27/aba7fea4981b21dc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="三维重建学习（一~六）传统视觉几何"/></a><div class="content"><a class="title" href="/2024/08/27/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80-%E5%85%AD%EF%BC%89%E4%BC%A0%E7%BB%9F%E8%A7%86%E8%A7%89%E5%87%A0%E4%BD%95/" title="三维重建学习（一~六）传统视觉几何">三维重建学习（一~六）传统视觉几何</a><time datetime="2024-08-27T08:41:55.000Z" title="发表于 2024-08-27 16:41:55">2024-08-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/27/%E9%9A%8F%E6%9C%BA%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/" title="随机微分方程"><img src="https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="随机微分方程"/></a><div class="content"><a class="title" href="/2024/08/27/%E9%9A%8F%E6%9C%BA%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/" title="随机微分方程">随机微分方程</a><time datetime="2024-08-27T04:39:15.000Z" title="发表于 2024-08-27 12:39:15">2024-08-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/27/TSDF%E6%88%AA%E6%96%AD%E7%AC%A6%E5%8F%B7%E8%B7%9D%E7%A6%BB/" title="TSDF截断符号距离"><img src="https://s3.bmp.ovh/imgs/2024/08/21/9a2c38e103166449.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TSDF截断符号距离"/></a><div class="content"><a class="title" href="/2024/08/27/TSDF%E6%88%AA%E6%96%AD%E7%AC%A6%E5%8F%B7%E8%B7%9D%E7%A6%BB/" title="TSDF截断符号距离">TSDF截断符号距离</a><time datetime="2024-08-27T04:39:00.000Z" title="发表于 2024-08-27 12:39:00">2024-08-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/27/Mamba%E5%AD%A6%E4%B9%A0/" title="Mamba学习"><img src="https://pic2.zhimg.com/v2-826c73167557ddbb76f443ddcc990171_b.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Mamba学习"/></a><div class="content"><a class="title" href="/2024/08/27/Mamba%E5%AD%A6%E4%B9%A0/" title="Mamba学习">Mamba学习</a><time datetime="2024-08-27T04:38:45.000Z" title="发表于 2024-08-27 12:38:45">2024-08-27</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s3.bmp.ovh/imgs/2024/08/08/ae1ec012ccabe947.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By CJH</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">欢迎欢迎，热烈欢迎</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>